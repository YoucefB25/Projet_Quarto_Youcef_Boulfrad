{
  "hash": "5238908db5aa5950680ace6ca2d5d06c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Meilleures Performances Globales\n---\n\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# ðŸ“Š DonnÃ©es des performances des modÃ¨les\ndata = {\n    \"Classe\": [\"1 (10592)\", \"2 (14165)\", \"3 (7151)\", \"4 (549)\", \"5 (1899)\", \"6 (3473)\", \"7 (4102)\", \"Total (41931)\"],\n    \"KNN\": [78.48, 80.76, 86.36, 63.64, 77.89, 73.05, 93.79, 81.42],\n    \"LDA\": [63.43, 65.83, 63.08, 48.18, 47.11, 52.45, 80.88, 64.04],\n    \"QDA\": [54.88, 55.10, 66.29, 48.18, 50.26, 48.27, 80.88, 58.60],\n    \"Bayesien NaÃ¯f\": [61.35, 61.14, 65.66, 60.00, 46.32, 44.81, 79.29, 61.70],\n    \"Arbre CART\": [74.61, 76.84, 85.03, 67.27, 69.21, 74.06, 90.86, 78.35],\n    \"ForÃªt AlÃ©atoire\": [82.73, 87.89, 94.55, 64.55, 71.84, 78.67, 94.28, 86.55],\n    \"Reg Log OVA\": [63.10, 75.89, 88.81, 26.36, 16.58, 27.67, 81.36, 68.07],\n    \"Reg Log OVO\": [66.97, 75.40, 86.78, 34.55, 25.79, 40.06, 80.39, 69.99],\n    \"Reg Multinom\": [66.82, 75.61, 87.34, 30.91, 24.21, 35.30, 79.66, 69.54],\n    \"RÃ©seau Neurones\": [78.29, 86.69, 84.55, 70.00, 83.95, 86.31, 92.33, 84.38],\n    \"SVM OVA\": [68.29, 78.57, 90.63, 20.00, 33.16, 38.33, 82.10, 72.22],\n    \"SVM OVO\": [70.55, 78.61, 90.35, 21.82, 34.47, 38.62, 81.49, 72.80]\n}\n\n# ðŸ“‹ CrÃ©ation du DataFrame et arrondi Ã  2 dÃ©cimales\ndf = pd.DataFrame(data).set_index(\"Classe\").round(2)\n\n# ðŸ“Š CrÃ©ation du tableau avec `matplotlib`\nfig, ax = plt.subplots(figsize=(5, 2))  # ðŸ” Ajustement de la taille\nax.axis(\"tight\")\nax.axis(\"off\")\n\n# ðŸ–Œ CrÃ©ation du tableau\ntable = ax.table(cellText=df.values, colLabels=df.columns, rowLabels=df.index,\n                 cellLoc=\"center\", loc=\"center\", colWidths=[0.15] + [0.08]*len(df.columns))\n\n# ðŸŽ¨ Application des couleurs conditionnelles sur les cellules de donnÃ©es\nfor i in range(len(df.index)):  # ðŸ”¢ Parcours des lignes (les classes)\n    for j in range(len(df.columns)):  # ðŸ”¢ Parcours des colonnes (modÃ¨les)\n        value = df.iloc[i, j]  # ðŸ“Œ RÃ©cupÃ©ration de la valeur\n        cell = table.get_celld().get((i+1, j))  # ðŸ“Œ +1 pour Ã©viter la ligne d'en-tÃªte\n        if cell:  # âœ… VÃ©rification que la cellule existe bien\n            if value >= 50:\n                cell.set_facecolor(\"#c2f0c2\")  # âœ… Vert clair pour valeurs â‰¥ 50%\n            else:\n                cell.set_facecolor(\"#f4cccc\")  # âœ… Rouge clair pour valeurs < 50%\n\n# ðŸ“Š Affichage du tableau formatÃ©\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](conclusions_files/figure-pdf/cell-2-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n- La ForÃªt AlÃ©atoire est le meilleur modÃ¨le global avec 86.55% de prÃ©cision moyenne.\n- Le RÃ©seau de Neurones suit de prÃ¨s avec 84.38%, prouvant l'efficacitÃ© des mÃ©thodes d'apprentissage profond.\n- Le KNN (81.42%) et l'Arbre CART (78.35%) sont Ã©galement compÃ©titifs.\n\n# Analyse des Classes Minoritaires (4, 5, 6)\n\n- Les classes peu reprÃ©sentÃ©es (4, 5, 6) sont souvent mal classÃ©es.\n- La ForÃªt AlÃ©atoire (64.55%, 71.84%, 78.67%) offre la meilleure robustesse.\n- Le RÃ©seau de Neurones (70.00%, 83.95%, 86.31%) sâ€™adapte bien aux dÃ©sÃ©quilibres.\n- L'Arbre CART (67.27%, 69.21%, 74.06%) est un compromis intÃ©ressant.\n\n## MÃ©thodes les Plus Faibles :\n\n- SVM et RÃ©gressions Logistiques (OVA et OVO) sous-performent sur les classes minoritaires (moins de 40% pour certaines).\n- QDA et BayÃ©sien NaÃ¯f sont globalement moins efficaces avec 58.60% et 61.70% respectivement.\n\n# MÃ©thodes ParamÃ©triques vs Non-ParamÃ©triques\n\nLes modÃ¨les non-paramÃ©triques surpassent largement les modÃ¨les paramÃ©triques.\n- ForÃªt AlÃ©atoire (86.55%), RÃ©seau de Neurones (84.38%), KNN (81.42%) et Arbre CART (78.35%) dominent le classement.\n- Ã€ lâ€™inverse, LDA (64.04%), QDA (58.60%) et BayÃ©sien NaÃ¯f (61.70%) sont nettement moins performants.\n\nLes modÃ¨les non-paramÃ©triques sont plus flexibles et capables de capturer des structures complexes dans les donnÃ©es,\nalors que les modÃ¨les paramÃ©triques reposent sur des hypothÃ¨ses restrictives qui limitent leur efficacitÃ©.\n\n# Multiclasse Natif vs AdaptÃ© (OVA/OVO)\n\nLes mÃ©thodes natives multiclasses (comme la ForÃªt AlÃ©atoire, Arbre CART, RÃ©seau de Neurones)\nont des performances globalement meilleures que les modÃ¨les binaires adaptÃ©s OVA et OVO.\n\n- La ForÃªt AlÃ©atoire (86.55%) et RÃ©seau de Neurones (84.38%), qui sont naturellement adaptÃ©s au multiclass,\nsurpassent les modÃ¨les SVM OVA (72.22%) et SVM OVO (72.80%), ainsi que les rÃ©gressions logistiques OVA et OVO.\n\n- Les mÃ©thodes binaires adaptÃ©es (OVA et OVO) peinent surtout sur les classes minoritaires,\navec des scores trÃ¨s faibles (ex. SVM OVA : 20.00% sur la classe 4 !).\n\n## Explication : L'OVA force une classe unique Ã  se dÃ©marquer contre toutes les autres,\ntandis que l'OVO compare les classes deux Ã  deux, ce qui est sous-optimal pour des classes dÃ©sÃ©quilibrÃ©es.\n\n# RÃ©sumÃ© Final\n\n- Les modÃ¨les non-paramÃ©triques sont les meilleurs grÃ¢ce Ã  leur flexibilitÃ© et capacitÃ© dâ€™adaptation aux classes dÃ©sÃ©quilibrÃ©es.\n- Les mÃ©thodes multiclasses natives (ForÃªt AlÃ©atoire, RÃ©seau de Neurones, Arbre CART) dominent les modÃ¨les binaires adaptÃ©s.\n- Si les classes minoritaires sont importantes, privilÃ©giez ForÃªt AlÃ©atoire, RÃ©seau de Neurones ou Arbre CART.\n- Les modÃ¨les OVA/OVO ne sont pas adaptÃ©s aux jeux de donnÃ©es avec des classes dÃ©sÃ©quilibrÃ©es.\n\n\n",
    "supporting": [
      "conclusions_files"
    ],
    "filters": []
  }
}