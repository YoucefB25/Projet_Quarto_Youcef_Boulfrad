[
  {
    "objectID": "donnees.html",
    "href": "donnees.html",
    "title": "Téléchargement et Préparation du Dataset Covertype",
    "section": "",
    "text": "La base de données Covertype provient de l’UCI Machine Learning Repository. Elle est utilisée pour classer les types de couvert forestier à partir de mesures cartographiques (sol, altitude, pente, distance aux points d’eau, etc.).\n\n\n\nNombre d’observations : 581 012\nNombre de variables : 54 (features continues et binaires)\nNombre de classes : 7 types de couverture forestière (1 à 7)\nProblème à résoudre : Classification supervisée\n\nLes classes ne sont pas équilibrées, ce qui peut influencer la performance des modèles de classification. Les proportions des classes dans l’ensemble original sont les suivantes :\n\n\n\nClasse\nType de forêt\nEffectif\nProportion (%)\n\n\n\n\n1\nÉpicéa\n211 840\n36.5\n\n\n2\nPin\n283 301\n48.8\n\n\n3\nPeuplier\n35 754\n6.2\n\n\n4\nBouleau\n2 747\n0.5\n\n\n5\nÉrable\n9 493\n1.6\n\n\n6\nHêtre\n17 367\n3.0\n\n\n7\nMélèze\n18 510\n3.2\n\n\n\n\n\n\n\n\n\nLe dataset Covertype est très grand (581 012 individus). En raison du temps de calcul important, nous avons décidé d’utiliser un échantillon plus petit, tout en conservant la distribution des classes.\n\n\n\nCertaines classes sont très majoritaires (ex : Pin et Épicéa représentent à eux seuls 85% des données), tandis que d’autres sont très minoritaires (ex : Bouleau à seulement 0.5%).\nNous avons appliqué un échantillonnage différencié :\n\nSous-échantillonnage des classes majoritaires (Épicéa et Pin) → 5% de leurs effectifs d’origine\nSur-échantillonnage relatif des classes minoritaires (Peuplier, Bouleau, Érable, Hêtre, Mélèze) → 20% de leurs effectifs d’origine\n\nNous ne supprimons pas totalement le déséquilibre, car nous souhaitons tester nos modèles dans des conditions réalistes, où certaines classes restent plus rares que d’autres.\nL’échantillon obtenu comptera un peu plus de 40 000 individus, dont 60% seront réservés à l’entrainement des modèles, 20% à la validation des hyperparamètres, et 20% aux tests.\n\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Téléchargement direct des données depuis l'URL\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\ncolumn_names = [f'Feature_{i}' for i in range(1, 55)] + ['Cover_Type']\ndata = pd.read_csv(url, header=None, names=column_names)\n\n# Définition des taux d'échantillonnage (différent selon les classes)\nsampling_rates = {1: 0.05, 2: 0.05, 3: 0.2, 4: 0.2, 5: 0.2, 6: 0.2, 7: 0.2}\n\n# Échantillonnage différencié par classe\nsampled_data = pd.concat([\n    data[data['Cover_Type'] == cls].sample(frac=sampling_rates[cls], random_state=42)\n    for cls in data['Cover_Type'].unique()\n])\n\n# Réinitialisation des index après échantillonnage\nsampled_data = sampled_data.reset_index(drop=True)\n\n# Affichage des effectifs par classe après échantillonnage\nprint(\"Effectifs par classe après échantillonnage différencié :\")\nprint(sampled_data['Cover_Type'].value_counts().sort_index())\n\n# Division des données en ensembles d'entraînement, validation et test\ntrain_data, temp_data = train_test_split(sampled_data, test_size=0.4, random_state=42, stratify=sampled_data['Cover_Type'])\nval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=temp_data['Cover_Type'])\n\n# Affichage des tailles des ensembles\nprint(f\"\\nTaille des ensembles :\")\nprint(f\"  - Entraînement : {len(train_data)} lignes\")\nprint(f\"  - Validation : {len(val_data)} lignes\")\nprint(f\"  - Test : {len(test_data)} lignes\")\n\n# Affichage des effectifs par classe dans chaque ensemble\nprint(\"\\nEffectifs par classe dans l'ensemble d'entraînement :\")\nprint(train_data['Cover_Type'].value_counts().sort_index())\n\nprint(\"\\nEffectifs par classe dans l'ensemble de validation :\")\nprint(val_data['Cover_Type'].value_counts().sort_index())\n\nprint(\"\\nEffectifs par classe dans l'ensemble de test :\")\nprint(test_data['Cover_Type'].value_counts().sort_index())\n\n# Sauvegarder les ensembles en fichiers CSV\ntrain_data.to_csv('covertype_train.csv', index=False)\nval_data.to_csv('covertype_val.csv', index=False)\ntest_data.to_csv('covertype_test.csv', index=False)\n\nEffectifs par classe après échantillonnage différencié :\nCover_Type\n1    10592\n2    14165\n3     7151\n4      549\n5     1899\n6     3473\n7     4102\nName: count, dtype: int64\n\nTaille des ensembles :\n  - Entraînement : 25158 lignes\n  - Validation : 8386 lignes\n  - Test : 8387 lignes\n\nEffectifs par classe dans l'ensemble d'entraînement :\nCover_Type\n1    6355\n2    8499\n3    4291\n4     329\n5    1139\n6    2084\n7    2461\nName: count, dtype: int64\n\nEffectifs par classe dans l'ensemble de validation :\nCover_Type\n1    2118\n2    2833\n3    1430\n4     110\n5     380\n6     695\n7     820\nName: count, dtype: int64\n\nEffectifs par classe dans l'ensemble de test :\nCover_Type\n1    2119\n2    2833\n3    1430\n4     110\n5     380\n6     694\n7     821\nName: count, dtype: int64"
  },
  {
    "objectID": "donnees.html#caractéristiques-du-dataset",
    "href": "donnees.html#caractéristiques-du-dataset",
    "title": "Téléchargement et Préparation du Dataset Covertype",
    "section": "",
    "text": "Nombre d’observations : 581 012\nNombre de variables : 54 (features continues et binaires)\nNombre de classes : 7 types de couverture forestière (1 à 7)\nProblème à résoudre : Classification supervisée\n\nLes classes ne sont pas équilibrées, ce qui peut influencer la performance des modèles de classification. Les proportions des classes dans l’ensemble original sont les suivantes :\n\n\n\nClasse\nType de forêt\nEffectif\nProportion (%)\n\n\n\n\n1\nÉpicéa\n211 840\n36.5\n\n\n2\nPin\n283 301\n48.8\n\n\n3\nPeuplier\n35 754\n6.2\n\n\n4\nBouleau\n2 747\n0.5\n\n\n5\nÉrable\n9 493\n1.6\n\n\n6\nHêtre\n17 367\n3.0\n\n\n7\nMélèze\n18 510\n3.2"
  },
  {
    "objectID": "donnees.html#objectif-de-léchantillonnage",
    "href": "donnees.html#objectif-de-léchantillonnage",
    "title": "Téléchargement et Préparation du Dataset Covertype",
    "section": "",
    "text": "Le dataset Covertype est très grand (581 012 individus). En raison du temps de calcul important, nous avons décidé d’utiliser un échantillon plus petit, tout en conservant la distribution des classes.\n\n\n\nCertaines classes sont très majoritaires (ex : Pin et Épicéa représentent à eux seuls 85% des données), tandis que d’autres sont très minoritaires (ex : Bouleau à seulement 0.5%).\nNous avons appliqué un échantillonnage différencié :\n\nSous-échantillonnage des classes majoritaires (Épicéa et Pin) → 5% de leurs effectifs d’origine\nSur-échantillonnage relatif des classes minoritaires (Peuplier, Bouleau, Érable, Hêtre, Mélèze) → 20% de leurs effectifs d’origine\n\nNous ne supprimons pas totalement le déséquilibre, car nous souhaitons tester nos modèles dans des conditions réalistes, où certaines classes restent plus rares que d’autres.\nL’échantillon obtenu comptera un peu plus de 40 000 individus, dont 60% seront réservés à l’entrainement des modèles, 20% à la validation des hyperparamètres, et 20% aux tests."
  },
  {
    "objectID": "donnees.html#téléchargement-et-préparation-des-données",
    "href": "donnees.html#téléchargement-et-préparation-des-données",
    "title": "Téléchargement et Préparation du Dataset Covertype",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Téléchargement direct des données depuis l'URL\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\ncolumn_names = [f'Feature_{i}' for i in range(1, 55)] + ['Cover_Type']\ndata = pd.read_csv(url, header=None, names=column_names)\n\n# Définition des taux d'échantillonnage (différent selon les classes)\nsampling_rates = {1: 0.05, 2: 0.05, 3: 0.2, 4: 0.2, 5: 0.2, 6: 0.2, 7: 0.2}\n\n# Échantillonnage différencié par classe\nsampled_data = pd.concat([\n    data[data['Cover_Type'] == cls].sample(frac=sampling_rates[cls], random_state=42)\n    for cls in data['Cover_Type'].unique()\n])\n\n# Réinitialisation des index après échantillonnage\nsampled_data = sampled_data.reset_index(drop=True)\n\n# Affichage des effectifs par classe après échantillonnage\nprint(\"Effectifs par classe après échantillonnage différencié :\")\nprint(sampled_data['Cover_Type'].value_counts().sort_index())\n\n# Division des données en ensembles d'entraînement, validation et test\ntrain_data, temp_data = train_test_split(sampled_data, test_size=0.4, random_state=42, stratify=sampled_data['Cover_Type'])\nval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=temp_data['Cover_Type'])\n\n# Affichage des tailles des ensembles\nprint(f\"\\nTaille des ensembles :\")\nprint(f\"  - Entraînement : {len(train_data)} lignes\")\nprint(f\"  - Validation : {len(val_data)} lignes\")\nprint(f\"  - Test : {len(test_data)} lignes\")\n\n# Affichage des effectifs par classe dans chaque ensemble\nprint(\"\\nEffectifs par classe dans l'ensemble d'entraînement :\")\nprint(train_data['Cover_Type'].value_counts().sort_index())\n\nprint(\"\\nEffectifs par classe dans l'ensemble de validation :\")\nprint(val_data['Cover_Type'].value_counts().sort_index())\n\nprint(\"\\nEffectifs par classe dans l'ensemble de test :\")\nprint(test_data['Cover_Type'].value_counts().sort_index())\n\n# Sauvegarder les ensembles en fichiers CSV\ntrain_data.to_csv('covertype_train.csv', index=False)\nval_data.to_csv('covertype_val.csv', index=False)\ntest_data.to_csv('covertype_test.csv', index=False)\n\nEffectifs par classe après échantillonnage différencié :\nCover_Type\n1    10592\n2    14165\n3     7151\n4      549\n5     1899\n6     3473\n7     4102\nName: count, dtype: int64\n\nTaille des ensembles :\n  - Entraînement : 25158 lignes\n  - Validation : 8386 lignes\n  - Test : 8387 lignes\n\nEffectifs par classe dans l'ensemble d'entraînement :\nCover_Type\n1    6355\n2    8499\n3    4291\n4     329\n5    1139\n6    2084\n7    2461\nName: count, dtype: int64\n\nEffectifs par classe dans l'ensemble de validation :\nCover_Type\n1    2118\n2    2833\n3    1430\n4     110\n5     380\n6     695\n7     820\nName: count, dtype: int64\n\nEffectifs par classe dans l'ensemble de test :\nCover_Type\n1    2119\n2    2833\n3    1430\n4     110\n5     380\n6     694\n7     821\nName: count, dtype: int64"
  },
  {
    "objectID": "QDA.html",
    "href": "QDA.html",
    "title": "Analyse Discriminante Quadratique (QDA) - Multiclasse",
    "section": "",
    "text": "L’Analyse Discriminante Quadratique (QDA) est une technique de classification qui, contrairement à LDA, permet aux classes d’avoir des matrices de covariance différentes. Cela le rend plus flexible mais peut aussi augmenter le risque de sur-apprentissage.\nContrairement à d’autres modèles initialement conçus pour des problèmes binaires et adaptés aux cas multiclasse via OVA ou OVO, QDA est intrinsèquement multiclasse. Il attribue directement une observation à l’une des classes disponibles en estimant des distributions normales multivariées et en utilisant la règle de Bayes."
  },
  {
    "objectID": "QDA.html#théorie",
    "href": "QDA.html#théorie",
    "title": "Analyse Discriminante Quadratique (QDA) - Multiclasse",
    "section": "",
    "text": "L’Analyse Discriminante Quadratique (QDA) est une technique de classification qui, contrairement à LDA, permet aux classes d’avoir des matrices de covariance différentes. Cela le rend plus flexible mais peut aussi augmenter le risque de sur-apprentissage.\nContrairement à d’autres modèles initialement conçus pour des problèmes binaires et adaptés aux cas multiclasse via OVA ou OVO, QDA est intrinsèquement multiclasse. Il attribue directement une observation à l’une des classes disponibles en estimant des distributions normales multivariées et en utilisant la règle de Bayes."
  },
  {
    "objectID": "QDA.html#hyperparamètre-utilisé",
    "href": "QDA.html#hyperparamètre-utilisé",
    "title": "Analyse Discriminante Quadratique (QDA) - Multiclasse",
    "section": "Hyperparamètre utilisé",
    "text": "Hyperparamètre utilisé\nNous allons optimiser :\n\nRégularisation (reg_param) : contrôle la variance de la covariance estimée et est sélectionnée en fonction de la précision sur l’ensemble de validation."
  },
  {
    "objectID": "QDA.html#métriques-dévaluation",
    "href": "QDA.html#métriques-dévaluation",
    "title": "Analyse Discriminante Quadratique (QDA) - Multiclasse",
    "section": "Métriques d’évaluation",
    "text": "Métriques d’évaluation\nNous afficherons :\n\nMatrice de confusion : montrant les erreurs de classification sur l’échantillon de test.\nTaux de bien classés sur l’échantillon de validation avec le meilleur hyperparamètre.\nTaux de bien classés sur l’échantillon de test avec ce même hyperparamètre.\nTaux de bien classés par classe sur l’échantillon de test pour observer la précision sur chaque classe."
  },
  {
    "objectID": "QDA.html#recherche-du-meilleur-reg_param-et-évaluation",
    "href": "QDA.html#recherche-du-meilleur-reg_param-et-évaluation",
    "title": "Analyse Discriminante Quadratique (QDA) - Multiclasse",
    "section": "Recherche du meilleur reg_param et évaluation",
    "text": "Recherche du meilleur reg_param et évaluation\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n# 🔇 Suppression des avertissements inutiles\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# 🔄 Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# 📊 Préparation des données\nX_train, y_train = train_data.drop('Cover_Type', axis=1), train_data['Cover_Type']\nX_val, y_val = val_data.drop('Cover_Type', axis=1), val_data['Cover_Type']\nX_test, y_test = test_data.drop('Cover_Type', axis=1), test_data['Cover_Type']\n\n# 🔢 Normalisation des données\nscaler = StandardScaler()\nX_train, X_val, X_test = scaler.fit_transform(X_train), scaler.transform(X_val), scaler.transform(X_test)\n\n# 🎯 Recherche du meilleur hyperparamètre reg_param\nreg_params = np.linspace(0, 1, 10)\nval_accuracies = []\n\nfor reg_param in reg_params:\n    qda = QuadraticDiscriminantAnalysis(reg_param=reg_param)\n    qda.fit(X_train, y_train)\n    acc = accuracy_score(y_val, qda.predict(X_val))\n    val_accuracies.append((reg_param, acc))\n\n# Sélection du meilleur hyperparamètre\nbest_reg_param, best_val_acc = max(val_accuracies, key=lambda x: x[1])\n\n# 📈 Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(reg_params, [acc for reg_param, acc in val_accuracies], marker='o', linestyle='dashed', label=\"Validation\")\nplt.xlabel(\"Régularisation (reg_param)\")\nplt.ylabel(\"Précision sur validation\")\nplt.title(\"Impact de la régularisation sur la performance de QDA\")\nplt.legend()\nplt.show()\n\n# 🏆 Modèle final avec le meilleur hyperparamètre\nfinal_model = QuadraticDiscriminantAnalysis(reg_param=best_reg_param)\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# 📊 Matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\n\n# 📈 Calcul des taux de bien classés par classe\nclass_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\noverall_test_accuracy = accuracy_score(y_test, y_test_pred)\n\n# 📝 Affichage des résultats\nprint(f\"\\n🔹 Meilleur hyperparamètre reg_param sur l'échantillon de validation : {best_reg_param:.2f}\")\nprint(f\"Taux de bien classés sur l'échantillon de validation avec cet hyperparamètre : {best_val_acc:.2%}\")\nprint(\"\\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\")\nprint(conf_matrix)\n\nprint(\"\\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre :\")\nfor i, acc in enumerate(class_accuracies, start=1):\n    print(f\"Classe {i} : {acc:.2%}\")\n\nprint(f\"\\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : {overall_test_accuracy:.2%}\")\n\n\n\n\n\n\n\n\n\n🔹 Meilleur hyperparamètre reg_param sur l'échantillon de validation : 0.89\nTaux de bien classés sur l'échantillon de validation avec cet hyperparamètre : 58.73%\n\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\n[[1163  626    1    1   89   14  225]\n [ 650 1561   51   27  402  125   17]\n [   0    7  948  136   50  289    0]\n [   0    0   45   53    0   12    0]\n [  28  103   27   19  191   12    0]\n [  11   34  230   53   31  335    0]\n [ 103   35    3    0   16    0  664]]\n\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre :\nClasse 1 : 54.88%\nClasse 2 : 55.10%\nClasse 3 : 66.29%\nClasse 4 : 48.18%\nClasse 5 : 50.26%\nClasse 6 : 48.27%\nClasse 7 : 80.88%\n\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : 58.60%"
  },
  {
    "objectID": "svm_ovo.html",
    "href": "svm_ovo.html",
    "title": "Support Vector Machines (SVM) - One-Versus-One (OVO)",
    "section": "",
    "text": "Les machines à vecteurs de support (SVM) sont des modèles de classification supervisés qui cherchent à maximiser la marge de séparation entre les classes. Pour un problème multiclasse, l’approche One-Versus-One (OVO) entraîne un SVM pour chaque paire de classes, ce qui permet une meilleure séparation lorsque les classes sont bien distinctes."
  },
  {
    "objectID": "svm_ovo.html#théorie",
    "href": "svm_ovo.html#théorie",
    "title": "Support Vector Machines (SVM) - One-Versus-One (OVO)",
    "section": "",
    "text": "Les machines à vecteurs de support (SVM) sont des modèles de classification supervisés qui cherchent à maximiser la marge de séparation entre les classes. Pour un problème multiclasse, l’approche One-Versus-One (OVO) entraîne un SVM pour chaque paire de classes, ce qui permet une meilleure séparation lorsque les classes sont bien distinctes."
  },
  {
    "objectID": "svm_ovo.html#hyperparamètre-utilisé",
    "href": "svm_ovo.html#hyperparamètre-utilisé",
    "title": "Support Vector Machines (SVM) - One-Versus-One (OVO)",
    "section": "Hyperparamètre utilisé",
    "text": "Hyperparamètre utilisé\nNous allons optimiser :\n\nParamètre de régularisation (C) : contrôle la pénalisation des erreurs de classification et est sélectionné en fonction de la précision sur l’ensemble de validation."
  },
  {
    "objectID": "svm_ovo.html#métriques-dévaluation",
    "href": "svm_ovo.html#métriques-dévaluation",
    "title": "Support Vector Machines (SVM) - One-Versus-One (OVO)",
    "section": "Métriques d’évaluation",
    "text": "Métriques d’évaluation\nNous afficherons :\n\nMatrice de confusion : montrant les erreurs de classification sur l’échantillon de test.\nTaux de bien classés sur l’échantillon de validation avec le meilleur hyperparamètre.\nTaux de bien classés sur l’échantillon de test avec ce même hyperparamètre.\nTaux de bien classés par classe sur l’échantillon de test pour observer la précision sur chaque classe."
  },
  {
    "objectID": "svm_ovo.html#recherche-du-meilleur-c-et-évaluation",
    "href": "svm_ovo.html#recherche-du-meilleur-c-et-évaluation",
    "title": "Support Vector Machines (SVM) - One-Versus-One (OVO)",
    "section": "Recherche du meilleur C et évaluation",
    "text": "Recherche du meilleur C et évaluation\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n# 🔇 Suppression des avertissements inutiles\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# 🔄 Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# 📊 Préparation des données\nX_train, y_train = train_data.drop('Cover_Type', axis=1), train_data['Cover_Type']\nX_val, y_val = val_data.drop('Cover_Type', axis=1), val_data['Cover_Type']\nX_test, y_test = test_data.drop('Cover_Type', axis=1), test_data['Cover_Type']\n\n# 🔢 Normalisation des données\nscaler = StandardScaler()\nX_train, X_val, X_test = scaler.fit_transform(X_train), scaler.transform(X_val), scaler.transform(X_test)\n\n# 🎯 Recherche du meilleur hyperparamètre C\nC_values = np.arange(0.1, 1.1, 0.1)\nval_accuracies = []\n\nfor C in C_values:\n    model = OneVsOneClassifier(SVC(kernel='rbf', C=C))\n    model.fit(X_train, y_train)\n    acc = accuracy_score(y_val, model.predict(X_val))\n    val_accuracies.append((C, acc))\n\n# Sélection du meilleur hyperparamètre\nbest_C, best_val_acc = max(val_accuracies, key=lambda x: x[1])\n\n\n# 📈 Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(C_values, [acc for C, acc in val_accuracies], marker='o', linestyle='dashed', label=\"Validation\")\nplt.xlabel(\"Paramètre de régularisation (C)\")\nplt.ylabel(\"Précision sur validation\")\nplt.title(\"Impact de la régularisation sur la performance du SVM (OVO)\")\nplt.legend()\nplt.show()\n\n# 🏆 Modèle final avec le meilleur hyperparamètre\nfinal_model = OneVsOneClassifier(SVC(kernel='rbf', C=best_C))\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# 📊 Matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\n\n# 📈 Calcul des taux de bien classés par classe\nclass_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\noverall_test_accuracy = accuracy_score(y_test, y_test_pred)\n\n# 📝 Affichage des résultats\nprint(f\"\\n🔹 Meilleur hyperparamètre C sur l'échantillon de validation : {best_C:.2f}\")\nprint(f\"Taux de bien classés sur l'échantillon de validation avec cet hyperparamètre : {best_val_acc:.2%}\")\nprint(\"\\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\")\nprint(conf_matrix)\n\nprint(\"\\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre  :\")\nfor i, acc in enumerate(class_accuracies, start=1):\n    print(f\"Classe {i} : {acc:.2%}\")\n\nprint(f\"\\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : {overall_test_accuracy:.2%}\")\n\n\n\n\n\n\n\n\n\n🔹 Meilleur hyperparamètre C sur l'échantillon de validation : 1.00\nTaux de bien classés sur l'échantillon de validation avec cet hyperparamètre : 72.54%\n\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\n[[1495  519    1    0    5    5   94]\n [ 433 2227   85    1   36   47    4]\n [   0   53 1292   18    4   63    0]\n [   0    0   79   24    0    7    0]\n [   1  212   25    0  131   11    0]\n [   0   45  377    3    1  268    0]\n [ 144    8    0    0    0    0  669]]\n\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre  :\nClasse 1 : 70.55%\nClasse 2 : 78.61%\nClasse 3 : 90.35%\nClasse 4 : 21.82%\nClasse 5 : 34.47%\nClasse 6 : 38.62%\nClasse 7 : 81.49%\n\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : 72.80%"
  },
  {
    "objectID": "arbre_decision.html",
    "href": "arbre_decision.html",
    "title": "Arbre de Décision - CART",
    "section": "",
    "text": "L’algorithme CART (Classification and Regression Trees) est un modèle d’apprentissage supervisé qui construit un arbre de décision en divisant l’espace des caractéristiques en sous-ensembles homogènes."
  },
  {
    "objectID": "arbre_decision.html#théorie",
    "href": "arbre_decision.html#théorie",
    "title": "Arbre de Décision - CART",
    "section": "",
    "text": "L’algorithme CART (Classification and Regression Trees) est un modèle d’apprentissage supervisé qui construit un arbre de décision en divisant l’espace des caractéristiques en sous-ensembles homogènes."
  },
  {
    "objectID": "arbre_decision.html#hyperparamètre-utilisé",
    "href": "arbre_decision.html#hyperparamètre-utilisé",
    "title": "Arbre de Décision - CART",
    "section": "Hyperparamètre utilisé",
    "text": "Hyperparamètre utilisé\nNous allons optimiser :\n\nProfondeur maximale de l’arbre (max_depth) : sélectionnée en fonction de la précision sur l’ensemble de validation."
  },
  {
    "objectID": "arbre_decision.html#métriques-dévaluation",
    "href": "arbre_decision.html#métriques-dévaluation",
    "title": "Arbre de Décision - CART",
    "section": "Métriques d’évaluation",
    "text": "Métriques d’évaluation\nNous afficherons :\n\nMatrice de confusion : récapitulant les erreurs de classification.\nTaux de bien classés sur l’échantillon de validation avec le meilleur hyperparamètre.\nTaux de bien classés sur l’échantillon de test avec ce même hyperparamètre.\nTaux de bien classés par classe sur l’échantillon de test."
  },
  {
    "objectID": "arbre_decision.html#recherche-de-la-meilleure-max_depth-et-évaluation",
    "href": "arbre_decision.html#recherche-de-la-meilleure-max_depth-et-évaluation",
    "title": "Arbre de Décision - CART",
    "section": "Recherche de la meilleure max_depth et évaluation",
    "text": "Recherche de la meilleure max_depth et évaluation\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport warnings\n\n# 🔇 Suppression des avertissements inutiles\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# 🔄 Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# 📊 Préparation des données\nX_train, y_train = train_data.drop('Cover_Type', axis=1), train_data['Cover_Type']\nX_val, y_val = val_data.drop('Cover_Type', axis=1), val_data['Cover_Type']\nX_test, y_test = test_data.drop('Cover_Type', axis=1), test_data['Cover_Type']\n\n# 🎯 Recherche du meilleur hyperparamètre max_depth\ndepth_range = range(1, 30)\nval_accuracies = []\n\nfor depth in depth_range:\n    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n    tree.fit(X_train, y_train)\n    acc = accuracy_score(y_val, tree.predict(X_val))\n    val_accuracies.append((depth, acc))\n\n# Sélection de la meilleure profondeur d'arbre\nbest_depth, best_val_acc = max(val_accuracies, key=lambda x: x[1])\n\n\n# 📈 Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(depth_range, [acc for depth, acc in val_accuracies], marker='o', linestyle='dashed', label=\"Validation\")\nplt.xlabel(\"Profondeur de l'arbre\")\nplt.ylabel(\"Précision sur validation\")\nplt.title(\"Impact de la profondeur de l'arbre sur la performance de CART\")\nplt.legend()\nplt.show()\n\n# 🏆 Modèle final avec la meilleure profondeur\nfinal_model = DecisionTreeClassifier(max_depth=best_depth, random_state=42)\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# 📊 Matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\n\n# 📈 Calcul des taux de bien classés par classe\nclass_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\noverall_test_accuracy = accuracy_score(y_test, y_test_pred)\n\n# 📝 Affichage des résultats\nprint(f\"\\n🔹 Meilleure profondeur de l'arbre sur l’échantillon de validation : {best_depth}\")\nprint(f\"Taux de bien classés sur l’échantillon de validation avec cet hyperparamètre : {best_val_acc:.2%}\")\nprint(\"\\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\")\nprint(conf_matrix)\n\nprint(\"\\n📈 Taux de bien classés par classe sur l’échantillon de test, avec le meilleur hyperparamètre  :\")\nfor i, acc in enumerate(class_accuracies, start=1):\n    print(f\"Classe {i} : {acc:.2%}\")\n\nprint(f\"\\n🔹 Taux de bien classés sur l’échantillon de test avec le meilleur hyperparamètre : {overall_test_accuracy:.2%}\")\n\n\n\n\n\n\n\n\n\n🔹 Meilleure profondeur de l'arbre sur l’échantillon de validation : 25\nTaux de bien classés sur l’échantillon de validation avec cet hyperparamètre : 78.43%\n\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\n[[1581  449    1    0   11    1   76]\n [ 459 2177   43    1   93   52    8]\n [   2   46 1216   24   13  129    0]\n [   0    0   23   74    0   13    0]\n [  10   83   13    0  263   11    0]\n [   4   40  128    4    4  514    0]\n [  61   14    0    0    0    0  746]]\n\n📈 Taux de bien classés par classe sur l’échantillon de test, avec le meilleur hyperparamètre  :\nClasse 1 : 74.61%\nClasse 2 : 76.84%\nClasse 3 : 85.03%\nClasse 4 : 67.27%\nClasse 5 : 69.21%\nClasse 6 : 74.06%\nClasse 7 : 90.86%\n\n🔹 Taux de bien classés sur l’échantillon de test avec le meilleur hyperparamètre : 78.35%"
  },
  {
    "objectID": "svm_ova.html",
    "href": "svm_ova.html",
    "title": "Support Vector Machines (SVM) - One-Versus-All (OVA)",
    "section": "",
    "text": "Les machines à vecteurs de support (SVM) sont des modèles de classification supervisés qui cherchent à maximiser la marge de séparation entre les classes. Pour un problème multiclasse, l’approche One-Versus-All (OVA) entraîne un SVM pour chaque classe, la comparant à toutes les autres classes combinées."
  },
  {
    "objectID": "svm_ova.html#théorie",
    "href": "svm_ova.html#théorie",
    "title": "Support Vector Machines (SVM) - One-Versus-All (OVA)",
    "section": "",
    "text": "Les machines à vecteurs de support (SVM) sont des modèles de classification supervisés qui cherchent à maximiser la marge de séparation entre les classes. Pour un problème multiclasse, l’approche One-Versus-All (OVA) entraîne un SVM pour chaque classe, la comparant à toutes les autres classes combinées."
  },
  {
    "objectID": "svm_ova.html#hyperparamètre-utilisé",
    "href": "svm_ova.html#hyperparamètre-utilisé",
    "title": "Support Vector Machines (SVM) - One-Versus-All (OVA)",
    "section": "Hyperparamètre utilisé",
    "text": "Hyperparamètre utilisé\nNous allons optimiser :\n\nParamètre de régularisation (C) : contrôle la pénalisation des erreurs de classification et est sélectionné en fonction de la précision sur l’ensemble de validation."
  },
  {
    "objectID": "svm_ova.html#métriques-dévaluation",
    "href": "svm_ova.html#métriques-dévaluation",
    "title": "Support Vector Machines (SVM) - One-Versus-All (OVA)",
    "section": "Métriques d’évaluation",
    "text": "Métriques d’évaluation\nNous afficherons :\n\nMatrice de confusion : montrant les erreurs de classification sur l’échantillon de test.\nTaux de bien classés sur l’échantillon de validation avec le meilleur hyperparamètre.\nTaux de bien classés sur l’échantillon de test avec ce même hyperparamètre.\nTaux de bien classés par classe sur l’échantillon de test pour observer la précision sur chaque classe."
  },
  {
    "objectID": "svm_ova.html#recherche-du-meilleur-c-et-évaluation",
    "href": "svm_ova.html#recherche-du-meilleur-c-et-évaluation",
    "title": "Support Vector Machines (SVM) - One-Versus-All (OVA)",
    "section": "Recherche du meilleur C et évaluation",
    "text": "Recherche du meilleur C et évaluation\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n# 🔇 Suppression des avertissements inutiles\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# 🔄 Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# 📊 Préparation des données\nX_train, y_train = train_data.drop('Cover_Type', axis=1), train_data['Cover_Type']\nX_val, y_val = val_data.drop('Cover_Type', axis=1), val_data['Cover_Type']\nX_test, y_test = test_data.drop('Cover_Type', axis=1), test_data['Cover_Type']\n\n# 🔢 Normalisation des données\nscaler = StandardScaler()\nX_train, X_val, X_test = scaler.fit_transform(X_train), scaler.transform(X_val), scaler.transform(X_test)\n\n# 🎯 Recherche du meilleur hyperparamètre C\nC_values = np.arange(0.1, 1.1, 0.1)\nval_accuracies = []\n\nfor C in C_values:\n    model = OneVsRestClassifier(SVC(kernel='rbf', C=C))\n    model.fit(X_train, y_train)\n    acc = accuracy_score(y_val, model.predict(X_val))\n    val_accuracies.append((C, acc))\n\n# Sélection du meilleur hyperparamètre\nbest_C, best_val_acc = max(val_accuracies, key=lambda x: x[1])\n\n\n# 📈 Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(C_values, [acc for C, acc in val_accuracies], marker='o', linestyle='dashed', label=\"Validation\")\nplt.xlabel(\"Paramètre de régularisation (C)\")\nplt.ylabel(\"Précision sur validation\")\nplt.title(\"Impact de la régularisation sur la performance du SVM (OVA)\")\nplt.legend()\nplt.show()\n\n# 🏆 Modèle final avec le meilleur hyperparamètre\nfinal_model = OneVsRestClassifier(SVC(kernel='rbf', C=best_C))\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# 📊 Matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\n\n# 📈 Calcul des taux de bien classés par classe\nclass_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\noverall_test_accuracy = accuracy_score(y_test, y_test_pred)\n\n# 📝 Affichage des résultats\nprint(f\"\\n🔹 Meilleur hyperparamètre C sur l'échantillon de validation : {best_C:.2f}\")\nprint(f\"Taux de bien classés sur l'échantillon de validation avec cet hyperparamètre : {best_val_acc:.2%}\")\nprint(\"\\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\")\nprint(conf_matrix)\n\nprint(\"\\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre  :\")\nfor i, acc in enumerate(class_accuracies, start=1):\n    print(f\"Classe {i} : {acc:.2%}\")\n\nprint(f\"\\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : {overall_test_accuracy:.2%}\")\n\n\n\n\n\n\n\n\n\n🔹 Meilleur hyperparamètre C sur l'échantillon de validation : 1.00\nTaux de bien classés sur l'échantillon de validation avec cet hyperparamètre : 71.88%\n\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\n[[1447  551    1    0    7    6  107]\n [ 424 2226   83    1   32   58    9]\n [   0   50 1296   18    4   62    0]\n [   0    0   81   22    0    7    0]\n [   5  202   30    0  126   17    0]\n [   0   47  378    3    0  266    0]\n [ 140    5    0    0    2    0  674]]\n\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre  :\nClasse 1 : 68.29%\nClasse 2 : 78.57%\nClasse 3 : 90.63%\nClasse 4 : 20.00%\nClasse 5 : 33.16%\nClasse 6 : 38.33%\nClasse 7 : 82.10%\n\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : 72.22%"
  },
  {
    "objectID": "bayesien_naif.html",
    "href": "bayesien_naif.html",
    "title": "Classifieur Bayésien Naïf",
    "section": "",
    "text": "Le classificateur Bayésien Naïf repose sur le théorème de Bayes et l’hypothèse d’indépendance conditionnelle entre les variables explicatives. Il est particulièrement efficace pour les problèmes de classification textuelle et fonctionne bien même avec peu de données d’entraînement."
  },
  {
    "objectID": "bayesien_naif.html#théorie",
    "href": "bayesien_naif.html#théorie",
    "title": "Classifieur Bayésien Naïf",
    "section": "",
    "text": "Le classificateur Bayésien Naïf repose sur le théorème de Bayes et l’hypothèse d’indépendance conditionnelle entre les variables explicatives. Il est particulièrement efficace pour les problèmes de classification textuelle et fonctionne bien même avec peu de données d’entraînement."
  },
  {
    "objectID": "bayesien_naif.html#hyperparamètre-utilisé",
    "href": "bayesien_naif.html#hyperparamètre-utilisé",
    "title": "Classifieur Bayésien Naïf",
    "section": "Hyperparamètre utilisé",
    "text": "Hyperparamètre utilisé\nNous allons optimiser :\n\nvar_smoothing : Ce paramètre permet d’ajouter un lissage aux variances estimées pour éviter les divisions par zéro. Il est sélectionné en fonction de la précision sur l’ensemble de validation."
  },
  {
    "objectID": "bayesien_naif.html#métriques-dévaluation",
    "href": "bayesien_naif.html#métriques-dévaluation",
    "title": "Classifieur Bayésien Naïf",
    "section": "Métriques d’évaluation",
    "text": "Métriques d’évaluation\nNous afficherons :\n\nMatrice de confusion : montrant les erreurs de classification sur l’échantillon de test.\nTaux de bien classés sur l’échantillon de validation avec le meilleur hyperparamètre.\nTaux de bien classés sur l’échantillon de test avec ce même hyperparamètre.\nTaux de bien classés par classe sur l’échantillon de test pour observer la précision sur chaque classe."
  },
  {
    "objectID": "bayesien_naif.html#recherche-du-meilleur-var_smoothing-et-évaluation",
    "href": "bayesien_naif.html#recherche-du-meilleur-var_smoothing-et-évaluation",
    "title": "Classifieur Bayésien Naïf",
    "section": "Recherche du meilleur var_smoothing et évaluation",
    "text": "Recherche du meilleur var_smoothing et évaluation\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport warnings\n\n# 🔇 Suppression des avertissements inutiles\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# 🔄 Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# 📊 Préparation des données\nX_train, y_train = train_data.drop('Cover_Type', axis=1), train_data['Cover_Type']\nX_val, y_val = val_data.drop('Cover_Type', axis=1), val_data['Cover_Type']\nX_test, y_test = test_data.drop('Cover_Type', axis=1), test_data['Cover_Type']\n\n# 🎯 Recherche du meilleur hyperparamètre var_smoothing\nvar_smoothing_values = np.logspace(-9, 0, 10)\nval_accuracies = []\n\nfor smoothing in var_smoothing_values:\n    gnb = GaussianNB(var_smoothing=smoothing)\n    gnb.fit(X_train, y_train)\n    acc = accuracy_score(y_val, gnb.predict(X_val))\n    val_accuracies.append((smoothing, acc))\n\n# Sélection du meilleur hyperparamètre\nbest_smoothing, best_val_acc = max(val_accuracies, key=lambda x: x[1])\n\n\n# 📈 Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(var_smoothing_values, [acc for smoothing, acc in val_accuracies], marker='o', linestyle='dashed', label=\"Validation\")\nplt.xscale('log')\nplt.xlabel(\"Valeur de var_smoothing\")\nplt.ylabel(\"Précision sur validation\")\nplt.title(\"Optimisation du paramètre var_smoothing pour GaussianNB\")\nplt.legend()\nplt.show()\n\n# 🏆 Modèle final avec le meilleur hyperparamètre\nfinal_model = GaussianNB(var_smoothing=best_smoothing)\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# 📊 Matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\n\n# 📈 Calcul des taux de bien classés par classe\nclass_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\noverall_test_accuracy = accuracy_score(y_test, y_test_pred)\n\n# 📝 Affichage des résultats\nprint(f\"\\n🔹 Meilleur var_smoothing sur l'échantillon de validation : {best_smoothing:.1e}\")\nprint(f\"Taux de bien classés sur l'échantillon de validation avec cet hyperparamètre : {best_val_acc:.2%}\")\nprint(\"\\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\")\nprint(conf_matrix)\n\nprint(\"\\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre  :\")\nfor i, acc in enumerate(class_accuracies, start=1):\n    print(f\"Classe {i} : {acc:.2%}\")\n\nprint(f\"\\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : {overall_test_accuracy:.2%}\")\n\n\n\n\n\n\n\n\n\n🔹 Meilleur var_smoothing sur l'échantillon de validation : 1.0e-07\nTaux de bien classés sur l'échantillon de validation avec cet hyperparamètre : 61.23%\n\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\n[[1300  475   12    0   57    9  266]\n [ 615 1732  101    1  222   89   73]\n [   0   72  939  151   72  196    0]\n [   0    0   33   66    0   11    0]\n [   2  179    4    0  176   19    0]\n [   0   66  271   34   12  311    0]\n [ 162    2    3    0    3    0  651]]\n\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre  :\nClasse 1 : 61.35%\nClasse 2 : 61.14%\nClasse 3 : 65.66%\nClasse 4 : 60.00%\nClasse 5 : 46.32%\nClasse 6 : 44.81%\nClasse 7 : 79.29%\n\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : 61.70%"
  },
  {
    "objectID": "LDA.html",
    "href": "LDA.html",
    "title": "Analyse Discriminante Linéaire (LDA) - Multiclasse",
    "section": "",
    "text": "L’Analyse Discriminante Linéaire (LDA) est une technique de classification qui cherche à trouver une combinaison linéaire de caractéristiques maximisant la séparation entre plusieurs classes.\nContrairement à d’autres modèles initialement conçus pour des problèmes binaires et adaptés aux cas multiclasse via OVA ou OVO, LDA est intrinsèquement multiclasse. Il attribue directement une observation à l’une des classes disponibles en estimant des distributions normales multivariées et en utilisant la règle de Bayes."
  },
  {
    "objectID": "LDA.html#théorie",
    "href": "LDA.html#théorie",
    "title": "Analyse Discriminante Linéaire (LDA) - Multiclasse",
    "section": "",
    "text": "L’Analyse Discriminante Linéaire (LDA) est une technique de classification qui cherche à trouver une combinaison linéaire de caractéristiques maximisant la séparation entre plusieurs classes.\nContrairement à d’autres modèles initialement conçus pour des problèmes binaires et adaptés aux cas multiclasse via OVA ou OVO, LDA est intrinsèquement multiclasse. Il attribue directement une observation à l’une des classes disponibles en estimant des distributions normales multivariées et en utilisant la règle de Bayes."
  },
  {
    "objectID": "LDA.html#hyperparamètre-utilisé",
    "href": "LDA.html#hyperparamètre-utilisé",
    "title": "Analyse Discriminante Linéaire (LDA) - Multiclasse",
    "section": "Hyperparamètre utilisé",
    "text": "Hyperparamètre utilisé\nNous allons optimiser :\n\nRégularisation (shrinkage) : contrôle la variance de la covariance estimée et est sélectionnée en fonction de la précision sur l’ensemble de validation."
  },
  {
    "objectID": "LDA.html#métriques-dévaluation",
    "href": "LDA.html#métriques-dévaluation",
    "title": "Analyse Discriminante Linéaire (LDA) - Multiclasse",
    "section": "Métriques d’évaluation",
    "text": "Métriques d’évaluation\nNous afficherons :\n\nMatrice de confusion : montrant les erreurs de classification sur l’échantillon de test.\nTaux de bien classés sur l’échantillon de validation avec le meilleur hyperparamètre.\nTaux de bien classés sur l’échantillon de test avec ce même hyperparamètre.\nTaux de bien classés par classe sur l’échantillon de test pour observer la précision sur chaque classe."
  },
  {
    "objectID": "LDA.html#recherche-du-meilleur-shrinkage-et-évaluation",
    "href": "LDA.html#recherche-du-meilleur-shrinkage-et-évaluation",
    "title": "Analyse Discriminante Linéaire (LDA) - Multiclasse",
    "section": "Recherche du meilleur shrinkage et évaluation",
    "text": "Recherche du meilleur shrinkage et évaluation\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n# 🔇 Suppression des avertissements inutiles\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# 🔄 Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# 📊 Préparation des données\nX_train, y_train = train_data.drop('Cover_Type', axis=1), train_data['Cover_Type']\nX_val, y_val = val_data.drop('Cover_Type', axis=1), val_data['Cover_Type']\nX_test, y_test = test_data.drop('Cover_Type', axis=1), test_data['Cover_Type']\n\n# 🔢 Normalisation des données\nscaler = StandardScaler()\nX_train, X_val, X_test = scaler.fit_transform(X_train), scaler.transform(X_val), scaler.transform(X_test)\n\n# 🎯 Recherche du meilleur hyperparamètre shrinkage\nshrinkage_values = np.linspace(0, 1, 10)\nval_accuracies = []\n\nfor shrinkage in shrinkage_values:\n    lda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage=shrinkage)\n    lda.fit(X_train, y_train)\n    acc = accuracy_score(y_val, lda.predict(X_val))\n    val_accuracies.append((shrinkage, acc))\n\n# Sélection du meilleur hyperparamètre\nbest_shrinkage, best_val_acc = max(val_accuracies, key=lambda x: x[1])\n\n# 📈 Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(shrinkage_values, [acc for shrinkage, acc in val_accuracies], marker='o', linestyle='dashed', label=\"Validation\")\nplt.xlabel(\"Shrinkage\")\nplt.ylabel(\"Précision sur validation\")\nplt.title(\"Impact du shrinkage sur la performance de LDA\")\nplt.legend()\nplt.show()\n\n# 🏆 Modèle final avec le meilleur hyperparamètre\nfinal_model = LinearDiscriminantAnalysis(solver='lsqr', shrinkage=best_shrinkage)\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# 📊 Matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\n\n# 📈 Calcul des taux de bien classés par classe\nclass_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\noverall_test_accuracy = accuracy_score(y_test, y_test_pred)\n\n# 📝 Affichage des résultats\nprint(f\"\\n🔹 Meilleur hyperparamètre shrinkage sur l'échantillon de validation : {best_shrinkage:.2f}\")\nprint(f\"Taux de bien classés sur l'échantillon de validation avec cet hyperparamètre : {best_val_acc:.2%}\")\nprint(\"\\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\")\nprint(conf_matrix)\n\nprint(\"\\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre :\")\nfor i, acc in enumerate(class_accuracies, start=1):\n    print(f\"Classe {i} : {acc:.2%}\")\n\nprint(f\"\\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : {overall_test_accuracy:.2%}\")\n\n\n\n\n\n\n\n\n\n🔹 Meilleur hyperparamètre shrinkage sur l'échantillon de validation : 0.00\nTaux de bien classés sur l'échantillon de validation avec cet hyperparamètre : 63.37%\n\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\n[[1344  520    1    0   27    2  225]\n [ 548 1865   48   17  218  124   13]\n [   0   29  902  130   32  337    0]\n [   0    0   44   53    0   13    0]\n [   2  160   27    0  179   12    0]\n [   0   53  210   25   42  364    0]\n [ 154    0    3    0    0    0  664]]\n\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre :\nClasse 1 : 63.43%\nClasse 2 : 65.83%\nClasse 3 : 63.08%\nClasse 4 : 48.18%\nClasse 5 : 47.11%\nClasse 6 : 52.45%\nClasse 7 : 80.88%\n\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : 64.04%"
  },
  {
    "objectID": "foret_aleatoire.html",
    "href": "foret_aleatoire.html",
    "title": "Random Forest - Forêt Aléatoire",
    "section": "",
    "text": "La forêt aléatoire est un algorithme d’apprentissage supervisé basé sur un ensemble d’arbres de décision. Il fonctionne en combinant plusieurs arbres pour améliorer la précision et réduire le risque de surapprentissage."
  },
  {
    "objectID": "foret_aleatoire.html#théorie",
    "href": "foret_aleatoire.html#théorie",
    "title": "Random Forest - Forêt Aléatoire",
    "section": "",
    "text": "La forêt aléatoire est un algorithme d’apprentissage supervisé basé sur un ensemble d’arbres de décision. Il fonctionne en combinant plusieurs arbres pour améliorer la précision et réduire le risque de surapprentissage."
  },
  {
    "objectID": "foret_aleatoire.html#hyperparamètre-utilisé",
    "href": "foret_aleatoire.html#hyperparamètre-utilisé",
    "title": "Random Forest - Forêt Aléatoire",
    "section": "Hyperparamètre utilisé",
    "text": "Hyperparamètre utilisé\nNous allons optimiser :\n\nNombre d’arbres (n_estimators) : sélectionné en fonction de la précision sur l’ensemble de validation."
  },
  {
    "objectID": "foret_aleatoire.html#métriques-dévaluation",
    "href": "foret_aleatoire.html#métriques-dévaluation",
    "title": "Random Forest - Forêt Aléatoire",
    "section": "Métriques d’évaluation",
    "text": "Métriques d’évaluation\nNous afficherons :\n\nMatrice de confusion : récapitulant les erreurs de classification.\nTaux de bien classés sur l’échantillon de validation avec le meilleur hyperparamètre.\nTaux de bien classés sur l’échantillon de test avec ce même hyperparamètre.\nTaux de bien classés par classe sur l’échantillon de test."
  },
  {
    "objectID": "foret_aleatoire.html#recherche-du-meilleur-n_estimators-et-évaluation",
    "href": "foret_aleatoire.html#recherche-du-meilleur-n_estimators-et-évaluation",
    "title": "Random Forest - Forêt Aléatoire",
    "section": "Recherche du meilleur n_estimators et évaluation",
    "text": "Recherche du meilleur n_estimators et évaluation\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport warnings\n\n# 🔇 Suppression des avertissements inutiles\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# 🔄 Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# 📊 Préparation des données\nX_train, y_train = train_data.drop('Cover_Type', axis=1), train_data['Cover_Type']\nX_val, y_val = val_data.drop('Cover_Type', axis=1), val_data['Cover_Type']\nX_test, y_test = test_data.drop('Cover_Type', axis=1), test_data['Cover_Type']\n\n# 🎯 Recherche du meilleur hyperparamètre n_estimators\nn_estimators_range = range(50, 1000, 50)\nval_accuracies = []\n\nfor n in n_estimators_range:\n    rf = RandomForestClassifier(n_estimators=n, random_state=42, n_jobs=-1)\n    rf.fit(X_train, y_train)\n    acc = accuracy_score(y_val, rf.predict(X_val))\n    val_accuracies.append((n, acc))\n\n# Sélection du meilleur nombre d'arbres\nbest_n, best_val_acc = max(val_accuracies, key=lambda x: x[1])\n\n\n# 📈 Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(n_estimators_range, [acc for n, acc in val_accuracies], marker='o', linestyle='dashed', label=\"Validation\")\nplt.xlabel(\"Nombre d'arbres\")\nplt.ylabel(\"Précision sur validation\")\nplt.title(\"Impact du nombre d'arbres sur la performance de Random Forest\")\nplt.legend()\nplt.show()\n\n# 🏆 Modèle final avec le meilleur nombre d'arbres\nfinal_model = RandomForestClassifier(n_estimators=best_n, random_state=42, n_jobs=-1)\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# 📊 Matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\n\n# 📈 Calcul des taux de bien classés par classe\nclass_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\noverall_test_accuracy = accuracy_score(y_test, y_test_pred)\n\n# 📝 Affichage des résultats\nprint(f\"\\n🔹 Meilleur nombre d'arbres sur l’échantillon de validation : {best_n}\")\nprint(f\"Taux de bien classés sur l’échantillon de validation avec cet hyperparamètre : {best_val_acc:.2%}\")\nprint(\"\\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\")\nprint(conf_matrix)\n\nprint(\"\\n📈 Taux de bien classés par classe sur l’échantillon de test, avec le meilleur hyperparamètre  :\")\nfor i, acc in enumerate(class_accuracies, start=1):\n    print(f\"Classe {i} : {acc:.2%}\")\n\nprint(f\"\\n🔹 Taux de bien classés sur l’échantillon de test avec le meilleur hyperparamètre : {overall_test_accuracy:.2%}\")\n\n\n\n\n\n\n\n\n\n🔹 Meilleur nombre d'arbres sur l’échantillon de validation : 450\nTaux de bien classés sur l’échantillon de validation avec cet hyperparamètre : 86.25%\n\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\n[[1753  312    0    0    5    2   47]\n [ 247 2490   40    1   20   31    4]\n [   0   15 1352   11    1   51    0]\n [   0    0   37   71    0    2    0]\n [   3   86   16    0  273    2    0]\n [   1   17  128    2    0  546    0]\n [  43    4    0    0    0    0  774]]\n\n📈 Taux de bien classés par classe sur l’échantillon de test, avec le meilleur hyperparamètre  :\nClasse 1 : 82.73%\nClasse 2 : 87.89%\nClasse 3 : 94.55%\nClasse 4 : 64.55%\nClasse 5 : 71.84%\nClasse 6 : 78.67%\nClasse 7 : 94.28%\n\n🔹 Taux de bien classés sur l’échantillon de test avec le meilleur hyperparamètre : 86.55%"
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "Présentation des techniques de classification supervisée utilisées",
    "section": "",
    "text": "En classification supervisée, les algorithmes peuvent être divisés en méthodes paramétriques et méthodes non-paramétriques. Cette distinction repose sur la manière dont les modèles apprennent et généralisent les données.\n\n\nLes modèles paramétriques supposent l’existence d’une distribution sous-jacente aux données et estiment ses paramètres à partir des données d’entrainement.\n\n\n\nRégression Logistique (OVA et OVO)\nRégression Multinomiale (Softmax Regression)\nAnalyse Discriminante Linéaire (LDA)\nAnalyse Discriminante Quadratique (QDA)\nClassifieur Bayésien Naïf\n\n\n\n\n\nLes modèles non-paramétriques ne supposent pas l’existence d’une distribution sous-jacente aux données. Ils estiment les limites entre les classes de façon “gloutonne” en maximisant les différences inter-classes et minisant les différences intra-classes sur les données d’entrainement.\n\n\n\nArbre de Décision (CART)\nForêt Aléatoire (Random Forest)\nK-Nearest Neighbors (KNN)\nMachines à Vecteurs de Support (SVM - OVA et OVO)\nRéseau de Neurones (MLP avec sortie Softmax)"
  },
  {
    "objectID": "presentation.html#classification-paramétrique-vs-non-paramétrique",
    "href": "presentation.html#classification-paramétrique-vs-non-paramétrique",
    "title": "Présentation des techniques de classification supervisée utilisées",
    "section": "",
    "text": "En classification supervisée, les algorithmes peuvent être divisés en méthodes paramétriques et méthodes non-paramétriques. Cette distinction repose sur la manière dont les modèles apprennent et généralisent les données.\n\n\nLes modèles paramétriques supposent l’existence d’une distribution sous-jacente aux données et estiment ses paramètres à partir des données d’entrainement.\n\n\n\nRégression Logistique (OVA et OVO)\nRégression Multinomiale (Softmax Regression)\nAnalyse Discriminante Linéaire (LDA)\nAnalyse Discriminante Quadratique (QDA)\nClassifieur Bayésien Naïf\n\n\n\n\n\nLes modèles non-paramétriques ne supposent pas l’existence d’une distribution sous-jacente aux données. Ils estiment les limites entre les classes de façon “gloutonne” en maximisant les différences inter-classes et minisant les différences intra-classes sur les données d’entrainement.\n\n\n\nArbre de Décision (CART)\nForêt Aléatoire (Random Forest)\nK-Nearest Neighbors (KNN)\nMachines à Vecteurs de Support (SVM - OVA et OVO)\nRéseau de Neurones (MLP avec sortie Softmax)"
  },
  {
    "objectID": "presentation.html#classifieurs-multiclasses-natifs-vs-classifieurs-binaires-adaptés",
    "href": "presentation.html#classifieurs-multiclasses-natifs-vs-classifieurs-binaires-adaptés",
    "title": "Présentation des techniques de classification supervisée utilisées",
    "section": "Classifieurs Multiclasses Natifs vs Classifieurs Binaires Adaptés",
    "text": "Classifieurs Multiclasses Natifs vs Classifieurs Binaires Adaptés\nCertains classifieurs sont conçus pour gérer directement plusieurs classes (classifieurs multiclasses natifs), tandis que d’autres sont conçus pour distinguer uniquement deux classes et doivent être adaptés pour des problèmes multiclasses.\n\n1. Classifieurs Multiclasses Natifs\nCes classifieurs peuvent traiter directement un problème à plusieurs classes sans nécessiter de transformation :\n\nRégression Multinomiale (Softmax Regression)\nAnalyse Discriminante Linéaire (LDA)\nAnalyse Discriminante Quadratique (QDA)\nClassifieur Bayésien Naïf\nArbre de Décision (CART)\nForêt Aléatoire (Random Forest)\nRéseau de Neurones (MLP avec sortie Softmax)\nK-Nearest Neighbors (KNN)\n\n\n\n2. Classifieurs Binaires Adaptés\nLes classifieurs binaires doivent être transformés pour gérer plusieurs classes en utilisant des approches comme One-Versus-All (OVA) ou One-Versus-One (OVO) :\n\nRégression Logistique (adaptée en OVA et OVO)\nSupport Vector Machines (SVM) (adaptées en OVA et OVO)"
  },
  {
    "objectID": "presentation.html#one-versus-all-ova-vs-one-versus-one-ovo",
    "href": "presentation.html#one-versus-all-ova-vs-one-versus-one-ovo",
    "title": "Présentation des techniques de classification supervisée utilisées",
    "section": "One-Versus-All (OVA) vs One-Versus-One (OVO)",
    "text": "One-Versus-All (OVA) vs One-Versus-One (OVO)\nLes approches OVA et OVO permettent d’adapter des classifieurs binaires aux problèmes multiclasses.\n\n1. One-Versus-All (OVA)\nAvec cette approche, un modèle binaire est entraîné pour chaque classe en la comparant à toutes les autres classes regroupées. Pour une classification à N classes, N modèles binaires sont entraînés.\nAvantages : - Moins de modèles à entraîner (N contre N(N-1)/2 en OVO), soit 7 modèles pour 7 classes. - Plus efficace pour les jeux de données avec un grand nombre de classes.\nInconvénients : - Les classes déséquilibrées peuvent poser problème car le modèle doit distinguer une classe contre un ensemble de classes plus nombreuses. - Peut générer des scores de confiance biaisés si les classes sont très déséquilibrées.\nExemples de classifieurs utilisant OVA :\n\nRégression Logistique (OVA)\nSVM (OVA)\n\n\n\n2. One-Versus-One (OVO)\nAvec l’approche OVO, un modèle est entraîné pour chaque paire de classes. Pour une classification à N classes, N(N-1)/2 modèles binaires sont entraînés, soit 21 modèles pour 7 classes.\nAvantages : - Meilleure séparation entre classes si elles sont bien distinctes. - Moins sensible aux classes déséquilibrées car chaque modèle compare seulement deux classes à la fois.\nInconvénients : - Temps d’entraînement plus long à cause du grand nombre de modèles. - Peut nécessiter plus de ressources pour l’inférence.\nExemples de classifieurs utilisant OVO :\n\nRégression Logistique (OVO)\nSVM (OVO)"
  },
  {
    "objectID": "regression_logistique_ovo.html",
    "href": "regression_logistique_ovo.html",
    "title": "Régression Logistique Binomiale - One-Versus-One (OVO)",
    "section": "",
    "text": "La régression logistique binomiale est utilisée pour la classification binaire, mais elle peut être adaptée aux problèmes multiclasse via l’approche One-Versus-One (OVO). Ici, un modèle est entraîné pour chaque paire de classes, ce qui peut améliorer la précision lorsque les classes sont bien séparées."
  },
  {
    "objectID": "regression_logistique_ovo.html#théorie",
    "href": "regression_logistique_ovo.html#théorie",
    "title": "Régression Logistique Binomiale - One-Versus-One (OVO)",
    "section": "",
    "text": "La régression logistique binomiale est utilisée pour la classification binaire, mais elle peut être adaptée aux problèmes multiclasse via l’approche One-Versus-One (OVO). Ici, un modèle est entraîné pour chaque paire de classes, ce qui peut améliorer la précision lorsque les classes sont bien séparées."
  },
  {
    "objectID": "regression_logistique_ovo.html#hyperparamètre-utilisé",
    "href": "regression_logistique_ovo.html#hyperparamètre-utilisé",
    "title": "Régression Logistique Binomiale - One-Versus-One (OVO)",
    "section": "Hyperparamètre utilisé",
    "text": "Hyperparamètre utilisé\nNous allons optimiser :\n\nParamètre de régularisation (C) : qui contrôle la complexité du modèle et est sélectionné en fonction de la précision sur l’ensemble de validation."
  },
  {
    "objectID": "regression_logistique_ovo.html#métriques-dévaluation",
    "href": "regression_logistique_ovo.html#métriques-dévaluation",
    "title": "Régression Logistique Binomiale - One-Versus-One (OVO)",
    "section": "Métriques d’évaluation",
    "text": "Métriques d’évaluation\nNous afficherons :\n\nMatrice de confusion : montrant les erreurs de classification sur l’échantillon de test.\nTaux de bien classés sur l’échantillon de validation avec le meilleur hyperparamètre.\nTaux de bien classés sur l’échantillon de test avec ce même hyperparamètre.\nTaux de bien classés par classe sur l’échantillon de test pour observer la précision sur chaque classe."
  },
  {
    "objectID": "regression_logistique_ovo.html#recherche-du-meilleur-c-et-évaluation",
    "href": "regression_logistique_ovo.html#recherche-du-meilleur-c-et-évaluation",
    "title": "Régression Logistique Binomiale - One-Versus-One (OVO)",
    "section": "Recherche du meilleur C et évaluation",
    "text": "Recherche du meilleur C et évaluation\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n# 🔇 Suppression des avertissements inutiles\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# 🔄 Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# 📊 Préparation des données\nX_train, y_train = train_data.drop('Cover_Type', axis=1), train_data['Cover_Type']\nX_val, y_val = val_data.drop('Cover_Type', axis=1), val_data['Cover_Type']\nX_test, y_test = test_data.drop('Cover_Type', axis=1), test_data['Cover_Type']\n\n# 🔢 Normalisation des données\nscaler = StandardScaler()\nX_train, X_val, X_test = scaler.fit_transform(X_train), scaler.transform(X_val), scaler.transform(X_test)\n\n# 🎯 Recherche du meilleur hyperparamètre C\nC_values = np.arange(0.1, 1.1, 0.1)\nval_accuracies = []\n\nfor C in C_values:\n    model = OneVsOneClassifier(LogisticRegression(solver='saga', C=C, penalty='l2', max_iter=500))\n    model.fit(X_train, y_train)\n    acc = accuracy_score(y_val, model.predict(X_val))\n    val_accuracies.append((C, acc))\n\n# Sélection du meilleur hyperparamètre\nbest_C, best_val_acc = max(val_accuracies, key=lambda x: x[1])\n\n\n# 📈 Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(C_values, [acc for C, acc in val_accuracies], marker='o', linestyle='dashed', label=\"Validation\")\nplt.xlabel(\"Paramètre de régularisation (C)\")\nplt.ylabel(\"Précision sur validation\")\nplt.title(\"Impact de la régularisation sur la performance de la régression logistique (OVO)\")\nplt.legend()\nplt.show()\n\n# 🏆 Modèle final avec le meilleur hyperparamètre\nfinal_model = OneVsOneClassifier(LogisticRegression(solver='saga', C=best_C, penalty='l2', max_iter=500))\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# 📊 Matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\n\n# 📈 Calcul des taux de bien classés par classe\nclass_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\noverall_test_accuracy = accuracy_score(y_test, y_test_pred)\n\n# 📝 Affichage des résultats\nprint(f\"\\n🔹 Meilleur hyperparamètre C sur l'échantillon de validation : {best_C:.2f}\")\nprint(f\"Taux de bien classés sur l'échantillon de validation avec cet hyperparamètre : {best_val_acc:.2%}\")\nprint(\"\\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\")\nprint(conf_matrix)\n\nprint(\"\\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre  :\")\nfor i, acc in enumerate(class_accuracies, start=1):\n    print(f\"Classe {i} : {acc:.2%}\")\n\nprint(f\"\\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : {overall_test_accuracy:.2%}\")\n\n\n\n\n\n\n\n\n\n🔹 Meilleur hyperparamètre C sur l'échantillon de validation : 0.30\nTaux de bien classés sur l'échantillon de validation avec cet hyperparamètre : 69.53%\n\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\n[[1419  584    1    0    6    1  108]\n [ 517 2136   85    1   36   50    8]\n [   0   44 1241   35    4  106    0]\n [   0    0   56   38    0   16    0]\n [   2  254   21    0   98    5    0]\n [   0   53  359    2    2  278    0]\n [ 158    1    2    0    0    0  660]]\n\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre  :\nClasse 1 : 66.97%\nClasse 2 : 75.40%\nClasse 3 : 86.78%\nClasse 4 : 34.55%\nClasse 5 : 25.79%\nClasse 6 : 40.06%\nClasse 7 : 80.39%\n\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : 69.99%"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bienvenue !",
    "section": "",
    "text": "Ce cours explore différentes méthodes de Machine Learning pour la classification supervisée, c’est-à-dire l’affectation d’individus à des catégories discrètes (targets) en fonction de variables explicatives (features), qu’elles soient continues ou catégorielles.\nL’objectif est de comparer les performances de ces méthodes en évaluant :"
  },
  {
    "objectID": "index.html#le-dataset-utilisé-covertype",
    "href": "index.html#le-dataset-utilisé-covertype",
    "title": "Bienvenue !",
    "section": "Le dataset utilisé : Covertype",
    "text": "Le dataset utilisé : Covertype\nNous utilisons le dataset Covertype, issu de l’UCI Machine Learning Repository. Ce jeu de données comprend :\n- 581 012 observations\n- 54 variables explicatives\n- 7 classes différentes représentant des types de couvertures forestières\nLes classes sont fortement déséquilibrées numériquement, ce qui constitue un défi supplémentaire pour les modèles de classification."
  },
  {
    "objectID": "index.html#objectifs-du-cours",
    "href": "index.html#objectifs-du-cours",
    "title": "Bienvenue !",
    "section": "Objectifs du cours",
    "text": "Objectifs du cours\nNous expérimenterons plusieurs approches, notamment :\n- Méthodes paramétriques : régression logistique, LDA, QDA…\n- Méthodes non-paramétriques : KNN, arbres de décision, forêts aléatoires…\n- Modèles à base de réseaux de neurones\n- Stratégies One-Versus-All (OVA) et One-Versus-One (OVO) pour les modèles binaires\nCe cours vise ainsi à comparer l’efficacité de ces différentes méthodes et à identifier celles qui offrent les meilleurs compromis en termes de précision, robustesse et adaptation aux déséquilibres de classe."
  },
  {
    "objectID": "KNN.html",
    "href": "KNN.html",
    "title": "KNN - K-Nearest Neighbors",
    "section": "",
    "text": "Le K-Nearest Neighbors (KNN) est un algorithme de classification basé sur la proximité des données dans un espace multidimensionnel. Il attribue une classe à un point en fonction des K voisins les plus proches."
  },
  {
    "objectID": "KNN.html#théorie",
    "href": "KNN.html#théorie",
    "title": "KNN - K-Nearest Neighbors",
    "section": "",
    "text": "Le K-Nearest Neighbors (KNN) est un algorithme de classification basé sur la proximité des données dans un espace multidimensionnel. Il attribue une classe à un point en fonction des K voisins les plus proches."
  },
  {
    "objectID": "KNN.html#hyperparamètre-utilisé",
    "href": "KNN.html#hyperparamètre-utilisé",
    "title": "KNN - K-Nearest Neighbors",
    "section": "Hyperparamètre utilisé",
    "text": "Hyperparamètre utilisé\nNous allons optimiser :\n\nNombre de voisins (k) : déterminé en fonction de la précision sur l’ensemble de validation."
  },
  {
    "objectID": "KNN.html#métriques-dévaluation",
    "href": "KNN.html#métriques-dévaluation",
    "title": "KNN - K-Nearest Neighbors",
    "section": "Métriques d’évaluation",
    "text": "Métriques d’évaluation\nNous afficherons :\n\nMatrice de confusion : montrant les erreurs de classification sur l’échantillon de test.\nTaux de bien classés sur l’échantillon de validation avec le meilleur hyperparamètre.\nTaux de bien classés sur l’échantillon de test avec ce même hyperparamètre.\nTaux de bien classés par classe sur l’échantillon de test pour observer la précision sur chaque classe."
  },
  {
    "objectID": "KNN.html#recherche-du-meilleur-k-et-évaluation",
    "href": "KNN.html#recherche-du-meilleur-k-et-évaluation",
    "title": "KNN - K-Nearest Neighbors",
    "section": "Recherche du meilleur k et évaluation",
    "text": "Recherche du meilleur k et évaluation\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n# 🔇 Suppression des avertissements inutiles\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# 🔄 Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# 📊 Préparation des données\nX_train, y_train = train_data.drop('Cover_Type', axis=1), train_data['Cover_Type']\nX_val, y_val = val_data.drop('Cover_Type', axis=1), val_data['Cover_Type']\nX_test, y_test = test_data.drop('Cover_Type', axis=1), test_data['Cover_Type']\n\n# 🔢 Normalisation des données\nscaler = StandardScaler()\nX_train, X_val, X_test = scaler.fit_transform(X_train), scaler.transform(X_val), scaler.transform(X_test)\n\n# 🎯 Recherche du meilleur hyperparamètre k\nk_values = range(1, 51, 2)\nval_accuracies = []\n\nfor k in k_values:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    acc = accuracy_score(y_val, knn.predict(X_val))\n    val_accuracies.append((k, acc))\n\n# Sélection du meilleur hyperparamètre\nbest_k, best_val_acc = max(val_accuracies, key=lambda x: x[1])\n\n\n# 📈 Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(k_values, [acc for k, acc in val_accuracies], marker='o', linestyle='dashed', label=\"Validation\")\nplt.xlabel(\"Nombre de voisins (k)\")\nplt.ylabel(\"Précision sur validation\")\nplt.title(\"Impact du nombre de voisins sur la performance de KNN\")\nplt.legend()\nplt.show()\n\n# 🏆 Modèle final avec le meilleur hyperparamètre\nfinal_model = KNeighborsClassifier(n_neighbors=best_k)\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# 📊 Matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\n\n# 📈 Calcul des taux de bien classés par classe\nclass_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\noverall_test_accuracy = accuracy_score(y_test, y_test_pred)\n\n# 📝 Affichage des résultats\nprint(f\"\\n🔹 Meilleur nombre de voisins (k) sur l'échantillon de validation : {best_k}\")\nprint(f\"Taux de bien classés sur l'échantillon de validation avec cet hyperparamètre : {best_val_acc:.2%}\")\nprint(\"\\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\")\nprint(conf_matrix)\n\nprint(\"\\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre  :\")\nfor i, acc in enumerate(class_accuracies, start=1):\n    print(f\"Classe {i} : {acc:.2%}\")\n\nprint(f\"\\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : {overall_test_accuracy:.2%}\")\n\n\n\n\n\n\n\n\n\n🔹 Meilleur nombre de voisins (k) sur l'échantillon de validation : 1\nTaux de bien classés sur l'échantillon de validation avec cet hyperparamètre : 81.40%\n\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\n[[1663  352    1    0   14    5   84]\n [ 370 2288   37    0   80   49    9]\n [   1   29 1235   20    4  141    0]\n [   0    3   29   70    0    8    0]\n [  13   53   10    0  296    8    0]\n [   3   31  142   11    0  507    0]\n [  40   10    0    0    1    0  770]]\n\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre  :\nClasse 1 : 78.48%\nClasse 2 : 80.76%\nClasse 3 : 86.36%\nClasse 4 : 63.64%\nClasse 5 : 77.89%\nClasse 6 : 73.05%\nClasse 7 : 93.79%\n\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : 81.42%"
  },
  {
    "objectID": "regression_multinomiale.html",
    "href": "regression_multinomiale.html",
    "title": "Régression Multinomiale (Softmax Regression)",
    "section": "",
    "text": "La régression multinomiale, aussi appelée régression logistique multinomiale, est une extension de la régression logistique qui permet de gérer plusieurs classes. Elle utilise une fonction Softmax en sortie pour assigner une probabilité à chaque classe."
  },
  {
    "objectID": "regression_multinomiale.html#théorie",
    "href": "regression_multinomiale.html#théorie",
    "title": "Régression Multinomiale (Softmax Regression)",
    "section": "",
    "text": "La régression multinomiale, aussi appelée régression logistique multinomiale, est une extension de la régression logistique qui permet de gérer plusieurs classes. Elle utilise une fonction Softmax en sortie pour assigner une probabilité à chaque classe."
  },
  {
    "objectID": "regression_multinomiale.html#hyperparamètre-utilisé",
    "href": "regression_multinomiale.html#hyperparamètre-utilisé",
    "title": "Régression Multinomiale (Softmax Regression)",
    "section": "Hyperparamètre utilisé",
    "text": "Hyperparamètre utilisé\nNous allons optimiser :\n\nParamètre de régularisation (C) : contrôle la pénalisation de la complexité du modèle et est sélectionné en fonction de la précision sur l’ensemble de validation."
  },
  {
    "objectID": "regression_multinomiale.html#métriques-dévaluation",
    "href": "regression_multinomiale.html#métriques-dévaluation",
    "title": "Régression Multinomiale (Softmax Regression)",
    "section": "Métriques d’évaluation",
    "text": "Métriques d’évaluation\nNous afficherons :\n\nMatrice de confusion : montrant les erreurs de classification sur l’échantillon de test.\nTaux de bien classés sur l’échantillon de validation avec le meilleur hyperparamètre.\nTaux de bien classés sur l’échantillon de test avec ce même hyperparamètre.\nTaux de bien classés par classe sur l’échantillon de test pour observer la précision sur chaque classe."
  },
  {
    "objectID": "regression_multinomiale.html#recherche-du-meilleur-c-et-évaluation",
    "href": "regression_multinomiale.html#recherche-du-meilleur-c-et-évaluation",
    "title": "Régression Multinomiale (Softmax Regression)",
    "section": "Recherche du meilleur C et évaluation",
    "text": "Recherche du meilleur C et évaluation\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n# 🔇 Suppression des avertissements inutiles\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# 🔄 Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# 📊 Préparation des données\nX_train, y_train = train_data.drop('Cover_Type', axis=1), train_data['Cover_Type']\nX_val, y_val = val_data.drop('Cover_Type', axis=1), val_data['Cover_Type']\nX_test, y_test = test_data.drop('Cover_Type', axis=1), test_data['Cover_Type']\n\n# 🔢 Normalisation des données\nscaler = StandardScaler()\nX_train, X_val, X_test = scaler.fit_transform(X_train), scaler.transform(X_val), scaler.transform(X_test)\n\n# 🎯 Recherche du meilleur hyperparamètre C\nC_values = np.arange(0.1, 1.1, 0.1)\nval_accuracies = []\n\nfor C in C_values:\n    model = LogisticRegression(multi_class='multinomial', solver='saga', C=C, penalty='l2', max_iter=500)\n    model.fit(X_train, y_train)\n    acc = accuracy_score(y_val, model.predict(X_val))\n    val_accuracies.append((C, acc))\n\n# Sélection du meilleur hyperparamètre\nbest_C, best_val_acc = max(val_accuracies, key=lambda x: x[1])\n\n\n# 📈 Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(C_values, [acc for C, acc in val_accuracies], marker='o', linestyle='dashed', label=\"Validation\")\nplt.xlabel(\"Paramètre de régularisation (C)\")\nplt.ylabel(\"Précision sur validation\")\nplt.title(\"Impact de la régularisation sur la performance de la régression multinomiale\")\nplt.legend()\nplt.show()\n\n# 🏆 Modèle final avec le meilleur hyperparamètre\nfinal_model = LogisticRegression(multi_class='multinomial', solver='saga', C=best_C, penalty='l2', max_iter=500)\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# 📊 Matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\n\n# 📈 Calcul des taux de bien classés par classe\nclass_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\noverall_test_accuracy = accuracy_score(y_test, y_test_pred)\n\n# 📝 Affichage des résultats\nprint(f\"\\n🔹 Meilleur hyperparamètre C sur l'échantillon de validation : {best_C:.2f}\")\nprint(f\"Taux de bien classés sur l'échantillon de validation avec cet hyperparamètre : {best_val_acc:.2%}\")\nprint(\"\\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\")\nprint(conf_matrix)\n\nprint(\"\\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre  :\")\nfor i, acc in enumerate(class_accuracies, start=1):\n    print(f\"Classe {i} : {acc:.2%}\")\n\nprint(f\"\\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : {overall_test_accuracy:.2%}\")\n\n/home/ensai/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n/home/ensai/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\n\n\n🔹 Meilleur hyperparamètre C sur l'échantillon de validation : 0.10\nTaux de bien classés sur l'échantillon de validation avec cet hyperparamètre : 69.14%\n\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\n[[1416  583    1    0    6    1  112]\n [ 511 2142   87    1   40   44    8]\n [   0   48 1249   38    8   87    0]\n [   0    0   61   34    0   15    0]\n [   2  257   24    0   92    5    0]\n [   0   64  374    3    8  245    0]\n [ 163    1    3    0    0    0  654]]\n\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre  :\nClasse 1 : 66.82%\nClasse 2 : 75.61%\nClasse 3 : 87.34%\nClasse 4 : 30.91%\nClasse 5 : 24.21%\nClasse 6 : 35.30%\nClasse 7 : 79.66%\n\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : 69.54%"
  },
  {
    "objectID": "reseau_neurones.html",
    "href": "reseau_neurones.html",
    "title": "Réseau de Neurones (MLP) avec sortie Softmax",
    "section": "",
    "text": "Un réseau de neurones multi-couches (MLP - Multi-Layer Perceptron) est un modèle d’apprentissage supervisé basé sur des couches de neurones artificiels. Il est particulièrement efficace pour la classification non linéaire.\nDans notre cas, nous utilisons une couche de sortie Softmax, qui permet de normaliser les sorties du réseau en probabilités pour une classification multiclasses."
  },
  {
    "objectID": "reseau_neurones.html#théorie",
    "href": "reseau_neurones.html#théorie",
    "title": "Réseau de Neurones (MLP) avec sortie Softmax",
    "section": "",
    "text": "Un réseau de neurones multi-couches (MLP - Multi-Layer Perceptron) est un modèle d’apprentissage supervisé basé sur des couches de neurones artificiels. Il est particulièrement efficace pour la classification non linéaire.\nDans notre cas, nous utilisons une couche de sortie Softmax, qui permet de normaliser les sorties du réseau en probabilités pour une classification multiclasses."
  },
  {
    "objectID": "reseau_neurones.html#hyperparamètre-utilisé",
    "href": "reseau_neurones.html#hyperparamètre-utilisé",
    "title": "Réseau de Neurones (MLP) avec sortie Softmax",
    "section": "Hyperparamètre utilisé",
    "text": "Hyperparamètre utilisé\nNous allons optimiser :\n\nNombre d’époques (epochs) : sélectionné en fonction de la précision sur l’ensemble de validation.\n\nNous utilisons également : - Optimiseur Adam avec un taux d’apprentissage adaptatif. - Taille du batch (batch_size) fixé à 32."
  },
  {
    "objectID": "reseau_neurones.html#métriques-dévaluation",
    "href": "reseau_neurones.html#métriques-dévaluation",
    "title": "Réseau de Neurones (MLP) avec sortie Softmax",
    "section": "Métriques d’évaluation",
    "text": "Métriques d’évaluation\nNous afficherons :\n\nMatrice de confusion : montrant les erreurs de classification sur l’échantillon de test.\nTaux de bien classés sur l’échantillon de validation avec le meilleur nombre d’époques.\nTaux de bien classés sur l’échantillon de test avec ce même hyperparamètre.\nTaux de bien classés par classe sur l’échantillon de test pour observer la précision sur chaque classe."
  },
  {
    "objectID": "reseau_neurones.html#recherche-du-meilleur-nombre-dépoques-et-évaluation",
    "href": "reseau_neurones.html#recherche-du-meilleur-nombre-dépoques-et-évaluation",
    "title": "Réseau de Neurones (MLP) avec sortie Softmax",
    "section": "Recherche du meilleur nombre d’époques et évaluation",
    "text": "Recherche du meilleur nombre d’époques et évaluation\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# 🔇 Suppression des avertissements inutiles\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# 🔌 Utilisation du CPU uniquement (désactivation GPU)\ndevice = torch.device(\"cpu\")\ntorch.backends.cudnn.enabled = False\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n\n# 🔄 Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# 📊 Préparation des données\nX_train, y_train = train_data.drop('Cover_Type', axis=1).values, train_data['Cover_Type'].values - 1\nX_val, y_val = val_data.drop('Cover_Type', axis=1).values, val_data['Cover_Type'].values - 1\nX_test, y_test = test_data.drop('Cover_Type', axis=1).values, test_data['Cover_Type'].values - 1\n\n# 🔢 Normalisation des données\nscaler = StandardScaler()\nX_train, X_val, X_test = scaler.fit_transform(X_train), scaler.transform(X_val), scaler.transform(X_test)\n\n# 📦 Conversion en tenseurs PyTorch\nX_train_torch = torch.tensor(X_train, dtype=torch.float32, device=device)\ny_train_torch = torch.tensor(y_train, dtype=torch.long, device=device)\nX_val_torch = torch.tensor(X_val, dtype=torch.float32, device=device)\ny_val_torch = torch.tensor(y_val, dtype=torch.long, device=device)\nX_test_torch = torch.tensor(X_test, dtype=torch.float32, device=device)\ny_test_torch = torch.tensor(y_test, dtype=torch.long, device=device)\n\n# 📚 Création des DataLoaders\nbatch_size = 32\ntrain_loader = DataLoader(TensorDataset(X_train_torch, y_train_torch), batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(TensorDataset(X_val_torch, y_val_torch), batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(TensorDataset(X_test_torch, y_test_torch), batch_size=batch_size, shuffle=False)\n\n# 🏗 Définition du modèle PyTorch (MLP)\nclass MLP(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(128, 64)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = self.relu1(self.fc1(x))\n        x = self.relu2(self.fc2(x))\n        return self.fc3(x)  # Pas de softmax ici (inclus dans CrossEntropyLoss)\n\n# 🎯 Initialisation du modèle\nnum_features, num_classes = X_train.shape[1], len(set(y_train))\nmodel = MLP(num_features, num_classes).to(device)\n\n# 🚀 Optimiseur et fonction de perte\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters())\n\n# 📈 Entraînement du modèle avec sélection du meilleur nombre d'époques\nnum_epochs = 100\ntrain_acc_list, val_acc_list = [], []\nbest_val_acc, best_epoch = 0, 0\n\nfor epoch in range(num_epochs):\n    # 🔄 Mode entraînement\n    model.train()\n    correct_train, total_train = 0, 0\n\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n\n        # 🎯 Calcul de l'accuracy sur l'entraînement\n        _, predicted = torch.max(outputs, 1)\n        correct_train += (predicted == y_batch).sum().item()\n        total_train += y_batch.size(0)\n\n    train_acc_list.append(correct_train / total_train)\n\n    # 🔄 Mode validation\n    model.eval()\n    correct_val, total_val = 0, 0\n\n    with torch.no_grad():\n        for X_batch, y_batch in val_loader:\n            outputs = model(X_batch)\n            _, predicted = torch.max(outputs, 1)\n            correct_val += (predicted == y_batch).sum().item()\n            total_val += y_batch.size(0)\n\n    val_accuracy = correct_val / total_val\n    val_acc_list.append(val_accuracy)\n\n    # 🎯 Sauvegarde de la meilleure époque\n    if val_accuracy &gt; best_val_acc:\n        best_val_acc, best_epoch = val_accuracy, epoch + 1\n\n# 📉 Affichage des courbes d'entraînement\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, num_epochs + 1), train_acc_list, label='Train Accuracy')\nplt.plot(range(1, num_epochs + 1), val_acc_list, label='Validation Accuracy')\nplt.axvline(best_epoch, color='r', linestyle='--', label=f'Best Epoch: {best_epoch}')\nplt.xlabel(\"Époques\")\nplt.ylabel(\"Taux de bonnes prédictions\")\nplt.title(\"Optimisation du modèle de réseau de neurones (PyTorch)\")\nplt.legend()\nplt.show()\n\n# 🔄 Ré-entraîner le modèle avec le meilleur nombre d'époques\nmodel.train()\nfor epoch in range(best_epoch):\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n\n# 🎯 Évaluation sur l'ensemble de test\nmodel.eval()\ncorrect_test, total_test = 0, 0\ny_test_pred_classes = []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        outputs = model(X_batch)\n        _, predicted = torch.max(outputs, 1)\n        y_test_pred_classes.extend(predicted.cpu().numpy())\n        correct_test += (predicted == y_batch).sum().item()\n        total_test += y_batch.size(0)\n\ntest_accuracy = correct_test / total_test\n\n# 📌 Affichage de la matrice de confusion et des métriques finales\nconf_matrix = confusion_matrix(y_test, y_test_pred_classes)\nprint(f\"\\n🔹 Meilleure époque : {best_epoch} avec une précision de validation de {best_val_acc:.2%}\")\nprint(\"\\n📊 Matrice de confusion :\")\nprint(conf_matrix)\n\nprint(\"\\n📈 Taux de bien classés par classe :\")\nclass_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\nfor i, acc in enumerate(class_accuracies, start=1):\n    print(f\"Classe {i} : {acc:.2%}\")\nprint(f\"\\nTaux de bien classés total : {test_accuracy:.2%}\")\n\n\n\n\n\n\n\n\n\n🔹 Meilleure époque : 99 avec une précision de validation de 84.40%\n\n📊 Matrice de confusion :\n[[1804  247    0    0    6    1   61]\n [ 429 2296   20    1   42   35   10]\n [   0   30 1265   15    6  114    0]\n [   0    0   31   71    0    8    0]\n [   4   76    4    0  292    4    0]\n [   2   16   90    6    0  580    0]\n [  28    5    0    0    0    0  788]]\n\n📈 Taux de bien classés par classe :\nClasse 1 : 85.13%\nClasse 2 : 81.04%\nClasse 3 : 88.46%\nClasse 4 : 64.55%\nClasse 5 : 76.84%\nClasse 6 : 83.57%\nClasse 7 : 95.98%\n\nTaux de bien classés total : 84.61%"
  },
  {
    "objectID": "regression_logistique_ova.html",
    "href": "regression_logistique_ova.html",
    "title": "Régression Logistique Binomiale - One-Versus-All (OVA)",
    "section": "",
    "text": "La régression logistique binomiale est utilisée pour la classification binaire, mais elle peut être adaptée aux problèmes multiclasse via l’approche One-Versus-All (OVA). Ici, un modèle est entraîné pour chaque classe contre toutes les autres combinées."
  },
  {
    "objectID": "regression_logistique_ova.html#théorie",
    "href": "regression_logistique_ova.html#théorie",
    "title": "Régression Logistique Binomiale - One-Versus-All (OVA)",
    "section": "",
    "text": "La régression logistique binomiale est utilisée pour la classification binaire, mais elle peut être adaptée aux problèmes multiclasse via l’approche One-Versus-All (OVA). Ici, un modèle est entraîné pour chaque classe contre toutes les autres combinées."
  },
  {
    "objectID": "regression_logistique_ova.html#hyperparamètre-utilisé",
    "href": "regression_logistique_ova.html#hyperparamètre-utilisé",
    "title": "Régression Logistique Binomiale - One-Versus-All (OVA)",
    "section": "Hyperparamètre utilisé",
    "text": "Hyperparamètre utilisé\nNous allons optimiser :\n\nParamètre de régularisation (C) : qui contrôle la complexité du modèle et est sélectionné en fonction de la précision sur l’ensemble de validation."
  },
  {
    "objectID": "regression_logistique_ova.html#métriques-dévaluation",
    "href": "regression_logistique_ova.html#métriques-dévaluation",
    "title": "Régression Logistique Binomiale - One-Versus-All (OVA)",
    "section": "Métriques d’évaluation",
    "text": "Métriques d’évaluation\nNous afficherons :\n\nMatrice de confusion : montrant les erreurs de classification sur l’échantillon de test.\nTaux de bien classés sur l’échantillon de validation avec le meilleur hyperparamètre.\nTaux de bien classés sur l’échantillon de test avec ce même hyperparamètre.\nTaux de bien classés par classe sur l’échantillon de test pour observer la précision sur chaque classe."
  },
  {
    "objectID": "regression_logistique_ova.html#recherche-du-meilleur-c-et-évaluation",
    "href": "regression_logistique_ova.html#recherche-du-meilleur-c-et-évaluation",
    "title": "Régression Logistique Binomiale - One-Versus-All (OVA)",
    "section": "Recherche du meilleur C et évaluation",
    "text": "Recherche du meilleur C et évaluation\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n# 🔇 Suppression des avertissements inutiles\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# 🔄 Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# 📊 Préparation des données\nX_train, y_train = train_data.drop('Cover_Type', axis=1), train_data['Cover_Type']\nX_val, y_val = val_data.drop('Cover_Type', axis=1), val_data['Cover_Type']\nX_test, y_test = test_data.drop('Cover_Type', axis=1), test_data['Cover_Type']\n\n# 🔢 Normalisation des données\nscaler = StandardScaler()\nX_train, X_val, X_test = scaler.fit_transform(X_train), scaler.transform(X_val), scaler.transform(X_test)\n\n# 🎯 Recherche du meilleur hyperparamètre C\nC_values = np.arange(0.1, 1.1, 0.1)\nval_accuracies = []\n\nfor C in C_values:\n    model = OneVsRestClassifier(LogisticRegression(solver='saga', C=C, penalty='l2', max_iter=500))\n    model.fit(X_train, y_train)\n    acc = accuracy_score(y_val, model.predict(X_val))\n    val_accuracies.append((C, acc))\n\n# Sélection du meilleur hyperparamètre\nbest_C, best_val_acc = max(val_accuracies, key=lambda x: x[1])\n\n\n# 📈 Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(C_values, [acc for C, acc in val_accuracies], marker='o', linestyle='dashed', label=\"Validation\")\nplt.xlabel(\"Paramètre de régularisation (C)\")\nplt.ylabel(\"Précision sur validation\")\nplt.title(\"Impact de la régularisation sur la performance de la régression logistique (OVA)\")\nplt.legend()\nplt.show()\n\n# 🏆 Modèle final avec le meilleur hyperparamètre\nfinal_model = OneVsRestClassifier(LogisticRegression(solver='saga', C=best_C, penalty='l2', max_iter=500))\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# 📊 Matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\n\n# 📈 Calcul des taux de bien classés par classe\nclass_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\noverall_test_accuracy = accuracy_score(y_test, y_test_pred)\n\n# 📝 Affichage des résultats\nprint(f\"\\n🔹 Meilleur hyperparamètre C sur l'échantillon de validation : {best_C:.2f}\")\nprint(f\"Taux de bien classés sur l'échantillon de validation avec cet hyperparamètre : {best_val_acc:.2%}\")\nprint(\"\\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\")\nprint(conf_matrix)\n\nprint(\"\\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre  :\")\nfor i, acc in enumerate(class_accuracies, start=1):\n    print(f\"Classe {i} : {acc:.2%}\")\n\nprint(f\"\\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : {overall_test_accuracy:.2%}\")\n\n\n\n\n\n\n\n\n\n🔹 Meilleur hyperparamètre C sur l'échantillon de validation : 0.80\nTaux de bien classés sur l'échantillon de validation avec cet hyperparamètre : 67.71%\n\n📊 Matrice de confusion sur l'échantillon de test, avec le meilleur hyperparamètre :\n[[1337  610    1    0    5    1  165]\n [ 495 2150  107    1   21   51    8]\n [   0   65 1270   20    8   67    0]\n [   0    0   70   29    0   11    0]\n [   2  273   27    0   63   15    0]\n [   0   92  386    3   21  192    0]\n [ 143    7    3    0    0    0  668]]\n\n📈 Taux de bien classés par classe sur l'échantillon de test, avec le meilleur hyperparamètre  :\nClasse 1 : 63.10%\nClasse 2 : 75.89%\nClasse 3 : 88.81%\nClasse 4 : 26.36%\nClasse 5 : 16.58%\nClasse 6 : 27.67%\nClasse 7 : 81.36%\n\n🔹 Taux de bien classés sur l'échantillon de test avec le meilleur hyperparamètre : 68.07%"
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Comparaison des Méthodes de Classification",
    "section": "",
    "text": "(% de bien classés)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClasse (effectif)\nKNN\nLDA\nQDA\nBayesien Naïf\nArbre CART\nForêt Aléatoire\nReg Log OVA\nReg Log OVO\nReg Multinom\nRéseau Neurones\nSVM OVA\nSVM OVO\n\n\n\n\n1 (10592)\n78.48\n63.43\n54.88\n61.35\n74.61\n82.73\n63.10\n66.97\n66.82\n78.29\n68.29\n70.55\n\n\n2 (14165)\n80.76\n65.83\n55.10\n61.14\n76.84\n87.89\n75.89\n75.40\n75.61\n86.69\n78.57\n78.61\n\n\n3 (7151)\n86.36\n63.08\n66.29\n65.66\n85.03\n94.55\n88.81\n86.78\n87.34\n84.55\n90.63\n90.35\n\n\n4 (549)\n63.64\n48.18\n48.18\n60.00\n67.27\n64.55\n26.36\n34.55\n30.91\n70.00\n20.00\n21.82\n\n\n5 (1899)\n77.89\n47.11\n50.26\n46.32\n69.21\n71.84\n16.58\n25.79\n24.21\n83.95\n33.16\n34.47\n\n\n6 (3473)\n73.05\n52.45\n48.27\n44.81\n74.06\n78.67\n27.67\n40.06\n35.30\n86.31\n38.33\n38.62\n\n\n7 (4102)\n93.79\n80.88\n80.88\n79.29\n90.86\n94.28\n81.36\n80.39\n79.66\n92.33\n82.10\n81.49\n\n\nTotal (41931)\n81.42\n64.04\n58.60\n61.70\n78.35\n86.55\n68.07\n69.99\n69.54\n84.38\n72.22\n72.80"
  },
  {
    "objectID": "conclusions.html#comparaison-des-performances-des-modèles",
    "href": "conclusions.html#comparaison-des-performances-des-modèles",
    "title": "Comparaison des Méthodes de Classification",
    "section": "",
    "text": "(% de bien classés)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClasse (effectif)\nKNN\nLDA\nQDA\nBayesien Naïf\nArbre CART\nForêt Aléatoire\nReg Log OVA\nReg Log OVO\nReg Multinom\nRéseau Neurones\nSVM OVA\nSVM OVO\n\n\n\n\n1 (10592)\n78.48\n63.43\n54.88\n61.35\n74.61\n82.73\n63.10\n66.97\n66.82\n78.29\n68.29\n70.55\n\n\n2 (14165)\n80.76\n65.83\n55.10\n61.14\n76.84\n87.89\n75.89\n75.40\n75.61\n86.69\n78.57\n78.61\n\n\n3 (7151)\n86.36\n63.08\n66.29\n65.66\n85.03\n94.55\n88.81\n86.78\n87.34\n84.55\n90.63\n90.35\n\n\n4 (549)\n63.64\n48.18\n48.18\n60.00\n67.27\n64.55\n26.36\n34.55\n30.91\n70.00\n20.00\n21.82\n\n\n5 (1899)\n77.89\n47.11\n50.26\n46.32\n69.21\n71.84\n16.58\n25.79\n24.21\n83.95\n33.16\n34.47\n\n\n6 (3473)\n73.05\n52.45\n48.27\n44.81\n74.06\n78.67\n27.67\n40.06\n35.30\n86.31\n38.33\n38.62\n\n\n7 (4102)\n93.79\n80.88\n80.88\n79.29\n90.86\n94.28\n81.36\n80.39\n79.66\n92.33\n82.10\n81.49\n\n\nTotal (41931)\n81.42\n64.04\n58.60\n61.70\n78.35\n86.55\n68.07\n69.99\n69.54\n84.38\n72.22\n72.80"
  },
  {
    "objectID": "conclusions.html#explication",
    "href": "conclusions.html#explication",
    "title": "Comparaison des Méthodes de Classification",
    "section": "Explication :",
    "text": "Explication :\n\nOVA force une classe unique à se démarquer contre toutes les autres.\nOVO compare les classes deux à deux, ce qui est sous-optimal pour des classes déséquilibrées."
  },
  {
    "objectID": "conclusions.html#comparaison-des-performances-des-modèles-de-bien-classés",
    "href": "conclusions.html#comparaison-des-performances-des-modèles-de-bien-classés",
    "title": "Comparaison des Méthodes de Classification",
    "section": "",
    "text": "Classe (effectif)\nKNN\nLDA\nQDA\nBayesien Naïf\nArbre CART\nForêt Aléatoire\nReg Log OVA\nReg Log OVO\nReg Multinom\nRéseau Neurones\nSVM OVA\nSVM OVO\n\n\n\n\n1 (10592)\n78.48\n63.43\n54.88\n61.35\n74.61\n82.73\n63.10\n66.97\n66.82\n78.29\n68.29\n70.55\n\n\n2 (14165)\n80.76\n65.83\n55.10\n61.14\n76.84\n87.89\n75.89\n75.40\n75.61\n86.69\n78.57\n78.61\n\n\n3 (7151)\n86.36\n63.08\n66.29\n65.66\n85.03\n94.55\n88.81\n86.78\n87.34\n84.55\n90.63\n90.35\n\n\n4 (549)\n63.64\n48.18\n48.18\n60.00\n67.27\n64.55\n26.36\n34.55\n30.91\n70.00\n20.00\n21.82\n\n\n5 (1899)\n77.89\n47.11\n50.26\n46.32\n69.21\n71.84\n16.58\n25.79\n24.21\n83.95\n33.16\n34.47\n\n\n6 (3473)\n73.05\n52.45\n48.27\n44.81\n74.06\n78.67\n27.67\n40.06\n35.30\n86.31\n38.33\n38.62\n\n\n7 (4102)\n93.79\n80.88\n80.88\n79.29\n90.86\n94.28\n81.36\n80.39\n79.66\n92.33\n82.10\n81.49\n\n\nTotal (41931)\n81.42\n64.04\n58.60\n61.70\n78.35\n86.55\n68.07\n69.99\n69.54\n84.38\n72.22\n72.80"
  }
]