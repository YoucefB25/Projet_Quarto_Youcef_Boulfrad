[
  {
    "objectID": "knn.html",
    "href": "knn.html",
    "title": "KNN - K-Nearest Neighbors",
    "section": "",
    "text": "Le KNN est un algorithme de classification basé sur la proximité des données dans un espace multidimensionnel.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Chargement des ensembles de données déjà préparés\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Recherche du meilleur nombre de voisins en utilisant uniquement l'ensemble d'entraînement\nneighbors = range(1, 51, 2)\ntrain_accuracies = []\nval_accuracies = []\n\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    \n    # Évaluation sur l'ensemble d'entraînement\n    y_train_pred = knn.predict(X_train)\n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    \n    # Évaluation sur l'ensemble de validation\n    y_val_pred = knn.predict(X_val)\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur hyperparamètre basé sur l'ensemble de validation\nbest_k = neighbors[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur nombre de voisins: {best_k}\")\n\n# Affichage du graphique comparant l'entraînement et la validation\nplt.figure(figsize=(8, 6))\nplt.plot(neighbors, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(neighbors, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Nombre de voisins\")\nplt.ylabel(\"Taux de bonnes prédictions\")\nplt.title(\"Optimisation du nombre de voisins pour KNN\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur k\nknn = KNeighborsClassifier(n_neighbors=best_k)\nknn.fit(X_train, y_train)\n\n# Évaluation sur l'ensemble de test\ny_test_pred = knn.predict(X_test)\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(confusion_matrix(y_test, y_test_pred))\nprint(classification_report(y_test, y_test_pred))\n\nMeilleur nombre de voisins: 1\n\n\n\n\n\n\n\n\n\n\nÉvaluation sur l'ensemble de test\n[[1413  263    0    0    6    2   24]\n [ 267 1914   39    0   20   16    5]\n [   0   44  210    3    0   24    0]\n [   0    1    8    8    0    4    0]\n [   8   23    2    0   41    0    0]\n [   1   14   28    1    0  100    0]\n [  27    4    0    0    0    0  128]]\n              precision    recall  f1-score   support\n\n           1       0.82      0.83      0.83      1708\n           2       0.85      0.85      0.85      2261\n           3       0.73      0.75      0.74       281\n           4       0.67      0.38      0.48        21\n           5       0.61      0.55      0.58        74\n           6       0.68      0.69      0.69       144\n           7       0.82      0.81      0.81       159\n\n    accuracy                           0.82      4648\n   macro avg       0.74      0.69      0.71      4648\nweighted avg       0.82      0.82      0.82      4648"
  },
  {
    "objectID": "knn.html#théorie",
    "href": "knn.html#théorie",
    "title": "KNN - K-Nearest Neighbors",
    "section": "",
    "text": "Le KNN est un algorithme de classification basé sur la proximité des données dans un espace multidimensionnel."
  },
  {
    "objectID": "knn.html#exemple-en-python",
    "href": "knn.html#exemple-en-python",
    "title": "KNN - K-Nearest Neighbors",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Chargement des ensembles de données déjà préparés\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Recherche du meilleur nombre de voisins en utilisant uniquement l'ensemble d'entraînement\nneighbors = range(1, 51, 2)\ntrain_accuracies = []\nval_accuracies = []\n\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    \n    # Évaluation sur l'ensemble d'entraînement\n    y_train_pred = knn.predict(X_train)\n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    \n    # Évaluation sur l'ensemble de validation\n    y_val_pred = knn.predict(X_val)\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur hyperparamètre basé sur l'ensemble de validation\nbest_k = neighbors[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur nombre de voisins: {best_k}\")\n\n# Affichage du graphique comparant l'entraînement et la validation\nplt.figure(figsize=(8, 6))\nplt.plot(neighbors, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(neighbors, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Nombre de voisins\")\nplt.ylabel(\"Taux de bonnes prédictions\")\nplt.title(\"Optimisation du nombre de voisins pour KNN\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur k\nknn = KNeighborsClassifier(n_neighbors=best_k)\nknn.fit(X_train, y_train)\n\n# Évaluation sur l'ensemble de test\ny_test_pred = knn.predict(X_test)\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(confusion_matrix(y_test, y_test_pred))\nprint(classification_report(y_test, y_test_pred))\n\nMeilleur nombre de voisins: 1\n\n\n\n\n\n\n\n\n\n\nÉvaluation sur l'ensemble de test\n[[1413  263    0    0    6    2   24]\n [ 267 1914   39    0   20   16    5]\n [   0   44  210    3    0   24    0]\n [   0    1    8    8    0    4    0]\n [   8   23    2    0   41    0    0]\n [   1   14   28    1    0  100    0]\n [  27    4    0    0    0    0  128]]\n              precision    recall  f1-score   support\n\n           1       0.82      0.83      0.83      1708\n           2       0.85      0.85      0.85      2261\n           3       0.73      0.75      0.74       281\n           4       0.67      0.38      0.48        21\n           5       0.61      0.55      0.58        74\n           6       0.68      0.69      0.69       144\n           7       0.82      0.81      0.81       159\n\n    accuracy                           0.82      4648\n   macro avg       0.74      0.69      0.71      4648\nweighted avg       0.82      0.82      0.82      4648"
  },
  {
    "objectID": "preparation_donnees_covertype.html",
    "href": "preparation_donnees_covertype.html",
    "title": "Téléchargement et Préparation du Dataset Covertype",
    "section": "",
    "text": "La base de données Covertype provient de l’UCI Machine Learning Repository.\nVous pouvez la télécharger directement via le lien ci-dessous :\n\nTélécharger le dataset Covertype (CSV)\n\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Téléchargement direct des données depuis l'URL\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\ncolumn_names = [f'Feature_{i}' for i in range(1, 55)] + ['Cover_Type']\ndata = pd.read_csv(url, header=None, names=column_names)\n\n# Réduction simple à 4% des données sans rééquilibrage des classes\nsample_size = int(0.04 * len(data))  # 4% de la base d'origine\ndata_sampled = data.sample(n=sample_size, random_state=42).reset_index(drop=True)\n\n# Division des données en ensembles d'entraînement, de validation et de test\ntrain_data, temp_data = train_test_split(data_sampled, test_size=0.4, random_state=42, stratify=data_sampled['Cover_Type'])\nval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=temp_data['Cover_Type'])\n\n# Afficher la taille des ensembles\nprint(f\"Entraînement : {len(train_data)} lignes\")\nprint(f\"Validation : {len(val_data)} lignes\")\nprint(f\"Test : {len(test_data)} lignes\")\n\n# Sauvegarder les ensembles en fichiers CSV\ntrain_data.to_csv('covertype_train.csv', index=False)\nval_data.to_csv('covertype_val.csv', index=False)\ntest_data.to_csv('covertype_test.csv', index=False)\n\nEntraînement : 13944 lignes\nValidation : 4648 lignes\nTest : 4648 lignes\n\n\n\n\n\n\nTélécharger l’ensemble d’entraînement\nTélécharger l’ensemble de validation\nTélécharger l’ensemble de test ```"
  },
  {
    "objectID": "preparation_donnees_covertype.html#chargement-et-réduction-des-données",
    "href": "preparation_donnees_covertype.html#chargement-et-réduction-des-données",
    "title": "Téléchargement et Préparation du Dataset Covertype",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Téléchargement direct des données depuis l'URL\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\ncolumn_names = [f'Feature_{i}' for i in range(1, 55)] + ['Cover_Type']\ndata = pd.read_csv(url, header=None, names=column_names)\n\n# Réduction simple à 4% des données sans rééquilibrage des classes\nsample_size = int(0.04 * len(data))  # 4% de la base d'origine\ndata_sampled = data.sample(n=sample_size, random_state=42).reset_index(drop=True)\n\n# Division des données en ensembles d'entraînement, de validation et de test\ntrain_data, temp_data = train_test_split(data_sampled, test_size=0.4, random_state=42, stratify=data_sampled['Cover_Type'])\nval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=temp_data['Cover_Type'])\n\n# Afficher la taille des ensembles\nprint(f\"Entraînement : {len(train_data)} lignes\")\nprint(f\"Validation : {len(val_data)} lignes\")\nprint(f\"Test : {len(test_data)} lignes\")\n\n# Sauvegarder les ensembles en fichiers CSV\ntrain_data.to_csv('covertype_train.csv', index=False)\nval_data.to_csv('covertype_val.csv', index=False)\ntest_data.to_csv('covertype_test.csv', index=False)\n\nEntraînement : 13944 lignes\nValidation : 4648 lignes\nTest : 4648 lignes"
  },
  {
    "objectID": "preparation_donnees_covertype.html#liens-pour-télécharger-les-ensembles-préparés",
    "href": "preparation_donnees_covertype.html#liens-pour-télécharger-les-ensembles-préparés",
    "title": "Téléchargement et Préparation du Dataset Covertype",
    "section": "",
    "text": "Télécharger l’ensemble d’entraînement\nTélécharger l’ensemble de validation\nTélécharger l’ensemble de test ```"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Accueil",
    "section": "",
    "text": "Bienvenue dans le Cours sur les Méthodes de Classification\nCe cours couvre différentes méthodes de classification en machine learning en utilisant le dataset Covertype. Vous trouverez des explications théoriques, des exemples concrets en Python et des comparaisons de performances.\nNaviguez via le menu pour explorer chaque méthode de classification."
  },
  {
    "objectID": "foret_aleatoire.html",
    "href": "foret_aleatoire.html",
    "title": "Random Forest - Forêt Aléatoire",
    "section": "",
    "text": "La forêt aléatoire est un algorithme d’apprentissage supervisé basé sur un ensemble d’arbres de décision. Elle fonctionne en combinant plusieurs arbres pour améliorer la précision et réduire le risque de surapprentissage.\n\n\n\nLorsqu’on évalue un modèle de classification, plusieurs métriques sont utilisées :\n\nMatrice de confusion : Tableau qui résume les performances du modèle en comparant les vraies classes aux classes prédites. Les lignes correspondent aux classes réelles, et les colonnes aux classes prédites.\nAccuracy (Précision globale) : Proportion des prédictions correctes parmi l’ensemble des données.\nPrecision (Précision par classe) : Nombre de vrais positifs divisé par la somme des vrais positifs et des faux positifs. Indique la fiabilité des prédictions positives.\nRecall (Rappel) : Nombre de vrais positifs divisé par la somme des vrais positifs et des faux négatifs. Indique la capacité du modèle à détecter les échantillons positifs.\nF1-score : Moyenne harmonique entre précision et rappel, utile lorsque les classes sont déséquilibrées.\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Chargement des ensembles de données déjà préparés\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Recherche du meilleur nombre d'arbres en utilisant l'ensemble de validation\nn_estimators_range = range(50, 1000, 50)\ntrain_accuracies = []\nval_accuracies = []\n\nfor n in n_estimators_range:\n    rf = RandomForestClassifier(n_estimators=n, random_state=42, n_jobs=-1)\n    rf.fit(X_train, y_train)\n    \n    # Évaluation sur l'ensemble d'entraînement\n    y_train_pred = rf.predict(X_train)\n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    \n    # Évaluation sur l'ensemble de validation\n    y_val_pred = rf.predict(X_val)\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur hyperparamètre basé sur l'ensemble de validation\nbest_n = n_estimators_range[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur nombre d'arbres: {best_n}\")\n\n# Affichage du graphique comparant l'entraînement et la validation\nplt.figure(figsize=(8, 6))\nplt.plot(n_estimators_range, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(n_estimators_range, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Nombre d'arbres\")\nplt.ylabel(\"Taux de bonnes prédictions\")\nplt.title(\"Optimisation du nombre d'arbres pour Random Forest\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur nombre d'arbres\nrf = RandomForestClassifier(n_estimators=best_n, random_state=42, n_jobs=-1)\nrf.fit(X_train, y_train)\n\n# Évaluation sur l'ensemble de test\ny_test_pred = rf.predict(X_test)\n\n# Affichage de la matrice de confusion avec annotations\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion (les lignes représentent les vraies classes et les colonnes les classes prédites) :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\nMeilleur nombre d'arbres: 750\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion (les lignes représentent les vraies classes et les colonnes les classes prédites) :\n[[1400  300    0    0    1    0    7]\n [ 235 1992   17    0    4   13    0]\n [   0   21  241    1    0   18    0]\n [   0    0    8   11    0    2    0]\n [   3   45    1    0   25    0    0]\n [   0   21   44    1    1   77    0]\n [  35    2    0    0    0    0  122]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.84      0.82      0.83      1708\n           2       0.84      0.88      0.86      2261\n           3       0.77      0.86      0.81       281\n           4       0.85      0.52      0.65        21\n           5       0.81      0.34      0.48        74\n           6       0.70      0.53      0.61       144\n           7       0.95      0.77      0.85       159\n\n    accuracy                           0.83      4648\n   macro avg       0.82      0.67      0.73      4648\nweighted avg       0.83      0.83      0.83      4648"
  },
  {
    "objectID": "foret_aleatoire.html#théorie",
    "href": "foret_aleatoire.html#théorie",
    "title": "Random Forest - Forêt Aléatoire",
    "section": "",
    "text": "La forêt aléatoire est un algorithme d’apprentissage supervisé basé sur un ensemble d’arbres de décision. Elle fonctionne en combinant plusieurs arbres pour améliorer la précision et réduire le risque de surapprentissage."
  },
  {
    "objectID": "foret_aleatoire.html#exemple-en-python",
    "href": "foret_aleatoire.html#exemple-en-python",
    "title": "Random Forest - Forêt Aléatoire",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Chargement des ensembles de données déjà préparés\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Recherche du meilleur nombre d'arbres en utilisant l'ensemble de validation\nn_estimators_range = range(50, 1000, 50)\ntrain_accuracies = []\nval_accuracies = []\n\nfor n in n_estimators_range:\n    rf = RandomForestClassifier(n_estimators=n, random_state=42, n_jobs=-1)\n    rf.fit(X_train, y_train)\n    \n    # Évaluation sur l'ensemble d'entraînement\n    y_train_pred = rf.predict(X_train)\n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    \n    # Évaluation sur l'ensemble de validation\n    y_val_pred = rf.predict(X_val)\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur hyperparamètre basé sur l'ensemble de validation\nbest_n = n_estimators_range[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur nombre d'arbres: {best_n}\")\n\n# Affichage du graphique comparant l'entraînement et la validation\nplt.figure(figsize=(8, 6))\nplt.plot(n_estimators_range, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(n_estimators_range, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Nombre d'arbres\")\nplt.ylabel(\"Taux de bonnes prédictions\")\nplt.title(\"Optimisation du nombre d'arbres pour Random Forest\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur nombre d'arbres\nrf = RandomForestClassifier(n_estimators=best_n, random_state=42, n_jobs=-1)\nrf.fit(X_train, y_train)\n\n# Évaluation sur l'ensemble de test\ny_test_pred = rf.predict(X_test)\n\n# Affichage de la matrice de confusion avec annotations\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion (les lignes représentent les vraies classes et les colonnes les classes prédites) :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\nMeilleur nombre d'arbres: 750\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion (les lignes représentent les vraies classes et les colonnes les classes prédites) :\n[[1400  300    0    0    1    0    7]\n [ 235 1992   17    0    4   13    0]\n [   0   21  241    1    0   18    0]\n [   0    0    8   11    0    2    0]\n [   3   45    1    0   25    0    0]\n [   0   21   44    1    1   77    0]\n [  35    2    0    0    0    0  122]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.84      0.82      0.83      1708\n           2       0.84      0.88      0.86      2261\n           3       0.77      0.86      0.81       281\n           4       0.85      0.52      0.65        21\n           5       0.81      0.34      0.48        74\n           6       0.70      0.53      0.61       144\n           7       0.95      0.77      0.85       159\n\n    accuracy                           0.83      4648\n   macro avg       0.82      0.67      0.73      4648\nweighted avg       0.83      0.83      0.83      4648"
  },
  {
    "objectID": "foret_aleatoire.html#évaluation-des-performances",
    "href": "foret_aleatoire.html#évaluation-des-performances",
    "title": "Random Forest - Forêt Aléatoire",
    "section": "",
    "text": "Lorsqu’on évalue un modèle de classification, plusieurs métriques sont utilisées :\n\nMatrice de confusion : Tableau qui résume les performances du modèle en comparant les vraies classes aux classes prédites. Les lignes correspondent aux classes réelles, et les colonnes aux classes prédites.\nAccuracy (Précision globale) : Proportion des prédictions correctes parmi l’ensemble des données.\nPrecision (Précision par classe) : Nombre de vrais positifs divisé par la somme des vrais positifs et des faux positifs. Indique la fiabilité des prédictions positives.\nRecall (Rappel) : Nombre de vrais positifs divisé par la somme des vrais positifs et des faux négatifs. Indique la capacité du modèle à détecter les échantillons positifs.\nF1-score : Moyenne harmonique entre précision et rappel, utile lorsque les classes sont déséquilibrées."
  },
  {
    "objectID": "arbre_decision.html",
    "href": "arbre_decision.html",
    "title": "Arbre de Décision - CART",
    "section": "",
    "text": "L’algorithme CART (Classification and Regression Trees) est un modèle d’apprentissage supervisé qui construit un arbre de décision en divisant l’espace des caractéristiques en sous-ensembles homogènes.\n\n\n\nLorsqu’on évalue un modèle de classification, plusieurs métriques sont utilisées :\n\nMatrice de confusion : Tableau qui résume les performances du modèle en comparant les vraies classes aux classes prédites. Les lignes correspondent aux classes réelles, et les colonnes aux classes prédites.\nAccuracy (Précision globale) : Proportion des prédictions correctes parmi l’ensemble des données.\nPrecision (Précision par classe) : Nombre de vrais positifs divisé par la somme des vrais positifs et des faux positifs. Indique la fiabilité des prédictions positives.\nRecall (Rappel) : Nombre de vrais positifs divisé par la somme des vrais positifs et des faux négatifs. Indique la capacité du modèle à détecter les échantillons positifs.\nF1-score : Moyenne harmonique entre précision et rappel, utile lorsque les classes sont déséquilibrées.\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Chargement des ensembles de données déjà préparés\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Recherche de la meilleure profondeur d'arbre en utilisant l'ensemble de validation\ndepth_range = range(1, 30)\ntrain_accuracies = []\nval_accuracies = []\n\nfor depth in depth_range:\n    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n    tree.fit(X_train, y_train)\n    \n    # Évaluation sur l'ensemble d'entraînement\n    y_train_pred = tree.predict(X_train)\n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    \n    # Évaluation sur l'ensemble de validation\n    y_val_pred = tree.predict(X_val)\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection de la meilleure profondeur basée sur l'ensemble de validation\nbest_depth = depth_range[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleure profondeur d'arbre: {best_depth}\")\n\n# Affichage du graphique comparant l'entraînement et la validation\nplt.figure(figsize=(8, 6))\nplt.plot(depth_range, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(depth_range, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Profondeur de l'arbre\")\nplt.ylabel(\"Taux de bonnes prédictions\")\nplt.title(\"Optimisation de la profondeur de l'arbre pour CART\")\nplt.legend()\nplt.show()\n\n# Modèle final avec la meilleure profondeur\ntree = DecisionTreeClassifier(max_depth=best_depth, random_state=42)\ntree.fit(X_train, y_train)\n\n# Évaluation sur l'ensemble de test\ny_test_pred = tree.predict(X_test)\n\n# Affichage de la matrice de confusion avec annotations\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion (les lignes représentent les vraies classes et les colonnes les classes prédites) :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\nMeilleure profondeur d'arbre: 14\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion (les lignes représentent les vraies classes et les colonnes les classes prédites) :\n[[1281  391    0    0    3    0   33]\n [ 399 1802   25    0   13   21    1]\n [   2   34  205    5    0   35    0]\n [   0    0    5   12    0    4    0]\n [   2   50    1    0   21    0    0]\n [   1   29   52    0    1   61    0]\n [  39    3    0    0    0    0  117]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.74      0.75      0.75      1708\n           2       0.78      0.80      0.79      2261\n           3       0.71      0.73      0.72       281\n           4       0.71      0.57      0.63        21\n           5       0.55      0.28      0.38        74\n           6       0.50      0.42      0.46       144\n           7       0.77      0.74      0.75       159\n\n    accuracy                           0.75      4648\n   macro avg       0.68      0.61      0.64      4648\nweighted avg       0.75      0.75      0.75      4648"
  },
  {
    "objectID": "arbre_decision.html#théorie",
    "href": "arbre_decision.html#théorie",
    "title": "Arbre de Décision - CART",
    "section": "",
    "text": "L’algorithme CART (Classification and Regression Trees) est un modèle d’apprentissage supervisé qui construit un arbre de décision en divisant l’espace des caractéristiques en sous-ensembles homogènes."
  },
  {
    "objectID": "arbre_decision.html#évaluation-des-performances",
    "href": "arbre_decision.html#évaluation-des-performances",
    "title": "Arbre de Décision - CART",
    "section": "",
    "text": "Lorsqu’on évalue un modèle de classification, plusieurs métriques sont utilisées :\n\nMatrice de confusion : Tableau qui résume les performances du modèle en comparant les vraies classes aux classes prédites. Les lignes correspondent aux classes réelles, et les colonnes aux classes prédites.\nAccuracy (Précision globale) : Proportion des prédictions correctes parmi l’ensemble des données.\nPrecision (Précision par classe) : Nombre de vrais positifs divisé par la somme des vrais positifs et des faux positifs. Indique la fiabilité des prédictions positives.\nRecall (Rappel) : Nombre de vrais positifs divisé par la somme des vrais positifs et des faux négatifs. Indique la capacité du modèle à détecter les échantillons positifs.\nF1-score : Moyenne harmonique entre précision et rappel, utile lorsque les classes sont déséquilibrées."
  },
  {
    "objectID": "arbre_decision.html#exemple-en-python",
    "href": "arbre_decision.html#exemple-en-python",
    "title": "Arbre de Décision - CART",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Chargement des ensembles de données déjà préparés\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Recherche de la meilleure profondeur d'arbre en utilisant l'ensemble de validation\ndepth_range = range(1, 30)\ntrain_accuracies = []\nval_accuracies = []\n\nfor depth in depth_range:\n    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n    tree.fit(X_train, y_train)\n    \n    # Évaluation sur l'ensemble d'entraînement\n    y_train_pred = tree.predict(X_train)\n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    \n    # Évaluation sur l'ensemble de validation\n    y_val_pred = tree.predict(X_val)\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection de la meilleure profondeur basée sur l'ensemble de validation\nbest_depth = depth_range[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleure profondeur d'arbre: {best_depth}\")\n\n# Affichage du graphique comparant l'entraînement et la validation\nplt.figure(figsize=(8, 6))\nplt.plot(depth_range, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(depth_range, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Profondeur de l'arbre\")\nplt.ylabel(\"Taux de bonnes prédictions\")\nplt.title(\"Optimisation de la profondeur de l'arbre pour CART\")\nplt.legend()\nplt.show()\n\n# Modèle final avec la meilleure profondeur\ntree = DecisionTreeClassifier(max_depth=best_depth, random_state=42)\ntree.fit(X_train, y_train)\n\n# Évaluation sur l'ensemble de test\ny_test_pred = tree.predict(X_test)\n\n# Affichage de la matrice de confusion avec annotations\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion (les lignes représentent les vraies classes et les colonnes les classes prédites) :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\nMeilleure profondeur d'arbre: 14\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion (les lignes représentent les vraies classes et les colonnes les classes prédites) :\n[[1281  391    0    0    3    0   33]\n [ 399 1802   25    0   13   21    1]\n [   2   34  205    5    0   35    0]\n [   0    0    5   12    0    4    0]\n [   2   50    1    0   21    0    0]\n [   1   29   52    0    1   61    0]\n [  39    3    0    0    0    0  117]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.74      0.75      0.75      1708\n           2       0.78      0.80      0.79      2261\n           3       0.71      0.73      0.72       281\n           4       0.71      0.57      0.63        21\n           5       0.55      0.28      0.38        74\n           6       0.50      0.42      0.46       144\n           7       0.77      0.74      0.75       159\n\n    accuracy                           0.75      4648\n   macro avg       0.68      0.61      0.64      4648\nweighted avg       0.75      0.75      0.75      4648"
  },
  {
    "objectID": "bayesien_naif.html",
    "href": "bayesien_naif.html",
    "title": "Classification - Bayésien Naïf",
    "section": "",
    "text": "Le classificateur Bayésien Naïf repose sur le théorème de Bayes et l’hypothèse d’indépendance conditionnelle entre les variables explicatives. Il est particulièrement efficace pour les problèmes de classification textuelle et fonctionne bien même avec peu de données d’entraînement.\n\n\n\nContrairement à d’autres modèles, le Bayésien Naïf possède peu d’hyperparamètres. Cependant, le paramètre var_smoothing dans GaussianNB permet d’éviter les divisions par zéro en ajoutant un lissage aux variances estimées.\nNous allons rechercher la meilleure valeur de var_smoothing en testant plusieurs options.\n\n\n\nLorsqu’on évalue un modèle de classification, plusieurs métriques sont utilisées :\n\nMatrice de confusion : Tableau qui résume les performances du modèle en comparant les vraies classes aux classes prédites. Les lignes correspondent aux classes réelles, et les colonnes aux classes prédites.\nAccuracy (Précision globale) : Proportion des prédictions correctes parmi l’ensemble des données.\nPrecision (Précision par classe) : Nombre de vrais positifs divisé par la somme des vrais positifs et des faux positifs. Indique la fiabilité des prédictions positives.\nRecall (Rappel) : Nombre de vrais positifs divisé par la somme des vrais positifs et des faux négatifs. Indique la capacité du modèle à détecter les échantillons positifs.\nF1-score : Moyenne harmonique entre précision et rappel, utile lorsque les classes sont déséquilibrées.\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Chargement des ensembles de données déjà préparés\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Recherche du meilleur var_smoothing\nvar_smoothing_values = np.logspace(-9, 0, 10)\nval_accuracies = []\n\nfor smoothing in var_smoothing_values:\n    gnb = GaussianNB(var_smoothing=smoothing)\n    gnb.fit(X_train, y_train)\n    y_val_pred = gnb.predict(X_val)\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur var_smoothing\nbest_smoothing = var_smoothing_values[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur var_smoothing: {best_smoothing:.1e}\")\n\n# Affichage du graphique de performance\nplt.figure(figsize=(8, 6))\nplt.plot(var_smoothing_values, val_accuracies, marker='o', linestyle='dashed', label='Validation Accuracy')\nplt.xscale('log')\nplt.xlabel(\"Valeur de var_smoothing\")\nplt.ylabel(\"Taux de bonnes prédictions\")\nplt.title(\"Optimisation du paramètre var_smoothing pour GaussianNB\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur hyperparamètre\ngnb = GaussianNB(var_smoothing=best_smoothing)\ngnb.fit(X_train, y_train)\n\n# Évaluation sur l'ensemble de test\ny_test_pred = gnb.predict(X_test)\n\n# Affichage de la matrice de confusion avec annotations\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion (les lignes représentent les vraies classes et les colonnes les classes prédites) :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\nMeilleur var_smoothing: 1.0e-02\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion (les lignes représentent les vraies classes et les colonnes les classes prédites) :\n[[1175  515    3    0    0    1   14]\n [ 512 1585  154    0    0    3    7]\n [   0   38  242    0    0    1    0]\n [   0    0   21    0    0    0    0]\n [   0   66    8    0    0    0    0]\n [   0   29  113    0    0    2    0]\n [ 150    0    0    0    0    0    9]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.64      0.69      0.66      1708\n           2       0.71      0.70      0.71      2261\n           3       0.45      0.86      0.59       281\n           4       0.00      0.00      0.00        21\n           5       0.00      0.00      0.00        74\n           6       0.29      0.01      0.03       144\n           7       0.30      0.06      0.10       159\n\n    accuracy                           0.65      4648\n   macro avg       0.34      0.33      0.30      4648\nweighted avg       0.63      0.65      0.63      4648\n\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))"
  },
  {
    "objectID": "bayesien_naif.html#théorie",
    "href": "bayesien_naif.html#théorie",
    "title": "Classification - Bayésien Naïf",
    "section": "",
    "text": "Le classificateur Bayésien Naïf repose sur le théorème de Bayes et l’hypothèse d’indépendance conditionnelle entre les variables explicatives. Il est particulièrement efficace pour les problèmes de classification textuelle et fonctionne bien même avec peu de données d’entraînement."
  },
  {
    "objectID": "bayesien_naif.html#évaluation-des-performances",
    "href": "bayesien_naif.html#évaluation-des-performances",
    "title": "Classification - Bayésien Naïf",
    "section": "",
    "text": "Lorsqu’on évalue un modèle de classification, plusieurs métriques sont utilisées :\n\nMatrice de confusion : Tableau qui résume les performances du modèle en comparant les vraies classes aux classes prédites. Les lignes correspondent aux classes réelles, et les colonnes aux classes prédites.\nAccuracy (Précision globale) : Proportion des prédictions correctes parmi l’ensemble des données.\nPrecision (Précision par classe) : Nombre de vrais positifs divisé par la somme des vrais positifs et des faux positifs. Indique la fiabilité des prédictions positives.\nRecall (Rappel) : Nombre de vrais positifs divisé par la somme des vrais positifs et des faux négatifs. Indique la capacité du modèle à détecter les échantillons positifs.\nF1-score : Moyenne harmonique entre précision et rappel, utile lorsque les classes sont déséquilibrées."
  },
  {
    "objectID": "bayesien_naif.html#exemple-en-python",
    "href": "bayesien_naif.html#exemple-en-python",
    "title": "Classification - Bayésien Naïf",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Chargement des ensembles de données déjà préparés\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Recherche du meilleur var_smoothing\nvar_smoothing_values = np.logspace(-9, 0, 10)\nval_accuracies = []\n\nfor smoothing in var_smoothing_values:\n    gnb = GaussianNB(var_smoothing=smoothing)\n    gnb.fit(X_train, y_train)\n    y_val_pred = gnb.predict(X_val)\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur var_smoothing\nbest_smoothing = var_smoothing_values[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur var_smoothing: {best_smoothing:.1e}\")\n\n# Affichage du graphique de performance\nplt.figure(figsize=(8, 6))\nplt.plot(var_smoothing_values, val_accuracies, marker='o', linestyle='dashed', label='Validation Accuracy')\nplt.xscale('log')\nplt.xlabel(\"Valeur de var_smoothing\")\nplt.ylabel(\"Taux de bonnes prédictions\")\nplt.title(\"Optimisation du paramètre var_smoothing pour GaussianNB\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur hyperparamètre\ngnb = GaussianNB(var_smoothing=best_smoothing)\ngnb.fit(X_train, y_train)\n\n# Évaluation sur l'ensemble de test\ny_test_pred = gnb.predict(X_test)\n\n# Affichage de la matrice de confusion avec annotations\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion (les lignes représentent les vraies classes et les colonnes les classes prédites) :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\nMeilleur var_smoothing: 1.0e-02\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion (les lignes représentent les vraies classes et les colonnes les classes prédites) :\n[[1175  515    3    0    0    1   14]\n [ 512 1585  154    0    0    3    7]\n [   0   38  242    0    0    1    0]\n [   0    0   21    0    0    0    0]\n [   0   66    8    0    0    0    0]\n [   0   29  113    0    0    2    0]\n [ 150    0    0    0    0    0    9]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.64      0.69      0.66      1708\n           2       0.71      0.70      0.71      2261\n           3       0.45      0.86      0.59       281\n           4       0.00      0.00      0.00        21\n           5       0.00      0.00      0.00        74\n           6       0.29      0.01      0.03       144\n           7       0.30      0.06      0.10       159\n\n    accuracy                           0.65      4648\n   macro avg       0.34      0.33      0.30      4648\nweighted avg       0.63      0.65      0.63      4648\n\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))"
  },
  {
    "objectID": "bayesien_naif.html#hyperparamètres",
    "href": "bayesien_naif.html#hyperparamètres",
    "title": "Classification - Bayésien Naïf",
    "section": "",
    "text": "Contrairement à d’autres modèles, le Bayésien Naïf possède peu d’hyperparamètres. Cependant, le paramètre var_smoothing dans GaussianNB permet d’éviter les divisions par zéro en ajoutant un lissage aux variances estimées.\nNous allons rechercher la meilleure valeur de var_smoothing en testant plusieurs options."
  },
  {
    "objectID": "lda.html",
    "href": "lda.html",
    "title": "Analyse Discriminante Linéaire - LDA",
    "section": "",
    "text": "L’Analyse Discriminante Linéaire (LDA) est une technique de classification qui cherche à trouver une combinaison linéaire de caractéristiques maximisant la séparation entre les classes.\nLDA est particulièrement utile lorsque les classes suivent une distribution normale et ont des variances similaires.\n\n\n\nLDA possède peu d’hyperparamètres. Cependant, on peut ajuster le paramètre solver qui détermine la méthode utilisée pour trouver les coefficients du modèle :\n\nsvd : Convient aux grands ensembles de données et ne suppose pas d’égalité des variances.\nlsqr : Méthode rapide qui fonctionne bien lorsque les covariances des classes sont similaires.\n\nNous avons exclu le solver eigen car il peut poser des problèmes de stabilité numérique lorsque les matrices de covariance sont mal conditionnées.\nNous allons tester les deux solveurs et choisir celui qui maximise la performance dans les configurations OVA (One-Versus-All) et OVO (One-Versus-One).\n\n\n\nComme pour les autres modèles de classification, nous utilisons :\n\nMatrice de confusion : Compare les vraies classes aux classes prédites.\nAccuracy (Précision globale) : Pourcentage de bonnes prédictions.\nPrecision (Précision par classe) : Fiabilité des prédictions positives.\nRecall (Rappel) : Capacité du modèle à détecter les classes positives.\nF1-score : Moyenne harmonique entre précision et rappel.\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche du meilleur solver\nsolvers = ['svd', 'lsqr']\nval_accuracies_ova = []\nval_accuracies_ovo = []\n\nfor solver in solvers:\n    # One-Versus-All (OVA)\n    lda_ova = OneVsRestClassifier(LinearDiscriminantAnalysis(solver=solver))\n    lda_ova.fit(X_train, y_train)\n    y_val_pred_ova = lda_ova.predict(X_val)\n    val_accuracies_ova.append(accuracy_score(y_val, y_val_pred_ova))\n    \n    # One-Versus-One (OVO)\n    lda_ovo = OneVsOneClassifier(LinearDiscriminantAnalysis(solver=solver))\n    lda_ovo.fit(X_train, y_train)\n    y_val_pred_ovo = lda_ovo.predict(X_val)\n    val_accuracies_ovo.append(accuracy_score(y_val, y_val_pred_ovo))\n\n# Sélection du meilleur solver pour OVA et OVO\nbest_solver_ova = solvers[val_accuracies_ova.index(max(val_accuracies_ova))]\nbest_solver_ovo = solvers[val_accuracies_ovo.index(max(val_accuracies_ovo))]\nprint(f\"Meilleur solver LDA (OVA): {best_solver_ova}\")\nprint(f\"Meilleur solver LDA (OVO): {best_solver_ovo}\")\n\n# Modèle final avec le meilleur solver pour OVA\nlda_ova = OneVsRestClassifier(LinearDiscriminantAnalysis(solver=best_solver_ova))\nlda_ova.fit(X_train, y_train)\ny_test_pred_ova = lda_ova.predict(X_test)\n\n# Modèle final avec le meilleur solver pour OVO\nlda_ovo = OneVsOneClassifier(LinearDiscriminantAnalysis(solver=best_solver_ovo))\nlda_ovo.fit(X_train, y_train)\ny_test_pred_ovo = lda_ovo.predict(X_test)\n\n# Affichage des résultats\nprint(\"\\nÉvaluation sur l'ensemble de test (OVA)\")\nprint(classification_report(y_test, y_test_pred_ova))\n\nprint(\"\\nÉvaluation sur l'ensemble de test (OVO)\")\nprint(classification_report(y_test, y_test_pred_ovo))\n\n# Affichage des matrices de confusion\nconf_matrix_ova = confusion_matrix(y_test, y_test_pred_ova)\nconf_matrix_ovo = confusion_matrix(y_test, y_test_pred_ovo)\nprint(\"\\nMatrice de confusion (OVA) :\")\nprint(conf_matrix_ova)\nprint(\"\\nMatrice de confusion (OVO) :\")\nprint(conf_matrix_ovo)\n\nMeilleur solver LDA (OVA): svd\nMeilleur solver LDA (OVO): svd\n\nÉvaluation sur l'ensemble de test (OVA)\n              precision    recall  f1-score   support\n\n           1       0.66      0.58      0.62      1178\n           2       0.64      0.69      0.67      1571\n           3       0.47      0.52      0.49       382\n           4       0.82      0.69      0.75       379\n           5       0.52      0.39      0.44       379\n           6       0.46      0.56      0.51       379\n           7       0.74      0.83      0.78       380\n\n    accuracy                           0.63      4648\n   macro avg       0.62      0.61      0.61      4648\nweighted avg       0.63      0.63      0.62      4648\n\n\nÉvaluation sur l'ensemble de test (OVO)\n              precision    recall  f1-score   support\n\n           1       0.70      0.59      0.64      1178\n           2       0.60      0.83      0.70      1571\n           3       0.53      0.62      0.57       382\n           4       0.72      0.92      0.81       379\n           5       0.67      0.12      0.20       379\n           6       0.57      0.21      0.31       379\n           7       0.84      0.75      0.79       380\n\n    accuracy                           0.64      4648\n   macro avg       0.66      0.58      0.57      4648\nweighted avg       0.65      0.64      0.62      4648\n\n\nMatrice de confusion (OVA) :\n[[ 685  365    1    0   21    1  105]\n [ 290 1087   29    3   97   58    7]\n [   0   20  197   27    4  134    0]\n [   0    0   74  262    0   43    0]\n [  11  175   37    0  148    8    0]\n [   0   43   82   27   16  211    0]\n [  58    3    1    0    1    0  317]]\n\nMatrice de confusion (OVO) :\n[[ 695  432    1    0    0    0   50]\n [ 210 1305   33    1   10    7    5]\n [   0   27  235   79    6   35    0]\n [   0    0   11  349    0   19    0]\n [   0  316   19    0   44    0    0]\n [   0   91  146   55    6   81    0]\n [  92    3    1    0    0    0  284]]"
  },
  {
    "objectID": "lda.html#théorie",
    "href": "lda.html#théorie",
    "title": "Analyse Discriminante Linéaire - LDA",
    "section": "",
    "text": "L’Analyse Discriminante Linéaire (LDA) est une technique de classification qui cherche à trouver une combinaison linéaire de caractéristiques maximisant la séparation entre les classes.\nLDA est particulièrement utile lorsque les classes suivent une distribution normale et ont des variances similaires."
  },
  {
    "objectID": "lda.html#hyperparamètres",
    "href": "lda.html#hyperparamètres",
    "title": "Analyse Discriminante Linéaire - LDA",
    "section": "",
    "text": "LDA possède peu d’hyperparamètres. Cependant, on peut ajuster le paramètre solver qui détermine la méthode utilisée pour trouver les coefficients du modèle :\n\nsvd : Convient aux grands ensembles de données et ne suppose pas d’égalité des variances.\nlsqr : Méthode rapide qui fonctionne bien lorsque les covariances des classes sont similaires.\n\nNous avons exclu le solver eigen car il peut poser des problèmes de stabilité numérique lorsque les matrices de covariance sont mal conditionnées.\nNous allons tester les deux solveurs et choisir celui qui maximise la performance dans les configurations OVA (One-Versus-All) et OVO (One-Versus-One)."
  },
  {
    "objectID": "lda.html#évaluation-des-performances",
    "href": "lda.html#évaluation-des-performances",
    "title": "Analyse Discriminante Linéaire - LDA",
    "section": "",
    "text": "Comme pour les autres modèles de classification, nous utilisons :\n\nMatrice de confusion : Compare les vraies classes aux classes prédites.\nAccuracy (Précision globale) : Pourcentage de bonnes prédictions.\nPrecision (Précision par classe) : Fiabilité des prédictions positives.\nRecall (Rappel) : Capacité du modèle à détecter les classes positives.\nF1-score : Moyenne harmonique entre précision et rappel."
  },
  {
    "objectID": "lda.html#exemple-en-python",
    "href": "lda.html#exemple-en-python",
    "title": "Analyse Discriminante Linéaire - LDA",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche du meilleur solver\nsolvers = ['svd', 'lsqr']\nval_accuracies_ova = []\nval_accuracies_ovo = []\n\nfor solver in solvers:\n    # One-Versus-All (OVA)\n    lda_ova = OneVsRestClassifier(LinearDiscriminantAnalysis(solver=solver))\n    lda_ova.fit(X_train, y_train)\n    y_val_pred_ova = lda_ova.predict(X_val)\n    val_accuracies_ova.append(accuracy_score(y_val, y_val_pred_ova))\n    \n    # One-Versus-One (OVO)\n    lda_ovo = OneVsOneClassifier(LinearDiscriminantAnalysis(solver=solver))\n    lda_ovo.fit(X_train, y_train)\n    y_val_pred_ovo = lda_ovo.predict(X_val)\n    val_accuracies_ovo.append(accuracy_score(y_val, y_val_pred_ovo))\n\n# Sélection du meilleur solver pour OVA et OVO\nbest_solver_ova = solvers[val_accuracies_ova.index(max(val_accuracies_ova))]\nbest_solver_ovo = solvers[val_accuracies_ovo.index(max(val_accuracies_ovo))]\nprint(f\"Meilleur solver LDA (OVA): {best_solver_ova}\")\nprint(f\"Meilleur solver LDA (OVO): {best_solver_ovo}\")\n\n# Modèle final avec le meilleur solver pour OVA\nlda_ova = OneVsRestClassifier(LinearDiscriminantAnalysis(solver=best_solver_ova))\nlda_ova.fit(X_train, y_train)\ny_test_pred_ova = lda_ova.predict(X_test)\n\n# Modèle final avec le meilleur solver pour OVO\nlda_ovo = OneVsOneClassifier(LinearDiscriminantAnalysis(solver=best_solver_ovo))\nlda_ovo.fit(X_train, y_train)\ny_test_pred_ovo = lda_ovo.predict(X_test)\n\n# Affichage des résultats\nprint(\"\\nÉvaluation sur l'ensemble de test (OVA)\")\nprint(classification_report(y_test, y_test_pred_ova))\n\nprint(\"\\nÉvaluation sur l'ensemble de test (OVO)\")\nprint(classification_report(y_test, y_test_pred_ovo))\n\n# Affichage des matrices de confusion\nconf_matrix_ova = confusion_matrix(y_test, y_test_pred_ova)\nconf_matrix_ovo = confusion_matrix(y_test, y_test_pred_ovo)\nprint(\"\\nMatrice de confusion (OVA) :\")\nprint(conf_matrix_ova)\nprint(\"\\nMatrice de confusion (OVO) :\")\nprint(conf_matrix_ovo)\n\nMeilleur solver LDA (OVA): svd\nMeilleur solver LDA (OVO): svd\n\nÉvaluation sur l'ensemble de test (OVA)\n              precision    recall  f1-score   support\n\n           1       0.66      0.58      0.62      1178\n           2       0.64      0.69      0.67      1571\n           3       0.47      0.52      0.49       382\n           4       0.82      0.69      0.75       379\n           5       0.52      0.39      0.44       379\n           6       0.46      0.56      0.51       379\n           7       0.74      0.83      0.78       380\n\n    accuracy                           0.63      4648\n   macro avg       0.62      0.61      0.61      4648\nweighted avg       0.63      0.63      0.62      4648\n\n\nÉvaluation sur l'ensemble de test (OVO)\n              precision    recall  f1-score   support\n\n           1       0.70      0.59      0.64      1178\n           2       0.60      0.83      0.70      1571\n           3       0.53      0.62      0.57       382\n           4       0.72      0.92      0.81       379\n           5       0.67      0.12      0.20       379\n           6       0.57      0.21      0.31       379\n           7       0.84      0.75      0.79       380\n\n    accuracy                           0.64      4648\n   macro avg       0.66      0.58      0.57      4648\nweighted avg       0.65      0.64      0.62      4648\n\n\nMatrice de confusion (OVA) :\n[[ 685  365    1    0   21    1  105]\n [ 290 1087   29    3   97   58    7]\n [   0   20  197   27    4  134    0]\n [   0    0   74  262    0   43    0]\n [  11  175   37    0  148    8    0]\n [   0   43   82   27   16  211    0]\n [  58    3    1    0    1    0  317]]\n\nMatrice de confusion (OVO) :\n[[ 695  432    1    0    0    0   50]\n [ 210 1305   33    1   10    7    5]\n [   0   27  235   79    6   35    0]\n [   0    0   11  349    0   19    0]\n [   0  316   19    0   44    0    0]\n [   0   91  146   55    6   81    0]\n [  92    3    1    0    0    0  284]]"
  },
  {
    "objectID": "reseau_neurones.html",
    "href": "reseau_neurones.html",
    "title": "Réseau de Neurones - Classification",
    "section": "",
    "text": "Un réseau de neurones multi-couches (MLP - Multi-Layer Perceptron) est un modèle d’apprentissage supervisé basé sur des couches de neurones artificiels. Il est particulièrement efficace pour la classification non linéaire.\nDans notre cas, nous utilisons une couche de sortie Softmax, qui permet de normaliser les sorties du réseau en probabilités pour une classification multiclasses.\n\n\n\nLors de l’entraînement d’un réseau de neurones, plusieurs hyperparamètres influencent la performance :\n\nTaille du pas d’apprentissage (learning_rate) : Déterminée automatiquement avec l’optimiseur Adam.\nNombre de couches cachées et neurones par couche : Influence la capacité d’apprentissage du modèle.\nNombre d’époques (epochs) : Nombre de fois que le modèle parcourt l’ensemble des données d’entraînement.\nTaille du batch (batch_size) : Nombre d’exemples utilisés pour calculer une mise à jour des poids.\n\nNous allons utiliser Adam avec un taux d’apprentissage adaptatif, ce qui signifie que le pas d’apprentissage s’ajustera automatiquement au fil des itérations.\n\n\n\nComme pour les autres modèles de classification, nous utilisons :\n\nMatrice de confusion : Un tableau qui compare les classes réelles aux classes prédites. Chaque ligne représente une classe réelle et chaque colonne une classe prédite. Une bonne classification est indiquée par des valeurs élevées sur la diagonale principale, tandis que les erreurs de classification apparaissent en dehors de cette diagonale.\nAccuracy (Précision globale) : Le pourcentage de prédictions correctes parmi l’ensemble des données. Elle est utile lorsque les classes sont équilibrées, mais peut être trompeuse si certaines classes sont largement majoritaires.\nPrecision (Précision par classe) : Pour une classe donnée, la précision mesure la proportion de prédictions correctes parmi toutes celles où le modèle a prédit cette classe. Une haute précision signifie que lorsqu’une classe est prédite, elle est souvent correcte.\nRecall (Rappel par classe) : Le rappel mesure la proportion de vrais exemples d’une classe qui sont correctement détectés par le modèle. Une haute valeur signifie que le modèle détecte bien les échantillons appartenant à cette classe.\nF1-score : Moyenne harmonique entre la précision et le rappel. Il équilibre ces deux mesures et est particulièrement utile lorsque les classes sont déséquilibrées.\n\nUne moyenne macro est utilisée pour donner un aperçu global des performances du modèle en attribuant un poids égal à chaque classe. Une moyenne pondérée (weighted avg) prend en compte la fréquence de chaque classe afin de refléter plus précisément la performance globale dans un contexte de classes déséquilibrées.\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type'] - 1  # Ajustement des labels pour correspondre à l'indexation Python\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type'] - 1\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type'] - 1\n\n# Normalisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Définition du modèle\nmodel = keras.Sequential([\n    keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(len(set(y_train)), activation='softmax')\n])\n\n# Utilisation d'Adam avec un pas d'apprentissage adaptatif\noptimizer = keras.optimizers.Adam()\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Entraînement du modèle\nhistory = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=32, verbose=0)\n\n# Détermination de la meilleure époque\nbest_epoch = history.history['val_accuracy'].index(max(history.history['val_accuracy'])) + 1\nbest_val_acc = max(history.history['val_accuracy'])\n\n# Affichage des courbes d'entraînement\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, 101), history.history['accuracy'], label='Train Accuracy')\nplt.plot(range(1, 101), history.history['val_accuracy'], label='Validation Accuracy')\nplt.axvline(best_epoch, color='r', linestyle='--', label=f'Best Epoch: {best_epoch}')\nplt.xlabel(\"Époques\")\nplt.ylabel(\"Taux de bonnes prédictions\")\nplt.title(\"Optimisation du modèle de réseau de neurones\")\nplt.legend()\nplt.show()\n\n# Ré-entraîner le modèle avec la meilleure époque\nmodel.fit(X_train, y_train, epochs=best_epoch, batch_size=32, verbose=0)\n\n# Évaluation sur l'ensemble de test\ny_test_pred = model.predict(X_test)\ny_test_pred_classes = y_test_pred.argmax(axis=1)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred_classes)\nprint(f\"\\nMeilleure époque : **{best_epoch}** avec une précision de validation de **{best_val_acc:.4f}**\")\nprint(\"\\nMatrice de confusion (les lignes représentent les vraies classes et les colonnes les classes prédites) :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred_classes))\n\n2025-02-20 15:23:17.178162: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-02-20 15:23:17.178735: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-02-20 15:23:17.181110: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-02-20 15:23:17.188202: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1740061397.200311  389951 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1740061397.204092  389951 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-02-20 15:23:17.216248: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n2025-02-20 15:23:18.557685: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n\n\n\n\n\n\n\n\n\n  1/146 ━━━━━━━━━━━━━━━━━━━━ 6s 43ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71/146 ━━━━━━━━━━━━━━━━━━━━ 0s 724us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b146/146 ━━━━━━━━━━━━━━━━━━━━ 0s 787us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b146/146 ━━━━━━━━━━━━━━━━━━━━ 0s 916us/step\n\nMeilleure époque : **89** avec une précision de validation de **0.8193**\n\nMatrice de confusion (les lignes représentent les vraies classes et les colonnes les classes prédites) :\n[[ 928  201    0    0    9    1   39]\n [ 223 1264   19    0   44   17    4]\n [   0    5  324   14    4   35    0]\n [   0    0   15  356    0    8    0]\n [   1   48    7    0  320    3    0]\n [   3   24   53    8    2  289    0]\n [  13    6    0    0    1    0  360]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           0       0.79      0.79      0.79      1178\n           1       0.82      0.80      0.81      1571\n           2       0.78      0.85      0.81       382\n           3       0.94      0.94      0.94       379\n           4       0.84      0.84      0.84       379\n           5       0.82      0.76      0.79       379\n           6       0.89      0.95      0.92       380\n\n    accuracy                           0.83      4648\n   macro avg       0.84      0.85      0.84      4648\nweighted avg       0.83      0.83      0.83      4648"
  },
  {
    "objectID": "reseau_neurones.html#théorie",
    "href": "reseau_neurones.html#théorie",
    "title": "Réseau de Neurones - Classification",
    "section": "",
    "text": "Un réseau de neurones multi-couches (MLP - Multi-Layer Perceptron) est un modèle d’apprentissage supervisé basé sur des couches de neurones artificiels. Il est particulièrement efficace pour la classification non linéaire.\nDans notre cas, nous utilisons une couche de sortie Softmax, qui permet de normaliser les sorties du réseau en probabilités pour une classification multiclasses."
  },
  {
    "objectID": "reseau_neurones.html#hyperparamètres",
    "href": "reseau_neurones.html#hyperparamètres",
    "title": "Réseau de Neurones - Classification",
    "section": "",
    "text": "Lors de l’entraînement d’un réseau de neurones, plusieurs hyperparamètres influencent la performance :\n\nTaille du pas d’apprentissage (learning_rate) : Déterminée automatiquement avec l’optimiseur Adam.\nNombre de couches cachées et neurones par couche : Influence la capacité d’apprentissage du modèle.\nNombre d’époques (epochs) : Nombre de fois que le modèle parcourt l’ensemble des données d’entraînement.\nTaille du batch (batch_size) : Nombre d’exemples utilisés pour calculer une mise à jour des poids.\n\nNous allons utiliser Adam avec un taux d’apprentissage adaptatif, ce qui signifie que le pas d’apprentissage s’ajustera automatiquement au fil des itérations."
  },
  {
    "objectID": "reseau_neurones.html#évaluation-des-performances",
    "href": "reseau_neurones.html#évaluation-des-performances",
    "title": "Réseau de Neurones - Classification",
    "section": "",
    "text": "Comme pour les autres modèles de classification, nous utilisons :\n\nMatrice de confusion : Un tableau qui compare les classes réelles aux classes prédites. Chaque ligne représente une classe réelle et chaque colonne une classe prédite. Une bonne classification est indiquée par des valeurs élevées sur la diagonale principale, tandis que les erreurs de classification apparaissent en dehors de cette diagonale.\nAccuracy (Précision globale) : Le pourcentage de prédictions correctes parmi l’ensemble des données. Elle est utile lorsque les classes sont équilibrées, mais peut être trompeuse si certaines classes sont largement majoritaires.\nPrecision (Précision par classe) : Pour une classe donnée, la précision mesure la proportion de prédictions correctes parmi toutes celles où le modèle a prédit cette classe. Une haute précision signifie que lorsqu’une classe est prédite, elle est souvent correcte.\nRecall (Rappel par classe) : Le rappel mesure la proportion de vrais exemples d’une classe qui sont correctement détectés par le modèle. Une haute valeur signifie que le modèle détecte bien les échantillons appartenant à cette classe.\nF1-score : Moyenne harmonique entre la précision et le rappel. Il équilibre ces deux mesures et est particulièrement utile lorsque les classes sont déséquilibrées.\n\nUne moyenne macro est utilisée pour donner un aperçu global des performances du modèle en attribuant un poids égal à chaque classe. Une moyenne pondérée (weighted avg) prend en compte la fréquence de chaque classe afin de refléter plus précisément la performance globale dans un contexte de classes déséquilibrées."
  },
  {
    "objectID": "reseau_neurones.html#exemple-en-python",
    "href": "reseau_neurones.html#exemple-en-python",
    "title": "Réseau de Neurones - Classification",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type'] - 1  # Ajustement des labels pour correspondre à l'indexation Python\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type'] - 1\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type'] - 1\n\n# Normalisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Définition du modèle\nmodel = keras.Sequential([\n    keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(len(set(y_train)), activation='softmax')\n])\n\n# Utilisation d'Adam avec un pas d'apprentissage adaptatif\noptimizer = keras.optimizers.Adam()\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Entraînement du modèle\nhistory = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=32, verbose=0)\n\n# Détermination de la meilleure époque\nbest_epoch = history.history['val_accuracy'].index(max(history.history['val_accuracy'])) + 1\nbest_val_acc = max(history.history['val_accuracy'])\n\n# Affichage des courbes d'entraînement\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, 101), history.history['accuracy'], label='Train Accuracy')\nplt.plot(range(1, 101), history.history['val_accuracy'], label='Validation Accuracy')\nplt.axvline(best_epoch, color='r', linestyle='--', label=f'Best Epoch: {best_epoch}')\nplt.xlabel(\"Époques\")\nplt.ylabel(\"Taux de bonnes prédictions\")\nplt.title(\"Optimisation du modèle de réseau de neurones\")\nplt.legend()\nplt.show()\n\n# Ré-entraîner le modèle avec la meilleure époque\nmodel.fit(X_train, y_train, epochs=best_epoch, batch_size=32, verbose=0)\n\n# Évaluation sur l'ensemble de test\ny_test_pred = model.predict(X_test)\ny_test_pred_classes = y_test_pred.argmax(axis=1)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred_classes)\nprint(f\"\\nMeilleure époque : **{best_epoch}** avec une précision de validation de **{best_val_acc:.4f}**\")\nprint(\"\\nMatrice de confusion (les lignes représentent les vraies classes et les colonnes les classes prédites) :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred_classes))\n\n2025-02-20 15:23:17.178162: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-02-20 15:23:17.178735: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-02-20 15:23:17.181110: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-02-20 15:23:17.188202: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1740061397.200311  389951 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1740061397.204092  389951 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-02-20 15:23:17.216248: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n2025-02-20 15:23:18.557685: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n\n\n\n\n\n\n\n\n\n  1/146 ━━━━━━━━━━━━━━━━━━━━ 6s 43ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71/146 ━━━━━━━━━━━━━━━━━━━━ 0s 724us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b146/146 ━━━━━━━━━━━━━━━━━━━━ 0s 787us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b146/146 ━━━━━━━━━━━━━━━━━━━━ 0s 916us/step\n\nMeilleure époque : **89** avec une précision de validation de **0.8193**\n\nMatrice de confusion (les lignes représentent les vraies classes et les colonnes les classes prédites) :\n[[ 928  201    0    0    9    1   39]\n [ 223 1264   19    0   44   17    4]\n [   0    5  324   14    4   35    0]\n [   0    0   15  356    0    8    0]\n [   1   48    7    0  320    3    0]\n [   3   24   53    8    2  289    0]\n [  13    6    0    0    1    0  360]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           0       0.79      0.79      0.79      1178\n           1       0.82      0.80      0.81      1571\n           2       0.78      0.85      0.81       382\n           3       0.94      0.94      0.94       379\n           4       0.84      0.84      0.84       379\n           5       0.82      0.76      0.79       379\n           6       0.89      0.95      0.92       380\n\n    accuracy                           0.83      4648\n   macro avg       0.84      0.85      0.84      4648\nweighted avg       0.83      0.83      0.83      4648"
  },
  {
    "objectID": "lda_ova.html",
    "href": "lda_ova.html",
    "title": "Analyse Discriminante Linéaire - OVA",
    "section": "",
    "text": "L’Analyse Discriminante Linéaire (LDA) est une technique de classification qui cherche à trouver une combinaison linéaire de caractéristiques maximisant la séparation entre les classes.\nL’approche One-Versus-All (OVA) consiste à entraîner un modèle pour chaque classe, en distinguant chaque classe des autres combinées.\n\n\n\nNous allons tester les hyperparamètres suivants : - Régularisation (shrinkage) : contrôle la variance de la covariance estimée (valeurs entre 0 et 1). - Standardisation des données : normalisation des features avant l’entraînement.\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche des meilleurs hyperparamètres\nshrinkage_values = np.linspace(0, 1, 10)\ntrain_accuracies = []\nval_accuracies = []\n\nfor shrinkage in shrinkage_values:\n    lda_ova = OneVsRestClassifier(LinearDiscriminantAnalysis(solver='lsqr', shrinkage=shrinkage))\n    lda_ova.fit(X_train, y_train)\n    \n    y_train_pred = lda_ova.predict(X_train)\n    y_val_pred = lda_ova.predict(X_val)\n    \n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur shrinkage\nbest_shrinkage = shrinkage_values[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur shrinkage LDA (OVA): {best_shrinkage}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(shrinkage_values, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(shrinkage_values, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Shrinkage\")\nplt.ylabel(\"Précision\")\nplt.title(\"Impact du shrinkage sur la performance de LDA (OVA)\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur shrinkage\nlda_ova = OneVsRestClassifier(LinearDiscriminantAnalysis(solver='lsqr', shrinkage=best_shrinkage))\nlda_ova.fit(X_train, y_train)\ny_test_pred = lda_ova.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion (OVA) :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\nMeilleur shrinkage LDA (OVA): 0.0\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion (OVA) :\n[[1035  478    2    0    6    6  181]\n [ 396 1710   47   12   12   73   11]\n [   0   16  211   17    1   36    0]\n [   0    0    5   12    0    4    0]\n [   2   57   12    0    1    2    0]\n [   0   24   69    2    4   45    0]\n [  24    0    0    0    0    0  135]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.71      0.61      0.65      1708\n           2       0.75      0.76      0.75      2261\n           3       0.61      0.75      0.67       281\n           4       0.28      0.57      0.38        21\n           5       0.04      0.01      0.02        74\n           6       0.27      0.31      0.29       144\n           7       0.41      0.85      0.56       159\n\n    accuracy                           0.68      4648\n   macro avg       0.44      0.55      0.47      4648\nweighted avg       0.69      0.68      0.68      4648"
  },
  {
    "objectID": "lda_ova.html#théorie",
    "href": "lda_ova.html#théorie",
    "title": "Analyse Discriminante Linéaire - OVA",
    "section": "",
    "text": "L’Analyse Discriminante Linéaire (LDA) est une technique de classification qui cherche à trouver une combinaison linéaire de caractéristiques maximisant la séparation entre les classes.\nL’approche One-Versus-All (OVA) consiste à entraîner un modèle pour chaque classe, en distinguant chaque classe des autres combinées."
  },
  {
    "objectID": "lda_ova.html#hyperparamètres",
    "href": "lda_ova.html#hyperparamètres",
    "title": "Analyse Discriminante Linéaire - OVA",
    "section": "",
    "text": "Nous allons tester les hyperparamètres suivants : - Régularisation (shrinkage) : contrôle la variance de la covariance estimée (valeurs entre 0 et 1). - Standardisation des données : normalisation des features avant l’entraînement."
  },
  {
    "objectID": "lda_ova.html#exemple-en-python",
    "href": "lda_ova.html#exemple-en-python",
    "title": "Analyse Discriminante Linéaire - OVA",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche des meilleurs hyperparamètres\nshrinkage_values = np.linspace(0, 1, 10)\ntrain_accuracies = []\nval_accuracies = []\n\nfor shrinkage in shrinkage_values:\n    lda_ova = OneVsRestClassifier(LinearDiscriminantAnalysis(solver='lsqr', shrinkage=shrinkage))\n    lda_ova.fit(X_train, y_train)\n    \n    y_train_pred = lda_ova.predict(X_train)\n    y_val_pred = lda_ova.predict(X_val)\n    \n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur shrinkage\nbest_shrinkage = shrinkage_values[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur shrinkage LDA (OVA): {best_shrinkage}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(shrinkage_values, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(shrinkage_values, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Shrinkage\")\nplt.ylabel(\"Précision\")\nplt.title(\"Impact du shrinkage sur la performance de LDA (OVA)\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur shrinkage\nlda_ova = OneVsRestClassifier(LinearDiscriminantAnalysis(solver='lsqr', shrinkage=best_shrinkage))\nlda_ova.fit(X_train, y_train)\ny_test_pred = lda_ova.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion (OVA) :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\nMeilleur shrinkage LDA (OVA): 0.0\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion (OVA) :\n[[1035  478    2    0    6    6  181]\n [ 396 1710   47   12   12   73   11]\n [   0   16  211   17    1   36    0]\n [   0    0    5   12    0    4    0]\n [   2   57   12    0    1    2    0]\n [   0   24   69    2    4   45    0]\n [  24    0    0    0    0    0  135]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.71      0.61      0.65      1708\n           2       0.75      0.76      0.75      2261\n           3       0.61      0.75      0.67       281\n           4       0.28      0.57      0.38        21\n           5       0.04      0.01      0.02        74\n           6       0.27      0.31      0.29       144\n           7       0.41      0.85      0.56       159\n\n    accuracy                           0.68      4648\n   macro avg       0.44      0.55      0.47      4648\nweighted avg       0.69      0.68      0.68      4648"
  },
  {
    "objectID": "lda_ovo.html",
    "href": "lda_ovo.html",
    "title": "Analyse Discriminante Linéaire - OVO",
    "section": "",
    "text": "L’Analyse Discriminante Linéaire (LDA) est une technique de classification qui cherche à trouver une combinaison linéaire de caractéristiques maximisant la séparation entre les classes.\nL’approche One-Versus-One (OVO) consiste à entraîner un modèle pour chaque paire de classes, ce qui est utile lorsque les classes sont bien séparées.\n\n\n\nNous allons tester les hyperparamètres suivants : - Régularisation (shrinkage) : contrôle la variance de la covariance estimée (valeurs entre 0 et 1). - Standardisation des données : normalisation des features avant l’entraînement.\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche des meilleurs hyperparamètres\nshrinkage_values = np.linspace(0, 1, 10)\ntrain_accuracies = []\nval_accuracies = []\n\nfor shrinkage in shrinkage_values:\n    lda_ovo = OneVsOneClassifier(LinearDiscriminantAnalysis(solver='lsqr', shrinkage=shrinkage))\n    lda_ovo.fit(X_train, y_train)\n    \n    y_train_pred = lda_ovo.predict(X_train)\n    y_val_pred = lda_ovo.predict(X_val)\n    \n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur shrinkage\nbest_shrinkage = shrinkage_values[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur shrinkage LDA (OVO): {best_shrinkage}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(shrinkage_values, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(shrinkage_values, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Shrinkage\")\nplt.ylabel(\"Précision\")\nplt.title(\"Impact du shrinkage sur la performance de LDA (OVO)\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur shrinkage\nlda_ovo = OneVsOneClassifier(LinearDiscriminantAnalysis(solver='lsqr', shrinkage=best_shrinkage))\nlda_ovo.fit(X_train, y_train)\ny_test_pred = lda_ovo.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion (OVO) :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\nMeilleur shrinkage LDA (OVO): 0.1111111111111111\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion (OVO) :\n[[1085  501    2    0    3    3  114]\n [ 388 1777   62    0   10   14   10]\n [   0   23  229   13    0   16    0]\n [   0    0    4   12    0    5    0]\n [   2   59   12    0    1    0    0]\n [   0   39   89    1    0   15    0]\n [  30    0    0    0    0    0  129]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.72      0.64      0.68      1708\n           2       0.74      0.79      0.76      2261\n           3       0.58      0.81      0.67       281\n           4       0.46      0.57      0.51        21\n           5       0.07      0.01      0.02        74\n           6       0.28      0.10      0.15       144\n           7       0.51      0.81      0.63       159\n\n    accuracy                           0.70      4648\n   macro avg       0.48      0.53      0.49      4648\nweighted avg       0.69      0.70      0.69      4648"
  },
  {
    "objectID": "lda_ovo.html#théorie",
    "href": "lda_ovo.html#théorie",
    "title": "Analyse Discriminante Linéaire - OVO",
    "section": "",
    "text": "L’Analyse Discriminante Linéaire (LDA) est une technique de classification qui cherche à trouver une combinaison linéaire de caractéristiques maximisant la séparation entre les classes.\nL’approche One-Versus-One (OVO) consiste à entraîner un modèle pour chaque paire de classes, ce qui est utile lorsque les classes sont bien séparées."
  },
  {
    "objectID": "lda_ovo.html#hyperparamètres",
    "href": "lda_ovo.html#hyperparamètres",
    "title": "Analyse Discriminante Linéaire - OVO",
    "section": "",
    "text": "Nous allons tester les hyperparamètres suivants : - Régularisation (shrinkage) : contrôle la variance de la covariance estimée (valeurs entre 0 et 1). - Standardisation des données : normalisation des features avant l’entraînement."
  },
  {
    "objectID": "lda_ovo.html#exemple-en-python",
    "href": "lda_ovo.html#exemple-en-python",
    "title": "Analyse Discriminante Linéaire - OVO",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche des meilleurs hyperparamètres\nshrinkage_values = np.linspace(0, 1, 10)\ntrain_accuracies = []\nval_accuracies = []\n\nfor shrinkage in shrinkage_values:\n    lda_ovo = OneVsOneClassifier(LinearDiscriminantAnalysis(solver='lsqr', shrinkage=shrinkage))\n    lda_ovo.fit(X_train, y_train)\n    \n    y_train_pred = lda_ovo.predict(X_train)\n    y_val_pred = lda_ovo.predict(X_val)\n    \n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur shrinkage\nbest_shrinkage = shrinkage_values[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur shrinkage LDA (OVO): {best_shrinkage}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(shrinkage_values, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(shrinkage_values, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Shrinkage\")\nplt.ylabel(\"Précision\")\nplt.title(\"Impact du shrinkage sur la performance de LDA (OVO)\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur shrinkage\nlda_ovo = OneVsOneClassifier(LinearDiscriminantAnalysis(solver='lsqr', shrinkage=best_shrinkage))\nlda_ovo.fit(X_train, y_train)\ny_test_pred = lda_ovo.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion (OVO) :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\nMeilleur shrinkage LDA (OVO): 0.1111111111111111\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion (OVO) :\n[[1085  501    2    0    3    3  114]\n [ 388 1777   62    0   10   14   10]\n [   0   23  229   13    0   16    0]\n [   0    0    4   12    0    5    0]\n [   2   59   12    0    1    0    0]\n [   0   39   89    1    0   15    0]\n [  30    0    0    0    0    0  129]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.72      0.64      0.68      1708\n           2       0.74      0.79      0.76      2261\n           3       0.58      0.81      0.67       281\n           4       0.46      0.57      0.51        21\n           5       0.07      0.01      0.02        74\n           6       0.28      0.10      0.15       144\n           7       0.51      0.81      0.63       159\n\n    accuracy                           0.70      4648\n   macro avg       0.48      0.53      0.49      4648\nweighted avg       0.69      0.70      0.69      4648"
  },
  {
    "objectID": "qda_ova.html",
    "href": "qda_ova.html",
    "title": "Analyse Discriminante Quadratique - OVA",
    "section": "",
    "text": "L’Analyse Discriminante Quadratique (QDA) est une technique de classification qui, contrairement à LDA, permet aux classes d’avoir des matrices de covariance différentes. Cela le rend plus flexible mais peut aussi augmenter le risque de sur-apprentissage.\nL’approche One-Versus-All (OVA) consiste à entraîner un modèle pour chaque classe, en distinguant chaque classe des autres combinées.\n\n\n\nNous allons tester les hyperparamètres suivants : - Régularisation (reg_param) : contrôle la variance de la covariance estimée (valeurs entre 0 et 1). - Standardisation des données : normalisation des features avant l’entraînement.\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche des meilleurs hyperparamètres\nreg_params = np.linspace(0, 1, 10)\ntrain_accuracies = []\nval_accuracies = []\n\nfor reg_param in reg_params:\n    qda_ova = OneVsRestClassifier(QuadraticDiscriminantAnalysis(reg_param=reg_param))\n    qda_ova.fit(X_train, y_train)\n    \n    y_train_pred = qda_ova.predict(X_train)\n    y_val_pred = qda_ova.predict(X_val)\n    \n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur reg_param\nbest_reg_param = reg_params[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur reg_param QDA (OVA): {best_reg_param}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(reg_params, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(reg_params, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Régularisation (reg_param)\")\nplt.ylabel(\"Précision\")\nplt.title(\"Impact de la régularisation sur la performance de QDA (OVA)\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur reg_param\nqda_ova = OneVsRestClassifier(QuadraticDiscriminantAnalysis(reg_param=best_reg_param))\nqda_ova.fit(X_train, y_train)\ny_test_pred = qda_ova.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion (OVA) :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n\n\nMeilleur reg_param QDA (OVA): 1.0\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion (OVA) :\n[[ 851  666    0    0   12    4  175]\n [ 370 1726   35   12   35   73   10]\n [   0   18  140   16   14   93    0]\n [   0    0    5   12    0    4    0]\n [   5   51   10    0    7    1    0]\n [   0   24   51    2    6   61    0]\n [  19    7    0    0    0    0  133]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.68      0.50      0.58      1708\n           2       0.69      0.76      0.73      2261\n           3       0.58      0.50      0.54       281\n           4       0.29      0.57      0.38        21\n           5       0.09      0.09      0.09        74\n           6       0.26      0.42      0.32       144\n           7       0.42      0.84      0.56       159\n\n    accuracy                           0.63      4648\n   macro avg       0.43      0.53      0.46      4648\nweighted avg       0.65      0.63      0.63      4648"
  },
  {
    "objectID": "qda_ova.html#théorie",
    "href": "qda_ova.html#théorie",
    "title": "Analyse Discriminante Quadratique - OVA",
    "section": "",
    "text": "L’Analyse Discriminante Quadratique (QDA) est une technique de classification qui, contrairement à LDA, permet aux classes d’avoir des matrices de covariance différentes. Cela le rend plus flexible mais peut aussi augmenter le risque de sur-apprentissage.\nL’approche One-Versus-All (OVA) consiste à entraîner un modèle pour chaque classe, en distinguant chaque classe des autres combinées."
  },
  {
    "objectID": "qda_ova.html#hyperparamètres",
    "href": "qda_ova.html#hyperparamètres",
    "title": "Analyse Discriminante Quadratique - OVA",
    "section": "",
    "text": "Nous allons tester les hyperparamètres suivants : - Régularisation (reg_param) : contrôle la variance de la covariance estimée (valeurs entre 0 et 1). - Standardisation des données : normalisation des features avant l’entraînement."
  },
  {
    "objectID": "qda_ova.html#exemple-en-python",
    "href": "qda_ova.html#exemple-en-python",
    "title": "Analyse Discriminante Quadratique - OVA",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche des meilleurs hyperparamètres\nreg_params = np.linspace(0, 1, 10)\ntrain_accuracies = []\nval_accuracies = []\n\nfor reg_param in reg_params:\n    qda_ova = OneVsRestClassifier(QuadraticDiscriminantAnalysis(reg_param=reg_param))\n    qda_ova.fit(X_train, y_train)\n    \n    y_train_pred = qda_ova.predict(X_train)\n    y_val_pred = qda_ova.predict(X_val)\n    \n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur reg_param\nbest_reg_param = reg_params[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur reg_param QDA (OVA): {best_reg_param}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(reg_params, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(reg_params, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Régularisation (reg_param)\")\nplt.ylabel(\"Précision\")\nplt.title(\"Impact de la régularisation sur la performance de QDA (OVA)\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur reg_param\nqda_ova = OneVsRestClassifier(QuadraticDiscriminantAnalysis(reg_param=best_reg_param))\nqda_ova.fit(X_train, y_train)\ny_test_pred = qda_ova.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion (OVA) :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n\n\nMeilleur reg_param QDA (OVA): 1.0\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion (OVA) :\n[[ 851  666    0    0   12    4  175]\n [ 370 1726   35   12   35   73   10]\n [   0   18  140   16   14   93    0]\n [   0    0    5   12    0    4    0]\n [   5   51   10    0    7    1    0]\n [   0   24   51    2    6   61    0]\n [  19    7    0    0    0    0  133]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.68      0.50      0.58      1708\n           2       0.69      0.76      0.73      2261\n           3       0.58      0.50      0.54       281\n           4       0.29      0.57      0.38        21\n           5       0.09      0.09      0.09        74\n           6       0.26      0.42      0.32       144\n           7       0.42      0.84      0.56       159\n\n    accuracy                           0.63      4648\n   macro avg       0.43      0.53      0.46      4648\nweighted avg       0.65      0.63      0.63      4648"
  },
  {
    "objectID": "qda_ovo.html",
    "href": "qda_ovo.html",
    "title": "Analyse Discriminante Quadratique - OVO",
    "section": "",
    "text": "L’Analyse Discriminante Quadratique (QDA) est une technique de classification qui, contrairement à LDA, permet aux classes d’avoir des matrices de covariance différentes. Cela le rend plus flexible mais peut aussi augmenter le risque de sur-apprentissage.\nL’approche One-Versus-One (OVO) consiste à entraîner un modèle pour chaque paire de classes, ce qui est utile lorsque les classes sont bien séparées.\n\n\n\nNous allons tester les hyperparamètres suivants : - Régularisation (reg_param) : contrôle la variance de la covariance estimée (valeurs entre 0 et 1). - Standardisation des données : normalisation des features avant l’entraînement.\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche des meilleurs hyperparamètres\nreg_params = np.linspace(0, 1, 10)\ntrain_accuracies = []\nval_accuracies = []\n\nfor reg_param in reg_params:\n    qda_ovo = OneVsOneClassifier(QuadraticDiscriminantAnalysis(reg_param=reg_param))\n    qda_ovo.fit(X_train, y_train)\n    \n    y_train_pred = qda_ovo.predict(X_train)\n    y_val_pred = qda_ovo.predict(X_val)\n    \n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur reg_param\nbest_reg_param = reg_params[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur reg_param QDA (OVO): {best_reg_param}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(reg_params, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(reg_params, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Régularisation (reg_param)\")\nplt.ylabel(\"Précision\")\nplt.title(\"Impact de la régularisation sur la performance de QDA (OVO)\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur reg_param\nqda_ovo = OneVsOneClassifier(QuadraticDiscriminantAnalysis(reg_param=best_reg_param))\nqda_ovo.fit(X_train, y_train)\ny_test_pred = qda_ovo.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion (OVO) :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n\n\nMeilleur reg_param QDA (OVO): 1.0\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion (OVO) :\n[[ 827  670    0    0   22    4  185]\n [ 329 1731   30   12   76   73   10]\n [   0   12  132   16   27   94    0]\n [   0    0    5   12    0    4    0]\n [   5   40    8    0   19    2    0]\n [   0   24   48    2    9   61    0]\n [  17    7    0    0    0    0  135]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.70      0.48      0.57      1708\n           2       0.70      0.77      0.73      2261\n           3       0.59      0.47      0.52       281\n           4       0.29      0.57      0.38        21\n           5       0.12      0.26      0.17        74\n           6       0.26      0.42      0.32       144\n           7       0.41      0.85      0.55       159\n\n    accuracy                           0.63      4648\n   macro avg       0.44      0.55      0.46      4648\nweighted avg       0.66      0.63      0.63      4648"
  },
  {
    "objectID": "qda_ovo.html#théorie",
    "href": "qda_ovo.html#théorie",
    "title": "Analyse Discriminante Quadratique - OVO",
    "section": "",
    "text": "L’Analyse Discriminante Quadratique (QDA) est une technique de classification qui, contrairement à LDA, permet aux classes d’avoir des matrices de covariance différentes. Cela le rend plus flexible mais peut aussi augmenter le risque de sur-apprentissage.\nL’approche One-Versus-One (OVO) consiste à entraîner un modèle pour chaque paire de classes, ce qui est utile lorsque les classes sont bien séparées."
  },
  {
    "objectID": "qda_ovo.html#hyperparamètres",
    "href": "qda_ovo.html#hyperparamètres",
    "title": "Analyse Discriminante Quadratique - OVO",
    "section": "",
    "text": "Nous allons tester les hyperparamètres suivants : - Régularisation (reg_param) : contrôle la variance de la covariance estimée (valeurs entre 0 et 1). - Standardisation des données : normalisation des features avant l’entraînement."
  },
  {
    "objectID": "qda_ovo.html#exemple-en-python",
    "href": "qda_ovo.html#exemple-en-python",
    "title": "Analyse Discriminante Quadratique - OVO",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche des meilleurs hyperparamètres\nreg_params = np.linspace(0, 1, 10)\ntrain_accuracies = []\nval_accuracies = []\n\nfor reg_param in reg_params:\n    qda_ovo = OneVsOneClassifier(QuadraticDiscriminantAnalysis(reg_param=reg_param))\n    qda_ovo.fit(X_train, y_train)\n    \n    y_train_pred = qda_ovo.predict(X_train)\n    y_val_pred = qda_ovo.predict(X_val)\n    \n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur reg_param\nbest_reg_param = reg_params[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur reg_param QDA (OVO): {best_reg_param}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(reg_params, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(reg_params, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Régularisation (reg_param)\")\nplt.ylabel(\"Précision\")\nplt.title(\"Impact de la régularisation sur la performance de QDA (OVO)\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur reg_param\nqda_ovo = OneVsOneClassifier(QuadraticDiscriminantAnalysis(reg_param=best_reg_param))\nqda_ovo.fit(X_train, y_train)\ny_test_pred = qda_ovo.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion (OVO) :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n  warnings.warn(\n\n\nMeilleur reg_param QDA (OVO): 1.0\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion (OVO) :\n[[ 827  670    0    0   22    4  185]\n [ 329 1731   30   12   76   73   10]\n [   0   12  132   16   27   94    0]\n [   0    0    5   12    0    4    0]\n [   5   40    8    0   19    2    0]\n [   0   24   48    2    9   61    0]\n [  17    7    0    0    0    0  135]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.70      0.48      0.57      1708\n           2       0.70      0.77      0.73      2261\n           3       0.59      0.47      0.52       281\n           4       0.29      0.57      0.38        21\n           5       0.12      0.26      0.17        74\n           6       0.26      0.42      0.32       144\n           7       0.41      0.85      0.55       159\n\n    accuracy                           0.63      4648\n   macro avg       0.44      0.55      0.46      4648\nweighted avg       0.66      0.63      0.63      4648"
  },
  {
    "objectID": "regression_logistique_ova.html",
    "href": "regression_logistique_ova.html",
    "title": "Régression Logistique Binomiale - OVA",
    "section": "",
    "text": "La régression logistique binomiale est utilisée pour la classification binaire, mais elle peut être adaptée aux problèmes multiclasse via l’approche One-Versus-All (OVA). Ici, un modèle est entraîné pour chaque classe contre toutes les autres combinées.\n\n\n\nNous allons tester un seul hyperparamètre pour réduire le temps d’entraînement : - Paramètre de régularisation (C) : contrôle la pénalisation de la complexité du modèle (valeurs entre 0.1 et 1).\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche du meilleur hyperparamètre (C seulement)\nC_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]   # Entre 0.1 et 1\ntrain_accuracies = []\nval_accuracies = []\n\nfor C in C_values:\n    model = OneVsRestClassifier(LogisticRegression(solver='saga', C=C, penalty='l2', max_iter=500))\n    model.fit(X_train, y_train)\n    \n    y_train_pred = model.predict(X_train)\n    y_val_pred = model.predict(X_val)\n    \n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur hyperparamètre\nbest_C = C_values[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur hyperparamètre : C={best_C}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(C_values, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(C_values, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Paramètre de régularisation (C)\")\nplt.ylabel(\"Précision\")\nplt.title(\"Impact de la régularisation sur la performance de la régression logistique (OVA)\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur hyperparamètre\nfinal_model = OneVsRestClassifier(LogisticRegression(solver='saga', C=best_C, penalty='l2', max_iter=500))\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n\n\nMeilleur hyperparamètre : C=0.4\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n\n\n\nMatrice de confusion :\n[[1201  474    2    0    0    0   31]\n [ 413 1789   54    0    0    4    1]\n [   0   28  246    0    0    7    0]\n [   0    0   14    4    0    3    0]\n [   1   61   12    0    0    0    0]\n [   0   44   95    0    0    5    0]\n [  77    0    0    0    0    0   82]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.71      0.70      0.71      1708\n           2       0.75      0.79      0.77      2261\n           3       0.58      0.88      0.70       281\n           4       1.00      0.19      0.32        21\n           5       0.00      0.00      0.00        74\n           6       0.26      0.03      0.06       144\n           7       0.72      0.52      0.60       159\n\n    accuracy                           0.72      4648\n   macro avg       0.57      0.44      0.45      4648\nweighted avg       0.70      0.72      0.70      4648\n\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))"
  },
  {
    "objectID": "regression_logistique_ova.html#théorie",
    "href": "regression_logistique_ova.html#théorie",
    "title": "Régression Logistique Binomiale - OVA",
    "section": "",
    "text": "La régression logistique binomiale est utilisée pour la classification binaire, mais elle peut être adaptée aux problèmes multiclasse via l’approche One-Versus-All (OVA). Ici, un modèle est entraîné pour chaque classe contre toutes les autres combinées."
  },
  {
    "objectID": "regression_logistique_ova.html#hyperparamètres",
    "href": "regression_logistique_ova.html#hyperparamètres",
    "title": "Régression Logistique Binomiale - OVA",
    "section": "",
    "text": "Nous allons tester un seul hyperparamètre pour réduire le temps d’entraînement : - Paramètre de régularisation (C) : contrôle la pénalisation de la complexité du modèle (valeurs entre 0.1 et 1)."
  },
  {
    "objectID": "regression_logistique_ova.html#exemple-en-python",
    "href": "regression_logistique_ova.html#exemple-en-python",
    "title": "Régression Logistique Binomiale - OVA",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche du meilleur hyperparamètre (C seulement)\nC_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]   # Entre 0.1 et 1\ntrain_accuracies = []\nval_accuracies = []\n\nfor C in C_values:\n    model = OneVsRestClassifier(LogisticRegression(solver='saga', C=C, penalty='l2', max_iter=500))\n    model.fit(X_train, y_train)\n    \n    y_train_pred = model.predict(X_train)\n    y_val_pred = model.predict(X_val)\n    \n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur hyperparamètre\nbest_C = C_values[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur hyperparamètre : C={best_C}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(C_values, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(C_values, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Paramètre de régularisation (C)\")\nplt.ylabel(\"Précision\")\nplt.title(\"Impact de la régularisation sur la performance de la régression logistique (OVA)\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur hyperparamètre\nfinal_model = OneVsRestClassifier(LogisticRegression(solver='saga', C=best_C, penalty='l2', max_iter=500))\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n\n\nMeilleur hyperparamètre : C=0.4\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n\n\n\nMatrice de confusion :\n[[1201  474    2    0    0    0   31]\n [ 413 1789   54    0    0    4    1]\n [   0   28  246    0    0    7    0]\n [   0    0   14    4    0    3    0]\n [   1   61   12    0    0    0    0]\n [   0   44   95    0    0    5    0]\n [  77    0    0    0    0    0   82]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.71      0.70      0.71      1708\n           2       0.75      0.79      0.77      2261\n           3       0.58      0.88      0.70       281\n           4       1.00      0.19      0.32        21\n           5       0.00      0.00      0.00        74\n           6       0.26      0.03      0.06       144\n           7       0.72      0.52      0.60       159\n\n    accuracy                           0.72      4648\n   macro avg       0.57      0.44      0.45      4648\nweighted avg       0.70      0.72      0.70      4648\n\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))"
  },
  {
    "objectID": "regression_multinomiale.html",
    "href": "regression_multinomiale.html",
    "title": "Régression Multinomiale - Classification Multiclasse",
    "section": "",
    "text": "La régression multinomiale, aussi appelée régression logistique multinomiale, est une extension de la régression logistique qui permet de gérer plusieurs classes. Elle utilise une fonction Softmax en sortie pour assigner une probabilité à chaque classe.\n\n\n\nNous allons optimiser un seul hyperparamètre pour réduire le temps d’entraînement : - Paramètre de régularisation (C) : contrôle la pénalisation de la complexité du modèle (valeurs entre 0.1 et 1).\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche du meilleur hyperparamètre (C seulement)\nC_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]  # Entre 0.1 et 1\nval_accuracies = []\n\nfor C in C_values:\n    model = LogisticRegression(multi_class='multinomial', solver='saga', C=C, penalty='l2', max_iter=500)\n    model.fit(X_train, y_train)\n    y_val_pred = model.predict(X_val)\n    acc = accuracy_score(y_val, y_val_pred)\n    val_accuracies.append((C, acc))\n\n# Sélection du meilleur hyperparamètre\nbest_C, best_acc = max(val_accuracies, key=lambda x: x[1])\nprint(f\"Meilleur hyperparamètre : C={best_C}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(C_values, [acc for C, acc in val_accuracies], marker='o', linestyle='dashed')\nplt.xlabel(\"Paramètre de régularisation (C)\")\nplt.ylabel(\"Précision sur validation\")\nplt.title(\"Impact de la régularisation sur la performance de la régression multinomiale\")\nplt.show()\n\n# Modèle final avec le meilleur hyperparamètre\nfinal_model = LogisticRegression(multi_class='multinomial', solver='saga', C=best_C, penalty='l2', max_iter=500)\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\n\nMeilleur hyperparamètre : C=0.6\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\n\n\nMatrice de confusion :\n[[ 768  343    0    0    9    1   57]\n [ 273 1188   24    0   52   29    5]\n [   0   20  214   46   10   92    0]\n [   0    0   26  328    0   25    0]\n [   3  210   14    0  141   11    0]\n [   0   42   88   40    3  206    0]\n [  79    2    0    0    1    0  298]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.68      0.65      0.67      1178\n           2       0.66      0.76      0.70      1571\n           3       0.58      0.56      0.57       382\n           4       0.79      0.87      0.83       379\n           5       0.65      0.37      0.47       379\n           6       0.57      0.54      0.55       379\n           7       0.83      0.78      0.81       380\n\n    accuracy                           0.68      4648\n   macro avg       0.68      0.65      0.66      4648\nweighted avg       0.68      0.68      0.67      4648\n\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn("
  },
  {
    "objectID": "regression_multinomiale.html#théorie",
    "href": "regression_multinomiale.html#théorie",
    "title": "Régression Multinomiale - Classification Multiclasse",
    "section": "",
    "text": "La régression multinomiale, aussi appelée régression logistique multinomiale, est une extension de la régression logistique qui permet de gérer plusieurs classes. Elle utilise une fonction Softmax en sortie pour assigner une probabilité à chaque classe."
  },
  {
    "objectID": "regression_multinomiale.html#hyperparamètres",
    "href": "regression_multinomiale.html#hyperparamètres",
    "title": "Régression Multinomiale - Classification Multiclasse",
    "section": "",
    "text": "Nous allons optimiser un seul hyperparamètre pour réduire le temps d’entraînement : - Paramètre de régularisation (C) : contrôle la pénalisation de la complexité du modèle (valeurs entre 0.1 et 1)."
  },
  {
    "objectID": "regression_multinomiale.html#exemple-en-python",
    "href": "regression_multinomiale.html#exemple-en-python",
    "title": "Régression Multinomiale - Classification Multiclasse",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche du meilleur hyperparamètre (C seulement)\nC_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]  # Entre 0.1 et 1\nval_accuracies = []\n\nfor C in C_values:\n    model = LogisticRegression(multi_class='multinomial', solver='saga', C=C, penalty='l2', max_iter=500)\n    model.fit(X_train, y_train)\n    y_val_pred = model.predict(X_val)\n    acc = accuracy_score(y_val, y_val_pred)\n    val_accuracies.append((C, acc))\n\n# Sélection du meilleur hyperparamètre\nbest_C, best_acc = max(val_accuracies, key=lambda x: x[1])\nprint(f\"Meilleur hyperparamètre : C={best_C}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(C_values, [acc for C, acc in val_accuracies], marker='o', linestyle='dashed')\nplt.xlabel(\"Paramètre de régularisation (C)\")\nplt.ylabel(\"Précision sur validation\")\nplt.title(\"Impact de la régularisation sur la performance de la régression multinomiale\")\nplt.show()\n\n# Modèle final avec le meilleur hyperparamètre\nfinal_model = LogisticRegression(multi_class='multinomial', solver='saga', C=best_C, penalty='l2', max_iter=500)\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\n\nMeilleur hyperparamètre : C=0.6\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\n\n\nMatrice de confusion :\n[[ 768  343    0    0    9    1   57]\n [ 273 1188   24    0   52   29    5]\n [   0   20  214   46   10   92    0]\n [   0    0   26  328    0   25    0]\n [   3  210   14    0  141   11    0]\n [   0   42   88   40    3  206    0]\n [  79    2    0    0    1    0  298]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.68      0.65      0.67      1178\n           2       0.66      0.76      0.70      1571\n           3       0.58      0.56      0.57       382\n           4       0.79      0.87      0.83       379\n           5       0.65      0.37      0.47       379\n           6       0.57      0.54      0.55       379\n           7       0.83      0.78      0.81       380\n\n    accuracy                           0.68      4648\n   macro avg       0.68      0.65      0.66      4648\nweighted avg       0.68      0.68      0.67      4648\n\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn("
  },
  {
    "objectID": "regression_logistique_ovo.html",
    "href": "regression_logistique_ovo.html",
    "title": "Régression Logistique Binomiale - OVO",
    "section": "",
    "text": "La régression logistique binomiale est utilisée pour la classification binaire, mais elle peut être adaptée aux problèmes multiclasse via l’approche One-Versus-One (OVO). Ici, un modèle est entraîné pour chaque paire de classes, ce qui peut améliorer la précision lorsque les classes sont bien séparées.\n\n\n\nNous allons tester un seul hyperparamètre pour réduire le temps d’entraînement : - Paramètre de régularisation (C) : contrôle la pénalisation de la complexité du modèle (valeurs entre 0.1 et 1).\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche du meilleur hyperparamètre (C seulement)\nC_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]   # Entre 0.1 et 1\ntrain_accuracies = []\nval_accuracies = []\n\nfor C in C_values:\n    model = OneVsOneClassifier(LogisticRegression(solver='saga', C=C, penalty='l2', max_iter=500))\n    model.fit(X_train, y_train)\n    \n    y_train_pred = model.predict(X_train)\n    y_val_pred = model.predict(X_val)\n    \n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur hyperparamètre\nbest_C = C_values[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur hyperparamètre : C={best_C}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(C_values, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(C_values, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Paramètre de régularisation (C)\")\nplt.ylabel(\"Précision\")\nplt.title(\"Impact de la régularisation sur la performance de la régression logistique (OVO)\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur hyperparamètre\nfinal_model = OneVsOneClassifier(LogisticRegression(solver='saga', C=best_C, penalty='l2', max_iter=500))\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n\n\nMeilleur hyperparamètre : C=0.5\n\n\n\n\n\n\n\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n\n\n\nMatrice de confusion :\n[[1210  471    0    0    0    2   25]\n [ 418 1789   29    0    1   23    1]\n [   0   30  221    1    0   29    0]\n [   0    0    6    9    0    6    0]\n [   1   68    1    0    0    4    0]\n [   0   35   81    0    0   28    0]\n [  70    0    0    0    0    0   89]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.71      0.71      0.71      1708\n           2       0.75      0.79      0.77      2261\n           3       0.65      0.79      0.71       281\n           4       0.90      0.43      0.58        21\n           5       0.00      0.00      0.00        74\n           6       0.30      0.19      0.24       144\n           7       0.77      0.56      0.65       159\n\n    accuracy                           0.72      4648\n   macro avg       0.58      0.50      0.52      4648\nweighted avg       0.70      0.72      0.71      4648"
  },
  {
    "objectID": "regression_logistique_ovo.html#théorie",
    "href": "regression_logistique_ovo.html#théorie",
    "title": "Régression Logistique Binomiale - OVO",
    "section": "",
    "text": "La régression logistique binomiale est utilisée pour la classification binaire, mais elle peut être adaptée aux problèmes multiclasse via l’approche One-Versus-One (OVO). Ici, un modèle est entraîné pour chaque paire de classes, ce qui peut améliorer la précision lorsque les classes sont bien séparées."
  },
  {
    "objectID": "regression_logistique_ovo.html#hyperparamètres",
    "href": "regression_logistique_ovo.html#hyperparamètres",
    "title": "Régression Logistique Binomiale - OVO",
    "section": "",
    "text": "Nous allons tester un seul hyperparamètre pour réduire le temps d’entraînement : - Paramètre de régularisation (C) : contrôle la pénalisation de la complexité du modèle (valeurs entre 0.1 et 1)."
  },
  {
    "objectID": "regression_logistique_ovo.html#exemple-en-python",
    "href": "regression_logistique_ovo.html#exemple-en-python",
    "title": "Régression Logistique Binomiale - OVO",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche du meilleur hyperparamètre (C seulement)\nC_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]   # Entre 0.1 et 1\ntrain_accuracies = []\nval_accuracies = []\n\nfor C in C_values:\n    model = OneVsOneClassifier(LogisticRegression(solver='saga', C=C, penalty='l2', max_iter=500))\n    model.fit(X_train, y_train)\n    \n    y_train_pred = model.predict(X_train)\n    y_val_pred = model.predict(X_val)\n    \n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur hyperparamètre\nbest_C = C_values[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur hyperparamètre : C={best_C}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(C_values, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(C_values, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Paramètre de régularisation (C)\")\nplt.ylabel(\"Précision\")\nplt.title(\"Impact de la régularisation sur la performance de la régression logistique (OVO)\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur hyperparamètre\nfinal_model = OneVsOneClassifier(LogisticRegression(solver='saga', C=best_C, penalty='l2', max_iter=500))\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n\n\nMeilleur hyperparamètre : C=0.5\n\n\n\n\n\n\n\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n\n\n\nMatrice de confusion :\n[[1210  471    0    0    0    2   25]\n [ 418 1789   29    0    1   23    1]\n [   0   30  221    1    0   29    0]\n [   0    0    6    9    0    6    0]\n [   1   68    1    0    0    4    0]\n [   0   35   81    0    0   28    0]\n [  70    0    0    0    0    0   89]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.71      0.71      0.71      1708\n           2       0.75      0.79      0.77      2261\n           3       0.65      0.79      0.71       281\n           4       0.90      0.43      0.58        21\n           5       0.00      0.00      0.00        74\n           6       0.30      0.19      0.24       144\n           7       0.77      0.56      0.65       159\n\n    accuracy                           0.72      4648\n   macro avg       0.58      0.50      0.52      4648\nweighted avg       0.70      0.72      0.71      4648"
  },
  {
    "objectID": "svm_ova.html",
    "href": "svm_ova.html",
    "title": "SVM - One-Versus-All (OVA)",
    "section": "",
    "text": "Les machines à vecteurs de support (SVM) sont des modèles de classification supervisés qui cherchent à maximiser la marge de séparation entre les classes. Pour un problème multiclasse, l’approche One-Versus-All (OVA) entraîne un SVM pour chaque classe, la comparant à toutes les autres classes combinées.\n\n\n\nNous allons tester un seul hyperparamètre pour réduire le temps d’entraînement : - Paramètre de régularisation (C) : contrôle la pénalisation des erreurs de classification (valeurs entre 0.1 et 1).\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche du meilleur hyperparamètre (C seulement)\nC_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]   # Entre 0.1 et 1\ntrain_accuracies = []\nval_accuracies = []\n\nfor C in C_values:\n    model = OneVsRestClassifier(SVC(kernel='rbf', C=C))\n    model.fit(X_train, y_train)\n    \n    y_train_pred = model.predict(X_train)\n    y_val_pred = model.predict(X_val)\n    \n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur hyperparamètre\nbest_C = C_values[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur hyperparamètre : C={best_C}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(C_values, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(C_values, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Paramètre de régularisation (C)\")\nplt.ylabel(\"Précision\")\nplt.title(\"Impact de la régularisation sur la performance du SVM (OVA)\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur hyperparamètre\nfinal_model = OneVsRestClassifier(SVC(kernel='rbf', C=best_C))\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\nMeilleur hyperparamètre : C=1\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion :\n[[1218  453    1    0    0    1   35]\n [ 368 1841   40    0    1    7    4]\n [   0   23  250    0    0    8    0]\n [   0    0   18    0    0    3    0]\n [   4   59    9    0    2    0    0]\n [   0   38   91    0    0   15    0]\n [  57    0    0    0    0    0  102]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.74      0.71      0.73      1708\n           2       0.76      0.81      0.79      2261\n           3       0.61      0.89      0.72       281\n           4       0.00      0.00      0.00        21\n           5       0.67      0.03      0.05        74\n           6       0.44      0.10      0.17       144\n           7       0.72      0.64      0.68       159\n\n    accuracy                           0.74      4648\n   macro avg       0.56      0.46      0.45      4648\nweighted avg       0.73      0.74      0.72      4648\n\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))"
  },
  {
    "objectID": "svm_ova.html#théorie",
    "href": "svm_ova.html#théorie",
    "title": "SVM - One-Versus-All (OVA)",
    "section": "",
    "text": "Les machines à vecteurs de support (SVM) sont des modèles de classification supervisés qui cherchent à maximiser la marge de séparation entre les classes. Pour un problème multiclasse, l’approche One-Versus-All (OVA) entraîne un SVM pour chaque classe, la comparant à toutes les autres classes combinées."
  },
  {
    "objectID": "svm_ova.html#hyperparamètres",
    "href": "svm_ova.html#hyperparamètres",
    "title": "SVM - One-Versus-All (OVA)",
    "section": "",
    "text": "Nous allons tester un seul hyperparamètre pour réduire le temps d’entraînement : - Paramètre de régularisation (C) : contrôle la pénalisation des erreurs de classification (valeurs entre 0.1 et 1)."
  },
  {
    "objectID": "svm_ova.html#exemple-en-python",
    "href": "svm_ova.html#exemple-en-python",
    "title": "SVM - One-Versus-All (OVA)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche du meilleur hyperparamètre (C seulement)\nC_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]   # Entre 0.1 et 1\ntrain_accuracies = []\nval_accuracies = []\n\nfor C in C_values:\n    model = OneVsRestClassifier(SVC(kernel='rbf', C=C))\n    model.fit(X_train, y_train)\n    \n    y_train_pred = model.predict(X_train)\n    y_val_pred = model.predict(X_val)\n    \n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur hyperparamètre\nbest_C = C_values[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur hyperparamètre : C={best_C}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(C_values, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(C_values, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Paramètre de régularisation (C)\")\nplt.ylabel(\"Précision\")\nplt.title(\"Impact de la régularisation sur la performance du SVM (OVA)\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur hyperparamètre\nfinal_model = OneVsRestClassifier(SVC(kernel='rbf', C=best_C))\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\nMeilleur hyperparamètre : C=1\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion :\n[[1218  453    1    0    0    1   35]\n [ 368 1841   40    0    1    7    4]\n [   0   23  250    0    0    8    0]\n [   0    0   18    0    0    3    0]\n [   4   59    9    0    2    0    0]\n [   0   38   91    0    0   15    0]\n [  57    0    0    0    0    0  102]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.74      0.71      0.73      1708\n           2       0.76      0.81      0.79      2261\n           3       0.61      0.89      0.72       281\n           4       0.00      0.00      0.00        21\n           5       0.67      0.03      0.05        74\n           6       0.44      0.10      0.17       144\n           7       0.72      0.64      0.68       159\n\n    accuracy                           0.74      4648\n   macro avg       0.56      0.46      0.45      4648\nweighted avg       0.73      0.74      0.72      4648\n\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))"
  },
  {
    "objectID": "svm_ovo.html",
    "href": "svm_ovo.html",
    "title": "SVM - One-Versus-One (OVO)",
    "section": "",
    "text": "Les machines à vecteurs de support (SVM) sont des modèles de classification supervisés qui cherchent à maximiser la marge de séparation entre les classes. Pour un problème multiclasse, l’approche One-Versus-One (OVO) entraîne un SVM pour chaque paire de classes, ce qui permet une meilleure séparation lorsque les classes sont bien distinctes.\n\n\n\nNous allons tester un seul hyperparamètre pour réduire le temps d’entraînement : - Paramètre de régularisation (C) : contrôle la pénalisation des erreurs de classification (valeurs entre 0.1 et 1).\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche du meilleur hyperparamètre (C seulement)\nC_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]  # Entre 0.1 et 1\ntrain_accuracies = []\nval_accuracies = []\n\nfor C in C_values:\n    model = OneVsOneClassifier(SVC(kernel='rbf', C=C))\n    model.fit(X_train, y_train)\n    \n    y_train_pred = model.predict(X_train)\n    y_val_pred = model.predict(X_val)\n    \n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur hyperparamètre\nbest_C = C_values[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur hyperparamètre : C={best_C}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(C_values, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(C_values, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Paramètre de régularisation (C)\")\nplt.ylabel(\"Précision\")\nplt.title(\"Impact de la régularisation sur la performance du SVM (OVO)\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur hyperparamètre\nfinal_model = OneVsOneClassifier(SVC(kernel='rbf', C=best_C))\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\nMeilleur hyperparamètre : C=1\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion :\n[[1223  454    0    0    0    0   31]\n [ 372 1834   42    0    0    9    4]\n [   0   23  252    0    0    6    0]\n [   0    0   18    0    0    3    0]\n [   4   64    6    0    0    0    0]\n [   0   39   98    0    0    7    0]\n [  56    0    0    0    0    0  103]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.74      0.72      0.73      1708\n           2       0.76      0.81      0.78      2261\n           3       0.61      0.90      0.72       281\n           4       0.00      0.00      0.00        21\n           5       0.00      0.00      0.00        74\n           6       0.28      0.05      0.08       144\n           7       0.75      0.65      0.69       159\n\n    accuracy                           0.74      4648\n   macro avg       0.45      0.45      0.43      4648\nweighted avg       0.71      0.74      0.72      4648\n\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))"
  },
  {
    "objectID": "svm_ovo.html#théorie",
    "href": "svm_ovo.html#théorie",
    "title": "SVM - One-Versus-One (OVO)",
    "section": "",
    "text": "Les machines à vecteurs de support (SVM) sont des modèles de classification supervisés qui cherchent à maximiser la marge de séparation entre les classes. Pour un problème multiclasse, l’approche One-Versus-One (OVO) entraîne un SVM pour chaque paire de classes, ce qui permet une meilleure séparation lorsque les classes sont bien distinctes."
  },
  {
    "objectID": "svm_ovo.html#hyperparamètres",
    "href": "svm_ovo.html#hyperparamètres",
    "title": "SVM - One-Versus-One (OVO)",
    "section": "",
    "text": "Nous allons tester un seul hyperparamètre pour réduire le temps d’entraînement : - Paramètre de régularisation (C) : contrôle la pénalisation des erreurs de classification (valeurs entre 0.1 et 1)."
  },
  {
    "objectID": "svm_ovo.html#exemple-en-python",
    "href": "svm_ovo.html#exemple-en-python",
    "title": "SVM - One-Versus-One (OVO)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement des ensembles de données\ntrain_data = pd.read_csv('covertype_train.csv')\nval_data = pd.read_csv('covertype_val.csv')\ntest_data = pd.read_csv('covertype_test.csv')\n\n# Préparation des données\nX_train = train_data.drop('Cover_Type', axis=1)\ny_train = train_data['Cover_Type']\n\nX_val = val_data.drop('Cover_Type', axis=1)\ny_val = val_data['Cover_Type']\n\nX_test = test_data.drop('Cover_Type', axis=1)\ny_test = test_data['Cover_Type']\n\n# Standardisation des données\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\n# Recherche du meilleur hyperparamètre (C seulement)\nC_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]  # Entre 0.1 et 1\ntrain_accuracies = []\nval_accuracies = []\n\nfor C in C_values:\n    model = OneVsOneClassifier(SVC(kernel='rbf', C=C))\n    model.fit(X_train, y_train)\n    \n    y_train_pred = model.predict(X_train)\n    y_val_pred = model.predict(X_val)\n    \n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    val_accuracies.append(accuracy_score(y_val, y_val_pred))\n\n# Sélection du meilleur hyperparamètre\nbest_C = C_values[val_accuracies.index(max(val_accuracies))]\nprint(f\"Meilleur hyperparamètre : C={best_C}\")\n\n# Affichage du graphique\nplt.figure(figsize=(8, 6))\nplt.plot(C_values, train_accuracies, marker='o', linestyle='dashed', label='Train Accuracy')\nplt.plot(C_values, val_accuracies, marker='s', linestyle='dashed', label='Validation Accuracy')\nplt.xlabel(\"Paramètre de régularisation (C)\")\nplt.ylabel(\"Précision\")\nplt.title(\"Impact de la régularisation sur la performance du SVM (OVO)\")\nplt.legend()\nplt.show()\n\n# Modèle final avec le meilleur hyperparamètre\nfinal_model = OneVsOneClassifier(SVC(kernel='rbf', C=best_C))\nfinal_model.fit(X_train, y_train)\ny_test_pred = final_model.predict(X_test)\n\n# Affichage de la matrice de confusion\nconf_matrix = confusion_matrix(y_test, y_test_pred)\nprint(\"\\nMatrice de confusion :\")\nprint(conf_matrix)\n\nprint(\"\\nÉvaluation sur l'ensemble de test\")\nprint(classification_report(y_test, y_test_pred))\n\nMeilleur hyperparamètre : C=1\n\n\n\n\n\n\n\n\n\n\nMatrice de confusion :\n[[1223  454    0    0    0    0   31]\n [ 372 1834   42    0    0    9    4]\n [   0   23  252    0    0    6    0]\n [   0    0   18    0    0    3    0]\n [   4   64    6    0    0    0    0]\n [   0   39   98    0    0    7    0]\n [  56    0    0    0    0    0  103]]\n\nÉvaluation sur l'ensemble de test\n              precision    recall  f1-score   support\n\n           1       0.74      0.72      0.73      1708\n           2       0.76      0.81      0.78      2261\n           3       0.61      0.90      0.72       281\n           4       0.00      0.00      0.00        21\n           5       0.00      0.00      0.00        74\n           6       0.28      0.05      0.08       144\n           7       0.75      0.65      0.69       159\n\n    accuracy                           0.74      4648\n   macro avg       0.45      0.45      0.43      4648\nweighted avg       0.71      0.74      0.72      4648\n\n\n\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/ensai/.local/share/virtualenvs/postagram_ensai-i0XV5lHB/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))"
  }
]